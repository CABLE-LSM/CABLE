!==============================================================================
! This source code is part of the
! Australian Community Atmosphere Biosphere Land Exchange (CABLE) model.
! This work is licensed under the CSIRO Open Source Software License
! Agreement (variation of the BSD / MIT License).
!
! You may not use this file except in compliance with this License.
! A copy of the License (CSIRO_BSD_MIT_License_v2.0_CABLE.txt) is located
! in each directory containing CABLE code.
!
! ==============================================================================
! Purpose: Offline driver for mpi worker in CABLE global run
!
! Contact: Bernard.Pak@csiro.au
!
! History: Since 1.4b, capability to run global offline (ncciy = YEAR),
!          inclusion of call to CASA-CNP (icycle>0)
!          soil_snow_type now ssnow (instead of ssoil)
!
!          MPI wrapper developed by Maciej Golebiewski (2012)
!          Modified from cable_driver.F90 in CABLE-2.0_beta r171 by B Pak
!
! ==============================================================================
! Uses:           mpi
!                 cable_mpicommon
!                 cable_def_types_mod
!                 cable_IO_vars_module
!                 cable_common_module
!                 cable_data_module
!                 cable_input_module
!                 cable_output_module
!                 cable_cbm_module
!                 casadimension
!                 casavariable
!                 phenvariable
!
! CALLs:       point2constants
!              casa_feedback
!              cbm
!              bgcdriver
!              sumcflux
!              find_extents
!              worker_decomp
!              worker_cable_params
!              worker_casa_params
!              worker_intype
!              worker_outtype
!              worker_casa_type
!              worker_restart_type
!              worker_end
!              ! 13C
!              worker_c13o2_flux_params
!              worker_c13o2_pool_params
!              worker_c13o2_luc_params
!              worker_c13o2_flux_types
!              worker_c13o2_pool_types
!              worker_c13o2_luc_types
!
! input  file: [SiteName].nc
!              poolcnpIn[SiteName].csv -- for CASA-CNP only
!              gridinfo_CSIRO_1x1.nc
!              def_veg_params.txt
!              def_soil_params.txt -- nearly redundant, can be switched on
!              restart_in.nc -- not strictly required
!
! output file: log_cable.txt
!              out_cable.nc
!              restart_out.nc
!              poolcnpOut.csv -- from CASA-CNP
!==============================================================================
MODULE cable_mpiworker

  USE cable_mpicommon
  USE cable_IO_vars_module, ONLY: wlogn

  IMPLICIT NONE

  SAVE

  PRIVATE

  ! MPI: MPI derived datatype for receiving input from the master
  INTEGER :: inp_t

  ! MPI: MPI derived datatype for sending results back to the master
  INTEGER :: send_t

  ! worker's struct for sending final casa results to the master
  INTEGER :: casa_t

  ! worker's struct for rec'ing/sending final casa results to/from the master
  INTEGER :: casa_dump_t

  ! worker's struct for rec'ing/sending casa pools to/from the master (for LUC calcs)
  INTEGER :: casa_LUC_t

  ! worker's struct for rec'ing/sending final casa results to/from the master
  INTEGER :: climate_t

  ! worker's struct for rec'ing/sending pop io to/from the master
  INTEGER :: pop_t

  ! worker's struct for rec'ing/sending blaze restart to/from the master
  INTEGER :: blaze_in_t
  INTEGER :: blaze_out_t
  INTEGER :: blaze_restart_t

  ! 13C
  ! MPI derived datatype handles for receiving c13o2 results from the workers and restart values
  integer :: c13o2_flux_t
  integer :: c13o2_pool_t
  integer :: c13o2_luc_t

  ! worker's struct for restart data to the master
  INTEGER :: restart_t

  ! worker's logfile unit
  !INTEGER :: wlogn

  PUBLIC :: mpidrv_worker

CONTAINS

  SUBROUTINE mpidrv_worker(comm)

    use mpi

    USE cable_def_types_mod
    USE cable_io_vars_module, ONLY: logn, gswpfile, ncciy, leaps, &
         verbose, fixedCO2, output, check, patchout, soilparmnew, &
         landpt, latitude, longitude
    USE cable_common_module,  ONLY: ktau_gl, kend_gl, knode_gl, cable_user, &
         cable_runtime, filename, &
         redistrb, wiltParam, satuParam, CurYear, &
         IS_LEAPYEAR, IS_CASA_TIME, calcsoilalbedo, &
         kwidth_gl
    USE cable_data_module,    ONLY: driver_type, point2constants
    USE cable_cbm_module,     only: cbm
    USE cable_climate_mod

    ! modules related to CASA-CNP
    USE casadimension,        ONLY: icycle
    USE casavariable,         ONLY: casafile, casa_biome, casa_pool, casa_flux, &
         casa_met, casa_balance
    USE phenvariable,         ONLY: phen_variable
    use casa_cable,           only: bgcdriver, POPdriver, casa_feedback, sumcflux
    use casa_inout,           only: casa_cnpflux

    !CLN added
    ! modules related to POP
    USE POP_Types,            ONLY: POP_TYPE
    USE POP_Constants,        ONLY: rshootfrac

    ! modules related to fire
    USE BLAZE_MOD,            ONLY: TYPE_BLAZE, BLAZE_ACCOUNTING, INI_BLAZE
    USE BLAZE_MPI,            ONLY: WORKER_BLAZE_TYPES ! , WORKER_SIMFIRE_TYPES
    USE SIMFIRE_MOD,          ONLY: TYPE_SIMFIRE, INI_SIMFIRE

    ! gm
    USE cable_adjust_JV_gm_module, ONLY: read_gm_LUT, LUT_VcmaxJmax, LUT_gm, LUT_Vcmax, LUT_Rd

    ! 13C
    use cable_c13o2_def,         only: c13o2_flux, c13o2_pool, c13o2_luc ! , &
    ! c13o2_update_sum_pools, c13o2_zero_sum_pools
    use cable_c13o2,             only: c13o2_sanity_pools
    use mo_isotope,              only: isoratio ! vpdbc13
    use mo_c13o2_photosynthesis, only: c13o2_discrimination_simple, c13o2_discrimination

    IMPLICIT NONE

    ! MPI:
    INTEGER :: comm ! MPI communicator for comms with the workers

    ! CABLE namelist: model configuration, runtime/user switches
    CHARACTER(LEN=200), PARAMETER :: CABLE_NAMELIST='cable.nml'

    ! timing variables
    INTEGER, PARAMETER ::  kstart = 1  ! start of simulation
    INTEGER, PARAMETER ::  mloop  = 30 ! CASA-CNP PreSpinup loops

    INTEGER :: &
         ktau,        &  ! increment equates to timestep, resets if spinning up
         ktau_tot,    &  ! NO reset when spinning up, total timesteps by model
         kend,        &  ! no. of time steps in run
         !CLN      kstart = 1, &  ! timestep to start at
         koffset = 0, &  ! timestep to start at
         ktauday,     &  ! day counter for CASA-CNP
         idoy,        &  ! day of year (1:365) counter for CASA-CNP
         nyear,       &  ! year counter for CASA-CNP
         YYYY,        &  !
         LOY,         &  ! Length of Year
         rank            ! Rank of this worker

    REAL      :: dels    ! time step size in seconds
    CHARACTER :: cRank*4 ! for worker-logfiles

    ! CABLE variables
    TYPE(met_type)       :: met     ! met input variables
    TYPE(air_type)       :: air     ! air property variables
    TYPE(canopy_type)    :: canopy  ! vegetation variables
    TYPE(radiation_type) :: rad     ! radiation variables
    TYPE(roughness_type) :: rough   ! roughness varibles
    TYPE(balances_type)  :: bal     ! energy and water balance variables
    TYPE(soil_snow_type) :: ssnow   ! soil and snow variables
    TYPE(climate_type)   :: climate     ! climate variables

    ! CABLE parameters
    TYPE(soil_parameter_type) :: soil     ! soil parameters
    TYPE(veg_parameter_type)  :: veg      ! vegetation parameters
    TYPE(driver_type)         :: C        ! constants used locally
    TYPE(sum_flux_type)       :: sum_flux ! cumulative flux variables
    TYPE(bgc_pool_type)       :: bgc      ! carbon pool variables

    ! CASA-CNP variables
    TYPE(casa_biome)     :: casabiome
    TYPE(casa_pool)      :: casapool
    TYPE(casa_flux)      :: casaflux
    TYPE(casa_met)       :: casamet
    TYPE(casa_balance)   :: casabal
    TYPE(phen_variable)  :: phen
    TYPE(POP_TYPE)       :: POP

    ! BLAZE variables
    TYPE(TYPE_BLAZE)     :: BLAZE
    TYPE(TYPE_SIMFIRE)   :: SIMFIRE
    ! REAL(r_2), DIMENSION(:), ALLOCATABLE :: POP_TO, POP_CWD, POP_STR

    ! 13C
    type(c13o2_flux) :: c13o2flux
    type(c13o2_pool) :: c13o2pools ! , sum_c13o2pools
    type(c13o2_luc)  :: c13o2luc
    ! I/O
    ! discrimination
    integer :: ileaf
    real(r_2), dimension(:,:), allocatable :: gpp
    real(r_2), dimension(:),   allocatable :: Ra

    ! declare vars for switches (default .FALSE.) etc declared thru namelist
    LOGICAL, SAVE :: &
         vegparmnew    = .FALSE., & ! using new format input file (BP dec 2007)
         spinup        = .FALSE., & ! model spinup to soil state equilibrium?
         spinConv      = .FALSE., & ! has spinup converged?
         spincasainput = .FALSE., & ! TRUE: SAVE input req'd to spin CASA-CNP
                                    ! FALSE: READ input to spin CASA-CNP
         spincasa      = .FALSE., & ! TRUE: CASA-CNP Will spin mloop times,
                                    ! FALSE: no spin up
         l_casacnp     = .FALSE., & ! using CASA-CNP with CABLE
         l_laiFeedbk   = .FALSE., & ! using prognostic LAI
         l_vcmaxFeedbk = .FALSE., & ! using prognostic Vcmax
         CASAONLY      = .FALSE., & ! ONLY Run CASA-CNP
         CALL1         = .TRUE.

    logical :: casa_time
    logical :: liseod, liseoy ! is end of day, is end of year

    REAL :: &
         delsoilM, & ! allowed variation in soil moisture for spin up
         delsoilT    ! allowed variation in soil temperature for spin up

    ! MPI:
    LOGICAL :: loop_exit     ! MPI: exit flag for bcast to workers
    INTEGER :: stat(MPI_STATUS_SIZE)
    INTEGER :: icomm ! separate dupes of MPI communicator for send and recv
    INTEGER :: ocomm ! separate dupes of MPI communicator for send and recv
    INTEGER :: ierr
    REAL    :: etime, etimelast

    ! switches etc defined thru namelist (by default cable.nml)
    NAMELIST/CABLE/        &
         filename,         & ! TYPE, containing input filenames
         vegparmnew,       & ! use new soil param. method
         soilparmnew,      & ! use new soil param. method
         calcsoilalbedo,   & ! switch: soil colour albedo - Ticket #27
         spinup,           & ! spinup model (soil) to steady state
         delsoilM,delsoilT,& !
         output,           &
         patchout,         &
         check,            &
         verbose,          &
         leaps,            &
         logn,             &
         fixedCO2,         &
         spincasainput,    &
         spincasa,         &
         l_casacnp,        &
         l_laiFeedbk,      &
         l_vcmaxFeedbk,    &
         icycle,           &
         casafile,         &
         ncciy,            &
         gswpfile,         &
         redistrb,         &
         wiltParam,        &
         satuParam,        &
         cable_user           ! additional USER switches

    INTEGER :: LALLOC

    ! command line arguments
    integer :: narg, len1, len2
    character(len=500) :: arg1
    character(len=200) :: arg2

    ! END header

    ! Maciej: make sure the variable does not go out of scope
    mp = 0

    ! Open, read and close the namelist file.
    OPEN(10, FILE=CABLE_NAMELIST)
    READ(10, NML=CABLE)   !where NML=CABLE defined above
    CLOSE(10)

    ! IF (IARGC() > 0) THEN
    !    CALL GETARG(1, filename%met)
    !    CALL GETARG(2, casafile%cnpipool)
    ! ENDIF
    narg = command_argument_count()
    if (narg > 0) then
       call get_command_argument(1, arg1, len1)
       filename%met = arg1(1:len1)
       call get_command_argument(2, arg2, len2)
       casafile%cnpipool = arg2(1:len2)
    endif

    IF (CABLE_USER%POPLUC .AND. TRIM(CABLE_USER%POPLUC_RunType) .EQ. 'static') &
         CABLE_USER%POPLUC= .FALSE.

    ! Get worker's rank and determine logfile-unit

    ! MPI: TODO: find a way to preserve workers log messages somewhere
    ! (either separate files or collated by the master to a single file
    ! or perhaps use MPI-IO - but probably not gonna work with random length
    ! text strings)
    ! LN: Done!
    IF (CABLE_USER%LogWorker) THEN
       CALL MPI_Comm_rank(comm, rank, ierr)
       WRITE(cRank,FMT='(I4.4)') rank
       wlogn = 1000+rank
       OPEN(wlogn,FILE="cable_log_"//cRank,STATUS="REPLACE")
    ELSE
       wlogn = 1000
       OPEN(wlogn, FILE="/dev/null")
    ENDIF

    ! INITIALISATION depending on nml settings

    CurYear = CABLE_USER%YearStart

    IF (icycle .GE. 11) THEN
       icycle                     = icycle - 10
       CASAONLY                   = .TRUE.
       CABLE_USER%CASA_DUMP_READ  = .TRUE.
       CABLE_USER%CASA_DUMP_WRITE = .FALSE.
    ELSEIF (icycle .EQ. 0) THEN
       !call_blaze=0 if blaze off, >0 if blaze on to some extent
       CABLE_USER%CASA_DUMP_READ  = .FALSE.
       spincasa                   = .FALSE.
       CABLE_USER%CALL_POP        = .FALSE.
       CABLE_USER%CALL_BLAZE      = 0
    ENDIF

    !! vh_js !!
    IF (icycle.GT.0) THEN
       l_casacnp = .TRUE.
    ELSE
       l_casacnp = .FALSE.
    ENDIF

    !! vh_js !! suggest LALLOC should ulitmately be a switch in the .nml file
    IF (CABLE_USER%CALL_POP) THEN
       LALLOC = 3 ! for use with POP: makes use of pipe model to partition between stem and leaf
    ELSE
       LALLOC = 0 ! default
    ENDIF

    IF ( TRIM(cable_user%MetType) .EQ. 'gpgs' ) THEN
       cable_user%MetType = 'gswp'
    ENDIF

    cable_runtime%offline = .TRUE.

    ! associate pointers used locally with global definitions
    CALL point2constants( C )

    if ( l_casacnp  .and. ( icycle == 0 .or. icycle > 3 ) ) then
       write(*,*) 'icycle must be 1 to 3 when using casaCNP'
       call MPI_Abort(comm, 50, ierr)
    endif
    if ( ( l_laiFeedbk .or. l_vcmaxFeedbk ) .and. ( .not. l_casacnp ) ) then
       write(*,*) 'casaCNP required to get prognostic LAI or Vcmax'
       call MPI_Abort(comm, 51, ierr)
    endif
    if ( l_vcmaxFeedbk .AND. icycle < 2 ) then
       write(*,*) 'icycle must be 2 to 3 to get prognostic Vcmax'
       call MPI_Abort(comm, 52, ierr)
    endif
    if ( icycle > 0 .and. ( .not. soilparmnew ) ) then
       write(*,*) 'casaCNP must use new soil parameters'
       call MPI_Abort(comm, 53, ierr)
    endif

    ! gm lookup table
    if (cable_user%explicit_gm .and. len(trim(cable_user%gm_LUT_file)) .gt. 1) then
      write(*,*) 'Reading gm LUT file'
      call read_gm_LUT(cable_user%gm_LUT_file, LUT_VcmaxJmax, LUT_gm, LUT_Vcmax, LUT_Rd)
    endif
    ! Open log file:
    ! MPI: worker logs go to the black hole
    ! by opening the file we don't need to touch any of the code that writes
    ! to it and may be called somewhere by a worker
    ! OPEN(logn,FILE=filename%log)

    !! Check for gswp run
    ! MPI: done by the master only; if check fails then master MPI_Aborts
    ! everyone
    !IF (ncciy /= 0) THEN
    !
    !   PRINT *, 'Looking for global offline run info.'
    !
    !   IF (ncciy < 1986 .OR. ncciy > 1995) THEN
    !      PRINT *, 'Year ', ncciy, ' outside range of dataset!'
    !      STOP 'Please check input in namelist file.'
    !   ELSE
    !
    !      CALL prepareFiles(ncciy)
    !
    !   ENDIF
    !
    !ENDIF

    ! Open met data and get site information from netcdf file.
    ! This retrieves time step size, number of timesteps, starting date,
    ! latitudes, longitudes, number of sites.
    ! MPI: master only; necessary info will be received by MPI below
    ! CALL open_met_file( dels, kend, spinup, C%TFRZ )

    ! Checks where parameters and initialisations should be loaded from.
    ! If they can be found in either the met file or restart file, they will
    ! load from there, with the met file taking precedence. Otherwise, they'll
    ! be chosen from a coarse global grid of veg and soil types, based on
    ! the lat/lon coordinates. Allocation of CABLE's main variables also here.
    ! MPI: master only; necessary info will be received by MPI below
    ! CALL load_parameters( met, air, ssnow, veg, bgc,                            &
    !                       soil, canopy, rough, rad, sum_flux,                   &
    !                       bal, logn, vegparmnew, casabiome, casapool,           &
    !                       casaflux, casamet, casabal, phen, C%EMSOIL,        &
    !                       C%TFRZ )

    ! Check for leap-year settings
    CALL MPI_Bcast(leaps, 1, MPI_LOGICAL, 0, comm, ierr)

    ktau_tot = 0
    SPINLOOP:DO
       YEARLOOP: DO YYYY=CABLE_USER%YearStart, CABLE_USER%YearEnd
          CurYear = YYYY
          IF ( leaps .AND. IS_LEAPYEAR( YYYY ) ) THEN
             LOY = 366
          ELSE
             LOY = 365
          ENDIF

          IF ( CALL1 ) THEN
             IF (.NOT.spinup)   spinConv=.TRUE.
             ! MPI: bcast to workers so that they don't need to open the met file themselves
             CALL MPI_Bcast(dels, 1, MPI_REAL, 0, comm, ierr)
          ENDIF

          ! MPI: receive from master ending time fields
          CALL MPI_Bcast(kend, 1, MPI_INTEGER, 0, comm, ierr)

          IF ( CALL1 ) THEN
             ! MPI: need to know extents before creating datatypes
             CALL find_extents()

             ! MPI: receive decomposition info from the master
             call worker_decomp(comm)

             ! MPI: in overlap version sends and receives occur on separate comms
             CALL MPI_Comm_dup(comm, icomm, ierr)
             CALL MPI_Comm_dup(comm, ocomm, ierr)

             ! MPI: data set in load_parameter is now received from
             ! the master
             CALL worker_cable_params(comm, met, air, ssnow, veg, bgc, soil, canopy, &
                  rough, rad, sum_flux, bal)
             ! 13C
             if (cable_user%c13o2) then
                allocate(gpp(size(canopy%An,1),size(canopy%An,2)))
                allocate(Ra(size(canopy%An,1)))
             endif

             ktauday = int(24.0*3600.0/dels)
             if (cable_user%call_climate) &
                  CALL worker_climate_types(comm, climate, ktauday)

             ! MPI: mvtype and mstype send out here instead of inside worker_casa_params
             !      so that old CABLE carbon module can use them. (BP May 2013)
             CALL MPI_Bcast(mvtype, 1, MPI_INTEGER, 0, comm, ierr)
             CALL MPI_Bcast(mstype, 1, MPI_INTEGER, 0, comm, ierr)

             ! MPI: casa parameters received only if cnp module is active
             IF (icycle>0) THEN
                CALL worker_casa_params(comm, casabiome, casapool, casaflux, casamet, casabal, phen)
                ! 13C
                if (cable_user%c13o2) then
                   call worker_c13o2_flux_params(comm, c13o2flux)
                   call worker_c13o2_pool_params(comm, c13o2pools)
                   if (cable_user%popluc) then
                      call worker_c13o2_luc_params(comm, c13o2luc)
                   endif
                endif
                ! MPI: POP restart received only if pop module AND casa are active
                if (cable_user%call_pop) then
                   call worker_pop_types(comm, veg, pop)
                endif

                ! CLN - call_blaze=0 if blaze off, >0 if blaze on to some extent
                if (cable_user%call_blaze>0) then
                   ! ! allocate biomass turnover pools when both pop and blaze are active
                   ! if (cable_user%call_pop) allocate(pop_to(mp), pop_cwd(mp), pop_str(mp))
                   call ini_blaze( mland, rad%latitude(landpt(:)%cstart), &
                                   rad%longitude(landpt(:)%cstart), blaze )
                   CALL MPI_Comm_rank(comm, rank, ierr)
                   BLAZE%IAM = rank

                   !par blaze restart not required uses climate data
                   allocate(latitude(mland))
                   allocate(longitude(mland))
                   call worker_blaze_types(comm, mland, blaze, blaze_restart_t, blaze_in_t, blaze_out_t)
                   !if ( .not. spinup ) then
                   !   ! print*, 'WORKER Receive 16 blaze restart'
                   !   call MPI_recv(MPI_BOTTOM, 1, blaze_restart_t, 0, ktau_gl, comm, stat, ierr)
                   !endif
                   ! cln:  burnt_area
                   if ( TRIM(blaze%burnt_area_src) == "SIMFIRE" ) then
                      call MPI_recv(MPI_BOTTOM, 1, blaze_in_t, 0, ktau_gl, comm, stat, ierr)
                      !CLN here we need to check for the SIMFIRE biome setting
                      call INI_SIMFIRE(mland, SIMFIRE, climate%modis_igbp(landpt(:)%cstart))
                      WRITE(wlogn,*)"After ini_simf"
                      CALL FLUSH(wlogn)
                      !par blaze restart not required uses climate data
                      !call worker_simfire_types(comm, mland, simfire, simfire_restart_t, simfire_inp_t, simfire_out_t)
                      !if (.not. spinup) then
                      !   ! print*, 'WORKER Receive 18 simfire restart'
                      !   call MPI_Recv(MPI_BOTTOM, 1, simfire_restart_t, 0, ktau_gl, comm, stat, ierr)
                      !endif
                   endif
                endif
             END IF ! icycle>0

             ! MPI: create inp_t type to receive input data from the master
             ! at the start of every timestep
             CALL worker_intype(comm, met, veg)

             ! MPI: casa parameters received only if cnp module is active
             ! MPI: create send_t type to send the results to the master
             ! at the end of every timestep
             CALL worker_outtype(comm, met, canopy, ssnow, rad, bal, air, soil, veg)
             if (cable_user%c13o2) call worker_c13o2_flux_type(comm, c13o2flux)

             ! MPI: casa parameters received only if cnp module is active
             ! MPI: create type to send casa results back to the master
             ! only if cnp module is active
             if (icycle>0) then
                call worker_casa_type(comm, casapool, casaflux, casamet, casabal, phen)
                ! 13C
                if (cable_user%c13o2) call worker_c13o2_pool_type(comm, c13o2pools)

                if (cable_user%casa_dump_read .or. cable_user%casa_dump_write) then
                   call worker_casa_dump_types(comm, casamet, casaflux, phen, climate, c13o2flux)
                endif

                if (cable_user%popluc) then
                   call worker_casa_LUC_types( comm, casapool, casabal, casaflux)
                   ! MPI: casa parameters received only if cnp module is active
                   ! 13C
                   if (cable_user%c13o2) then
                      call worker_c13o2_luc_type(comm, c13o2luc)
                   endif
                endif
             end if

             ! MPI: create type to send restart data back to the master
             ! only if restart file is to be created
             if (output%restart) then
                call worker_restart_type(comm, canopy, air)
             end if

             ! Open output file:
             ! MPI: only the master writes to the files
             ! CALL open_output_file( dels, soil, veg, bgc, rough )

             ssnow%otss_0   = ssnow%tgg(:,1)
             ssnow%otss     = ssnow%tgg(:,1)
             canopy%fes_cor = 0.
             canopy%fhs_cor = 0.
             met%ofsd = 0.1
             ! vh !
             met%pdep = 0.0
             if (trim(cable_user%MetType) .eq. 'cru') then
                casamet%glai = 1.0_r_2
                where (veg%iveg(:) .ge. 14) casamet%glai = 0.0_r_2
             endif

             ! CALL worker_sumcasa_types(comm, sum_casapool, sum_casaflux)
             ! ! 13C
             ! if (cable_user%c13o2) call c13o2_zero_sum_pools(sum_c13o2pools)
             ! count_sum_casa = 0

             IF (icycle>0 .AND. spincasa) THEN
                WRITE(wlogn,*) 'EXT spincasacnp enabled with mloop=', mloop
                CALL worker_spincasacnp(dels,kstart,kend,mloop,veg,soil,casabiome,casapool, &
                     casaflux,casamet,casabal,phen,POP,climate,LALLOC, &
                     c13o2flux, c13o2pools, icomm, ocomm)
                if (cable_user%c13o2) call c13o2_sanity_pools(casapool, casaflux, c13o2pools)
                SPINconv = .FALSE.
                CASAONLY = .TRUE.
                ktau_gl  = 0
                ktau     = 0
             ELSEIF ( casaonly .AND. (.NOT. spincasa) .AND. cable_user%popluc) THEN
                CALL worker_CASAONLY_LUC(dels,kstart,kend,veg,soil,casabiome,casapool, &
                     casaflux,casamet,casabal,phen,POP,climate,LALLOC, &
                     c13o2flux, c13o2pools, icomm, ocomm)
                if (cable_user%c13o2) call c13o2_sanity_pools(casapool, casaflux, c13o2pools)
                SPINconv = .FALSE.
                ktau_gl  = 0
                ktau     = 0
             ENDIF

          ELSE ! CALL1

             IF (icycle > 0) THEN
                ! re-initalise annual flux sums
                casabal%FCgppyear =0.0
                casabal%FCrpyear  =0.0
                casabal%FCnppyear =0.0
                casabal%FCrsyear  =0.0
                casabal%FCneeyear =0.0
             ENDIF

          ENDIF ! CALL1

          ! globally (WRT code) accessible kend through USE cable_common_module
          ktau_gl   = 0
          kwidth_gl = int(dels)
          kend_gl   = kend
          knode_gl  = 0

          IF (spincasa .or. casaonly) THEN
             EXIT
          ENDIF

          ! IF (.NOT.spincasa) THEN
          ! time step loop over ktau
          KTAULOOP:DO ktau=kstart, kend
             CALL CPU_TIME(etimelast)
             ! increment total timstep counter
             ktau_tot = ktau_tot + 1

             ! globally (WRT code) accessible kend through USE cable_common_module
             ktau_gl = ktau_gl + 1

             ! Check for today's day-of-year
             idoy = int(mod(real(ceiling((real(ktau+koffset))/real(ktauday))),real(loy)))
             if (idoy .eq. 0) idoy = LOY

             ! needed for CASA-CNP
             !MC - diff to serial
             nyear = INT((kend-kstart+1)/(LOY*ktauday))

             ! MPI: receive input data for this step from the master
             IF ( .NOT. CASAONLY ) THEN
                CALL MPI_Recv(MPI_BOTTOM, 1, inp_t, 0, ktau_gl, icomm, stat, ierr)
                ! MPI: receive casa_dump_data for this step from the master
             ELSEIF ( IS_CASA_TIME("dread", yyyy, ktau, kstart, koffset, &
                  ktauday, wlogn) ) THEN
                CALL MPI_Recv(MPI_BOTTOM, 1, casa_dump_t, 0, ktau_gl, icomm, stat, ierr)
             END IF

             ! Get met data and LAI, set time variables.
             ! Rainfall input may be augmented for spinup purposes:
             met%ofsd = met%fsd(:,1) + met%fsd(:,2)
             canopy%oldcansto = canopy%cansto
             ! Zero out lai where there is no vegetation acc. to veg. index
             WHERE (veg%iveg(:) .GE. 14) veg%vlai = 0.

             ! MPI: some fields need explicit init, because we don't transfer
             ! them for better performance
             ! in the serial version this is done in get_met_data
             ! after input has been read from the file
             met%tvair = met%tk
             met%tvrad = met%tk

             ! 13C
             if (cable_user%c13o2) then
                call MPI_Recv(c13o2flux%ca(1), mp, MPI_DOUBLE_PRECISION, 0, 0, icomm, stat, ierr)
             endif

             if ((ktau==1) .and. (icycle>1)) &
                  call casa_cnpflux(casaflux, casapool, casabal, .true.)

             ! Feedback prognostic vcmax and daily LAI from casaCNP to CABLE
             casa_time = IS_CASA_TIME("write", yyyy, ktau, kstart, koffset, ktauday, logn)
             liseod = mod((ktau-kstart+1),ktauday) == 0
             liseoy = mod((ktau-kstart+1)/ktauday,LOY) == 0

             IF (l_vcmaxFeedbk) THEN
                IF (MOD(ktau,ktauday) == 1) THEN
                   CALL casa_feedback( ktau, veg, casabiome, &
                        casapool, casamet, climate, ktauday )
                ENDIF
             ELSE
                veg%vcmax_shade = veg%vcmax
                veg%ejmax_shade = veg%ejmax

                veg%vcmax_sun = veg%vcmax
                veg%ejmax_sun = veg%ejmax
             ENDIF

             !MC - if (l_laiFeedbk) veg%vlai(:) = real(casamet%glai(:))
             if (l_laiFeedbk .and. (icycle>0)) veg%vlai(:) = real(casamet%glai(:))

             ! CALL land surface scheme for this timestep, all grid points:
             CALL cbm(ktau, dels, air, bgc, canopy, met, &
                  bal, rad, rough, soil, ssnow,          &
                  veg, climate)
             ! 13C
             if (cable_user%c13o2) then
                gpp  = canopy%An + canopy%Rd
                Ra   = isoratio(c13o2flux%ca, real(met%ca,r_2), 1.0_r_2)
                !MCTest
                c13o2flux%An       = canopy%An
                ! c13o2flux%An       = 1.005_r_2 * canopy%An !  * vpdbc13 / vpdbc13 ! Test 5 permil
                c13o2flux%Disc     = 0.0_r_2
                c13o2flux%Vstarch  = c13o2flux%Vstarch + 1.0e-6_r_2
                c13o2flux%Rstarch  = c13o2flux%Rstarch
                c13o2flux%Rsucrose = c13o2flux%Rsucrose
                c13o2flux%Rphoto   = c13o2flux%Rphoto
                ! do ileaf=1, mf
                !    if (cable_user%c13o2_simple_disc) then
                !       call c13o2_discrimination_simple( &
                !            ! -- Input
                !            ! isc3
                !            real(dels,r_2), canopy%isc3, &
                !            ! GPP and Leaf respiration
                !            gpp(:,ileaf), canopy%Rd(:,ileaf), &
                !            ! Ambient and stomatal CO2 concentration
                !            real(met%ca,r_2), canopy%ci(:,ileaf), &
                !            ! leaf temperature
                !            real(canopy%tlf,r_2), &
                !            ! Ambient isotope ratio
                !            Ra, &
                !            ! -- Inout
                !            ! Starch pool and its isotope ratio
                !            c13o2flux%Vstarch, c13o2flux%Rstarch, &
                !            ! -- Output
                !            ! discrimination
                !            c13o2flux%Disc(:,ileaf), &
                !            ! 13CO2 flux
                !            c13o2flux%An(:,ileaf) )
                !    else
                !       call c13o2_discrimination( &
                !            ! -- Input
                !            real(dels,r_2), canopy%isc3, &
                !            ! Photosynthesis variables
                !            ! Vcmax<< because of temperature dependence of Vcmax
                !            canopy%vcmax(:,ileaf), gpp(:,ileaf), canopy%Rd(:,ileaf), canopy%gammastar(:,ileaf), &
                !            ! CO2 concentrations
                !            real(met%ca,r_2), canopy%ci(:,ileaf), &
                !            ! Conductances
                !            canopy%gac(:,ileaf), canopy%gbc(:,ileaf), canopy%gsc(:,ileaf), &
                !            ! leaf temperature
                !            real(canopy%tlf,r_2), &
                !            ! Ambient isotope ratio
                !            Ra, &
                !            ! -- Inout
                !            ! Starch pool and isotope ratios of pools for respiration
                !            c13o2flux%Vstarch, c13o2flux%Rstarch, &
                !            c13o2flux%Rsucrose(:,ileaf), c13o2flux%Rphoto(:,ileaf), &
                !            ! -- Output
                !            ! discrimination
                !            c13o2flux%Disc(:,ileaf), &
                !            ! 13CO2 flux
                !            c13o2flux%An(:,ileaf) )
                !    endif ! cable_user%c13o2_simple_disc
                ! end do ! ileaf=1:mf
                !MCTest
             endif ! cable_user%c13o2

             !TRUNK - call of cable_climet before cbm
             if (cable_user%CALL_climate) &
                  CALL cable_climate(ktau, kstart, ktauday, idoy, LOY, met, &
                  climate, canopy, veg, ssnow, rad, dels, mp)

             ssnow%smelt  = ssnow%smelt  * dels
             ssnow%rnof1  = ssnow%rnof1  * dels
             ssnow%rnof2  = ssnow%rnof2  * dels
             ssnow%runoff = ssnow%runoff * dels

             CALL CPU_TIME(etime)
             write(wlogn,*) 'ktau, etime-etimelast ', ktau, etime-etimelast

             !jhan this is insufficient testing. condition for
             !spinup=.false. & we want CASA_dump.nc (spinConv=.true.)

             ! serial: IF ((icycle > 0) .OR. CABLE_USER%CASA_DUMP_WRITE) THEN
             IF (icycle>0) THEN
                call bgcdriver( ktau, kstart, dels, met,          &
                     ssnow, canopy, veg, soil,climate, casabiome,       &
                     casapool, casaflux, casamet, casabal,              &
                     phen, pop, ktauday, idoy, loy,   &
                     .FALSE., LALLOC, c13o2flux, c13o2pools )
                if (cable_user%c13o2) call c13o2_sanity_pools(casapool, casaflux, c13o2pools)
                write(wlogn,*) 'after bgcdriver ', MPI_BOTTOM, 1, casa_t, 0, ktau_gl, ocomm, ierr
                !TRUNK no if (liseod) then
                IF (liseod) THEN
                   CALL MPI_Send(MPI_BOTTOM, 1, casa_t, 0, ktau_gl, ocomm, ierr)
                   ! 13C
                   if (cable_user%c13o2) call MPI_Send(MPI_BOTTOM, 1, c13o2_pool_t, 0, ktau_gl, ocomm, ierr)
                   write(wlogn,*) 'after casa mpi_send ', ktau
                ENDIF

                IF (liseod) THEN
                   !call_blaze=0 if blaze off, >0 if blaze on to some extent
                   IF ( cable_user%CALL_BLAZE>0) THEN
                      CALL BLAZE_ACCOUNTING(BLAZE, climate, ktau, dels, YYYY, idoy)
                      call blaze_driver(blaze%ncells, blaze, simfire, casapool, casaflux, &
                           casamet, climate, rshootfrac, idoy, YYYY, 1, POP, veg)
                      !par send blaze_out back
                      CALL MPI_Send(MPI_BOTTOM, 1, blaze_out_t, 0, ktau_gl, ocomm, ierr)
                   ENDIF
                ENDIF

                IF (casa_time) THEN
                   write(wlogn,*) 'IN IS_CASA ', casapool%cplant(:,1)
                ENDIF

                ! MPI: send the results back to the master
                IF ( ((.NOT.spinup) .OR. (spinup.AND.spinConv)) .AND. &
                     IS_CASA_TIME("dwrit", yyyy, ktau, kstart, &
                     koffset, ktauday, wlogn) ) then
                   CALL MPI_Send(MPI_BOTTOM, 1, casa_dump_t, 0, ktau_gl, ocomm, ierr)
                endif

             ENDIF ! icycle>0

             if (.not. casaonly) then
                ! sumcflux is pulled out of subroutine cbm
                ! so that casaCNP can be called before adding the fluxes (Feb 2008, YP)
                call sumcflux( ktau, kstart, dels, &
                     canopy, sum_flux,             &
                     casaflux, l_vcmaxFeedbk )
                write(wlogn,*) 'after sumcflux ', ktau
             endif
             ! MPI: send the results back to the master
             CALL MPI_Send(MPI_BOTTOM, 1, send_t, 0, ktau_gl, ocomm, ierr)
             if (cable_user%c13o2) call MPI_Send(MPI_BOTTOM, 1, c13o2_flux_t, 0, ktau_gl, ocomm, ierr)

             ! Write time step's output to file if either: we're not spinning up
             ! or we're spinning up and the spinup has converged:
             ! MPI: writing done only by the master
             ! IF ((.NOT.spinup).OR.(spinup.AND.spinConv)) &
             !     CALL write_output(dels, ktau, met, canopy, ssnow, &
             !                       rad, bal, air, soil, veg, C%SBOLTZ, &
             !                       C%EMLEAF, C%EMSOIL)

             CALL1 = .FALSE.
             write(wlogn,*) 'after send results back to master ', ktau

          END DO KTAULOOP ! END Do loop over timestep ktau
          ! ELSE

          !    CALL1 = .FALSE.
          ! ENDIF

          flush(wlogn)

          IF (icycle >0 .and. cable_user%CALL_POP) THEN

             IF (CABLE_USER%POPLUC) THEN
                write(wlogn,*) 'before MPI_Send casa_LUC'
                ! worker sends casa updates required for LUC calculations here
                CALL MPI_Send(MPI_BOTTOM, 1, casa_LUC_t, 0, 0, ocomm, ierr)
                ! 13C
                if (cable_user%c13o2) then
                   call MPI_Send(MPI_BOTTOM, 1, c13o2_flux_t, 0, 0, ocomm, ierr)
                   call MPI_Send(MPI_BOTTOM, 1, c13o2_pool_t, 0, 0, ocomm, ierr)
                   call MPI_Send(MPI_BOTTOM, 1, c13o2_luc_t, 0, 0, ocomm, ierr)
                endif
                write(wlogn,*) 'after MPI_Send casa_LUC'
                ! master calls LUCDriver here
                ! worker receives casa and POP updates
                CALL MPI_Recv( POP%pop_grid(1), POP%np, pop_t, 0, 0, icomm, stat, ierr )
             ENDIF
             ! one annual time-step of POP
             !write(wlogn,*) 'laimax',  casabal%LAImax
             CALL POPdriver(casaflux, casabal, veg, POP)

             ! Call BLAZE again to compute turnovers depending on POP mortalities
             ! call_blaze=0 if blaze off, >0 if blaze on to some extent
             IF ( cable_user%CALL_BLAZE>0) THEN
                !MC - this is different to serial code
                call blaze_driver(blaze%ncells, blaze, simfire, casapool, casaflux, &
                     casamet, climate, rshootfrac, idoy, YYYY, 1, POP, veg)
             ENDIF

             CALL worker_send_pop(POP, ocomm)

             IF (CABLE_USER%POPLUC) then
                CALL MPI_Recv(MPI_BOTTOM, 1, casa_LUC_t, 0, nyear, icomm, stat, ierr)
                ! 13C
                if (cable_user%c13o2) then
                   call MPI_Recv(MPI_BOTTOM, 1, c13o2_flux_t, 0, nyear, icomm, stat, ierr)
                   call MPI_Recv(MPI_BOTTOM, 1, c13o2_pool_t, 0, nyear, icomm, stat, ierr)
                   call MPI_Recv(MPI_BOTTOM, 1, c13o2_luc_t, 0, nyear, icomm, stat, ierr)
                endif
             ENDIF

          ENDIF ! icycle >0 .and. cable_user%CALL_POP

          ! IF ( ((.NOT.spinup).OR.(spinup.AND.spinConv)).AND. &
          !      CABLE_USER%CALL_POP) THEN
          !    CONTINUE
          !    !CALL worker_send_pop(POP, ocomm)
          ! ENDIF

          IF ((icycle.gt.0) .AND. (.NOT.casaonly)) THEN
             ! re-initalise annual flux sums
             casabal%FCgppyear = 0.0_r_2
             casabal%FCrpyear  = 0.0_r_2
             casabal%FCnppyear = 0.0_r_2
             casabal%FCrsyear  = 0.0_r_2
             casabal%FCneeyear = 0.0_r_2
          ENDIF

       END DO YEARLOOP

       IF (spincasa .or. casaonly) THEN
          EXIT
       ENDIF

       ! MPI: learn from the master whether it's time to quit
       CALL MPI_Bcast(loop_exit, 1, MPI_LOGICAL, 0, comm, ierr)

       IF (loop_exit) THEN
          EXIT
       END IF

    END DO SPINLOOP

    IF (icycle > 0 .and. (.not.spincasa).and. (.not.casaonly)) THEN
       ! MPI: send casa results back to the master
       CALL MPI_Send(MPI_BOTTOM, 1, casa_t, 0, ktau_gl, ocomm, ierr)
       ! 13C
       if (cable_user%c13o2) then
          call MPI_Send(MPI_BOTTOM, 1, c13o2_flux_t, 0, ktau_gl, ocomm, ierr)
          call MPI_Send(MPI_BOTTOM, 1, c13o2_pool_t, 0, ktau_gl, ocomm, ierr)
       endif
       !call_blaze=0 if blaze off, >0 if blaze on to some extent
       if (cable_user%call_blaze>0) then
         !par send blaze_out back
         CALL MPI_Send(MPI_BOTTOM, 1, blaze_out_t, 0, ktau_gl, ocomm, ierr)
       end if
    END IF

    ! Write restart file if requested:
    IF (output%restart .AND. (.NOT. CASAONLY)) THEN
       ! MPI: send variables that are required by create_restart
       CALL MPI_Send(MPI_BOTTOM, 1, restart_t, 0, ktau_gl, comm, ierr)
       ! MPI: output file written by master only
       if (cable_user%CALL_climate) then
          CALL MPI_Send(MPI_BOTTOM, 1, climate_t, 0, ktau_gl, comm, ierr)
       endif
    END IF

    ! MPI: cleanup
    CALL worker_end(icycle, output%restart)

    ! Close log file
    ! MPI: closes handle to /dev/null in workers
    CLOSE(wlogn)

    RETURN

  END SUBROUTINE mpidrv_worker



  ! ============== PRIVATE SUBROUTINES USED ONLY BY THE MPI WORKERS ===============


  ! MPI: receives grid decomposition info from the master
  SUBROUTINE worker_decomp(comm)

    use mpi
    USE cable_def_types_mod, ONLY: mland, mp

    IMPLICIT NONE

    INTEGER, INTENT(IN) :: comm ! MPI communicator to talk to the workers

    INTEGER :: stat(MPI_STATUS_SIZE), ierr

    ! receive number of landpoints assigned to this worker
    CALL MPI_Recv(mland, 1, MPI_INTEGER, 0, 0, comm, stat, ierr)

    ! receive number of land patches assigned to this worker
    CALL MPI_Recv(mp, 1, MPI_INTEGER, 0, 0, comm, stat, ierr)

    RETURN

  END SUBROUTINE worker_decomp


  ! MPI: creates param_t type for the worker to receive the default parameters
  ! from the master process
  ! then receives the parameters
  ! and finally frees the MPI type
  SUBROUTINE worker_cable_params(comm,met,air,ssnow,veg,bgc,soil,canopy,&
       rough,rad,sum_flux,bal)

    use mpi

    USE cable_def_types_mod
    USE cable_IO_vars_module
    USE cable_input_module, ONLY: allocate_cable_vars
    USE cable_common_module,  ONLY: calcsoilalbedo

    IMPLICIT NONE

    ! subroutine arguments

    INTEGER, INTENT(IN) :: comm ! MPI communicator

    TYPE (met_type), INTENT(OUT) :: met
    TYPE (air_type), INTENT(OUT) :: air
    TYPE (soil_snow_type), INTENT(OUT) :: ssnow
    TYPE (veg_parameter_type), INTENT(OUT)  :: veg
    TYPE (bgc_pool_type), INTENT(OUT)  :: bgc
    TYPE (soil_parameter_type), INTENT(OUT) :: soil
    TYPE (canopy_type), INTENT(OUT)    :: canopy
    TYPE (roughness_type), INTENT(OUT) :: rough
    TYPE (radiation_type),INTENT(OUT)  :: rad
    TYPE (sum_flux_type), INTENT(OUT)  :: sum_flux
    TYPE (balances_type), INTENT(OUT)  :: bal


    ! local vars

    ! temp arrays for marshalling all fields into a single struct
    INTEGER, ALLOCATABLE, DIMENSION(:) :: blen
    INTEGER(KIND=MPI_ADDRESS_KIND), ALLOCATABLE, DIMENSION(:) :: displs
    INTEGER, ALLOCATABLE, DIMENSION(:) :: types

    ! temp vars for verifying block number and total length of inp_t
    INTEGER(KIND=MPI_ADDRESS_KIND) :: text, tmplb
    INTEGER :: tsize

    INTEGER :: stat(MPI_STATUS_SIZE), ierr
    INTEGER :: landp_t, patch_t, param_ts

    INTEGER :: r1len, r2len, i1len, llen ! block lengths
    INTEGER :: bidx ! block index
    INTEGER :: ntyp ! total number of blocks

    INTEGER :: rank, ierr2, rcount, pos, offset

    CHARACTER, DIMENSION(:), ALLOCATABLE :: rbuf

    CALL MPI_Comm_rank(comm, rank, ierr)

    ! mp and mland should have been received previously by
    ! worker_decomp

    ! creates types to receive slices of landpt and patch arrays from the master
    CALL decomp_types (landp_t, patch_t)

    ! Allocate spatial heterogeneity variables:
    ALLOCATE(landpt(mland))

    ! and receive own slice from the master
    CALL MPI_Recv (landpt, mland, landp_t, 0, 0, comm, stat, ierr)

    !adjust cstart & cend for worker slice
    offset=landpt(1)%cstart
    landpt(:)%cstart=landpt(:)%cstart-offset+1
    landpt(:)%cend=landpt(:)%cend-offset+1

    CALL allocate_cable_vars(air,bgc,canopy,met,bal,rad,rough,soil,ssnow, &
         sum_flux,veg,mp)

    ! receive slice of patch array that was allocated above inside
    ! allocate_cable_vars
    CALL MPI_Recv (patch, mp, patch_t, 0, 0, comm, stat, ierr)

    ! MPI: TODO: probably not a bad idea to free landp_t and patch_t types

    ntyp = nparam

    ! ntyp increases if include ... Ticket #27
    IF (calcsoilalbedo) THEN
       ntyp = nparam + 1
    END IF

    ALLOCATE (blen(ntyp))
    ALLOCATE (displs(ntyp))
    ALLOCATE (types(ntyp))

    ! default type is byte, to be overriden for multi-D types
    types = MPI_BYTE

    r1len = mp * extr1
    r2len = mp * extr2
    i1len = mp * extid
    llen = mp * extl

    bidx = 0

    ! the order of variables follows argument list
    ! the order of fields within follows alloc_*_type subroutines

    ! ----------- met --------------

    bidx = bidx + 1
    CALL MPI_Get_address (met%ca, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (met%year, displs(bidx), ierr)
    blen(bidx) = i1len

    bidx = bidx + 1
    CALL MPI_Get_address (met%moy, displs(bidx), ierr)
    blen(bidx) = i1len

    bidx = bidx + 1
    CALL MPI_Get_address (met%doy, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (met%hod, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (met%fsd, displs(bidx), ierr)
    blen(bidx) = swb * r1len

    bidx = bidx + 1
    CALL MPI_Get_address (met%fld, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (met%precip, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (met%precip_sn, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (met%tk, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (met%tvair, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (met%tvrad, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (met%pmb, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (met%ua, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (met%qv, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (met%qvair, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (met%da, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (met%dva, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (met%coszen, displs(bidx), ierr)
    blen(bidx) = r1len


    ! ----------- air --------------

    bidx = bidx + 1
    CALL MPI_Get_address (air%rho, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (air%volm, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (air%rlam, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (air%qsat, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (air%epsi, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (air%visc, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (air%psyc, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (air%dsatdk, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (air%cmolar, displs(bidx), ierr)
    blen(bidx) = r1len


    ! ----------- ssnow --------------

    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%dtmlt, displs(bidx), ierr)
    blen(bidx) = 3 * r1len

    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%pudsto, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%pudsmx, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%albsoilsn, displs(bidx), ierr)
    blen(bidx) = nrb * r1len

    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%cls, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%dfn_dtg, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%dfh_dtg, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%dfe_ddq, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%ddq_dtg, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%evapsn, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%fwtop, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%fwtop1, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%fwtop2, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%fwtop3, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%gammzz, displs(bidx), ierr)
    blen(bidx) = ms * r2len

    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%isflag, displs(bidx), ierr)
    blen(bidx) = i1len

    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%osnowd, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%potev, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (soil%pwb_min, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%runoff, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%rnof1, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%rnof2, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%rtsoil, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%sconds, displs(bidx), ierr)
    blen(bidx) = msn * r1len

    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%sdepth, displs(bidx), ierr)
    blen(bidx) = msn * r1len

    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%smass, displs(bidx), ierr)
    blen(bidx) = msn * r1len

    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%snage, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%snowd, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%smelt, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%ssdn, displs(bidx), ierr)
    blen(bidx) = msn * r1len

    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%ssdnn, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%tgg, displs(bidx), ierr)
    blen(bidx) = ms * r1len

    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%tggsn, displs(bidx), ierr)
    blen(bidx) = msn * r1len

    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%tss, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%wb, displs(bidx), ierr)
    blen(bidx) = ms * r2len

    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%wbfice, displs(bidx), ierr)
    blen(bidx) = ms * r2len

    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%wbice, displs(bidx), ierr)
    blen(bidx) = ms * r2len

    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%wblf, displs(bidx), ierr)
    blen(bidx) = ms * r2len

    ! additional for sli
    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%S, displs(bidx), ierr)
    blen(bidx) = ms * r2len

    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%Tsoil, displs(bidx), ierr)
    blen(bidx) = ms * r2len

    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%thetai, displs(bidx), ierr)
    blen(bidx) = ms * r2len

    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%snowliq, displs(bidx), ierr)
    blen(bidx) = msn * r2len

    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%Tsurface, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%h0, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%nsnow, displs(bidx), ierr)
    blen(bidx) = I1len

    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%lE, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%zdelta, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address(ssnow%rex, displs(bidx), ierr)
    blen(bidx) = ms * r2len
    ! end additional for sli

    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%wbtot, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%wb_lake, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%sinfil, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%evapfbl, displs(bidx), ierr)
    blen(bidx) = ms * r1len

    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%qstss, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%wetfac, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%owetfac, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%t_snwlr, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%tggav, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%otss, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%otss_0, displs(bidx), ierr)
    blen(bidx) = r1len

    ! ----------- veg --------------

    bidx = bidx + 1
    CALL MPI_Get_address (veg%canst1, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (veg%dleaf, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (veg%ejmax, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (veg%ejmax_shade, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (veg%ejmax_sun, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (veg%frac4, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (veg%froot, displs(bidx), ierr)
    blen(bidx) = ms * r1len

    bidx = bidx + 1
    CALL MPI_Get_address (veg%hc, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (veg%iveg, displs(bidx), ierr)
    blen(bidx) = i1len

    bidx = bidx + 1
    CALL MPI_Get_address (veg%ivegp, displs(bidx), ierr)
    blen(bidx) = i1len

    bidx = bidx + 1
    CALL MPI_Get_address (veg%ilu, displs(bidx), ierr)
    blen(bidx) = i1len

    bidx = bidx + 1
    CALL MPI_Get_address (veg%meth, displs(bidx), ierr)
    ! Maciej: veg%meth is REAL
    !    blen(bidx) = i1len
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (veg%rp20, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (veg%rpcoef, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (veg%shelrb, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (veg%wai, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (veg%vegcf, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (veg%tminvj, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (veg%tmaxvj, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (veg%vbeta, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (veg%xalbnir, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (veg%vcmax, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (veg%vcmax_sun, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (veg%vcmax_shade, displs(bidx), ierr)
    blen(bidx) = r1len

    !  bidx = bidx + 1
    !  CALL MPI_Get_address (veg%vlai, displs(bidx), ierr)
    !  blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (veg%xfang, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (veg%extkn, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (veg%deciduous, displs(bidx), ierr)
    ! Maciej: deciduous is logical
    !    blen(bidx) = r1len
    blen(bidx) = llen

    bidx = bidx + 1
    CALL MPI_Get_address (veg%a1gs, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (veg%d0gs, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (veg%alpha, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (veg%convex, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (veg%cfrd, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (veg%gswmin, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (veg%conkc0, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (veg%conko0, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (veg%ekc, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (veg%eko, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (veg%clitt, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (veg%zr, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (veg%gamma, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (veg%refl, displs(bidx), ierr)
    blen(bidx) = 2 * r1len

    bidx = bidx + 1
    CALL MPI_Get_address (veg%taul, displs(bidx), ierr)
    blen(bidx) = 2 * r1len

    bidx = bidx + 1
    CALL MPI_Get_address (veg%disturbance_interval, displs(bidx), ierr)
    blen(bidx) = 2 * i1len

    bidx = bidx + 1
    CALL MPI_Get_address (veg%disturbance_intensity, displs(bidx), ierr)
    ! Maciej: disturbance_intensity is REAL(r_2)
    !    blen(bidx) = 2 * r1len
    blen(bidx) = 2 * r2len

    ! Ticket #56, adding new veg parms
    bidx = bidx + 1
    CALL MPI_Get_address (veg%g0, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (veg%g1, displs(bidx), ierr)
    blen(bidx) = r1len
    ! Ticket #56, finish adding new veg parms

    bidx = bidx + 1
    CALL MPI_Get_address (veg%vcmaxcc, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (veg%ejmaxcc, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (veg%gmmax, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (veg%gm, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (veg%c4kci, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (veg%c4kcc, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (veg%bjv, displs(bidx), ierr)
    blen(bidx) = r1len


    ! ----------- bgc --------------

    bidx = bidx + 1
    CALL MPI_Get_address (bgc%cplant, displs(bidx), ierr)
    blen(bidx) = ncp * r1len

    bidx = bidx + 1
    CALL MPI_Get_address (bgc%csoil, displs(bidx), ierr)
    blen(bidx) = ncs * r1len

    bidx = bidx + 1
    CALL MPI_Get_address (bgc%ratecp, displs(bidx), ierr)
    blen(bidx) = ncp * extr1

    bidx = bidx + 1
    CALL MPI_Get_address (bgc%ratecs, displs(bidx), ierr)
    blen(bidx) = ncs * extr1

    ! ----------- soil --------------

    bidx = bidx + 1
    CALL MPI_Get_address (soil%albsoil, displs(bidx), ierr)
    blen(bidx) = nrb * r1len

    bidx = bidx + 1
    CALL MPI_Get_address (soil%bch, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (soil%c3, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (soil%clay, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (soil%cnsd, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (soil%css, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (soil%hsbh, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (soil%hyds, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (soil%i2bp3, displs(bidx), ierr)
    ! Maciej: i2bp3 is REAL
    !    blen(bidx) = i1len
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (soil%ibp2, displs(bidx), ierr)
    ! Maciej: ibp2 is REAL
    !    blen(bidx) = i1len
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (soil%isoilm, displs(bidx), ierr)
    ! Maciej isoilm is INTEGER
    !    blen(bidx) = r1len
    blen(bidx) = i1len

    bidx = bidx + 1
    CALL MPI_Get_address (soil%rhosoil, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (veg%rs20, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (soil%sand, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (soil%sfc, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (soil%silt, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (soil%ssat, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (soil%sucs, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (soil%swilt, displs(bidx), ierr)
    blen(bidx) = r1len

    ! extra for sli
    bidx = bidx + 1
    CALL MPI_Get_address (soil%zeta, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (soil%fsatmax, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (soil%ishorizon, displs(bidx), ierr)
    blen(bidx) = ms * i1len

    ! end extra for sil
    bidx = bidx + 1
    CALL MPI_Get_address (soil%zse, displs(bidx), ierr)
    blen(bidx) = ms * extr1

    bidx = bidx + 1
    CALL MPI_Get_address (soil%zshh, displs(bidx), ierr)
    blen(bidx) = (ms + 1) * extr1

    ! pass soilcolour albedo as well if including Ticket #27
    IF (calcsoilalbedo) THEN
       bidx = bidx + 1
       CALL MPI_Get_address (soil%soilcol, displs(bidx), ierr)
       blen(bidx) = r1len
    END IF

    ! ----------- canopy --------------

    bidx = bidx + 1
    CALL MPI_Get_address (canopy%fess, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (canopy%fesp, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (canopy%cansto, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (canopy%oldcansto, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (canopy%cduv, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (canopy%delwc, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (canopy%dewmm, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (canopy%dgdtg, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (canopy%fe, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (canopy%fh, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (canopy%fpn, displs(bidx), ierr)
    blen(bidx) = r1len


!!$    bidx = bidx + 1
!!$    CALL MPI_Get_address (canopy%A_sh, displs(bidx), ierr)
!!$    blen(bidx) = r2len
!!$
!!$    bidx = bidx + 1
!!$    CALL MPI_Get_address (canopy%A_sl, displs(bidx), ierr)
!!$    blen(bidx) = r2len
!!$
!!$    bidx = bidx + 1
!!$    CALL MPI_Get_address (canopy%A_slC, displs(bidx), ierr)
!!$    blen(bidx) = r2len
!!$
!!$    bidx = bidx + 1
!!$    CALL MPI_Get_address (canopy%A_shC, displs(bidx), ierr)
!!$    blen(bidx) = r2len
!!$
!!$    bidx = bidx + 1
!!$    CALL MPI_Get_address (canopy%A_slJ, displs(bidx), ierr)
!!$    blen(bidx) = r2len
!!$
!!$
!!$    bidx = bidx + 1
!!$    CALL MPI_Get_address (canopy%A_shJ, displs(bidx), ierr)
!!$    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (canopy%frp, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (canopy%frpw, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (canopy%frpr, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (canopy%frs, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (canopy%fnee, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (canopy%frday, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (canopy%fnv, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (canopy%fev, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (canopy%fevc, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (canopy%fevw, displs(bidx), ierr)
    blen(bidx) = r1len

    !  bidx = bidx + 1
    !  CALL MPI_Get_address (canopy%potev_c, displs(bidx), ierr)
    !  blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (canopy%fhv, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (canopy%fhvw, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (canopy%fns, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (canopy%fes, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (canopy%ofes, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (canopy%fhs, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (canopy%fwet, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (canopy%ga, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (canopy%ghflux, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (canopy%precis, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (canopy%qscrn, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (canopy%rnet, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (canopy%segg, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (canopy%sghflux, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (canopy%spill, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (canopy%through, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (canopy%tscrn, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (canopy%tv, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (canopy%us, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (canopy%uscrn, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (canopy%vlaiw, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (canopy%rghlai, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (canopy%wcint, displs(bidx), ierr)
    blen(bidx) = r1len



    !  bidx = bidx + 1
    !  CALL MPI_Get_address (canopy%rwater, displs(bidx), ierr)
    !  blen(bidx) = ms * r1len

    bidx = bidx + 1
    CALL MPI_Get_address (canopy%evapfbl, displs(bidx), ierr)
    ! MPI: gol124: changed to r1 when Bernard ported to CABLE_r491
    blen(bidx) = ms * r1len

    bidx = bidx + 1
    CALL MPI_Get_address (canopy%epot, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (canopy%fnpp, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (canopy%fevw_pot, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (canopy%gswx_T, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (canopy%cdtq, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (canopy%wetfac_cs, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (canopy%fwsoil, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (canopy%gswx, displs(bidx), ierr)
    blen(bidx) = mf * r1len

    bidx = bidx + 1
    CALL MPI_Get_address (canopy%zetar, displs(bidx), ierr)
    blen(bidx) = niter * r1len

    ! 13C
    bidx = bidx + 1
    call MPI_Get_address(canopy%An, displs(bidx), ierr)
    blen(bidx) = mf * r2len

    bidx = bidx + 1
    call MPI_Get_address(canopy%Rd, displs(bidx), ierr)
    blen(bidx) = mf * r2len

    bidx = bidx + 1
    call MPI_Get_address(canopy%isc3, displs(bidx), ierr)
    blen(bidx) = llen

    bidx = bidx + 1
    call MPI_Get_address(canopy%vcmax, displs(bidx), ierr)
    blen(bidx) = mf * r2len

    bidx = bidx + 1
    call MPI_Get_address(canopy%gammastar, displs(bidx), ierr)
    blen(bidx) = mf * r2len

    bidx = bidx + 1
    call MPI_Get_address(canopy%gsc, displs(bidx), ierr)
    blen(bidx) = mf * r2len

    bidx = bidx + 1
    call MPI_Get_address(canopy%gbc, displs(bidx), ierr)
    blen(bidx) = mf * r2len

    bidx = bidx + 1
    call MPI_Get_address(canopy%gac, displs(bidx), ierr)
    blen(bidx) = mf * r2len

    bidx = bidx + 1
    call MPI_Get_address(canopy%ci, displs(bidx), ierr)
    blen(bidx) = mf * r2len

    ! ------- rough -------

    bidx = bidx + 1
    CALL MPI_Get_address (rough%coexp, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (rough%disp, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (rough%hruff, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (rough%hruff_grmx, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (rough%rt0us, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (rough%rt1usa, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (rough%rt1usb, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (rough%rt1, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (rough%term2, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (rough%term3, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (rough%term5, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (rough%term6, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (rough%usuh, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (rough%za_uv, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (rough%za_tq, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (rough%z0m, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (rough%zref_uv, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (rough%zref_tq, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (rough%zruffs, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (rough%z0soilsn, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (rough%z0soil, displs(bidx), ierr)
    blen(bidx) = r1len

    ! --------rad --------

    bidx = bidx + 1
    CALL MPI_Get_address (rad%albedo, displs(bidx), ierr)
    blen(bidx) = nrb * r1len

    bidx = bidx + 1
    CALL MPI_Get_address (rad%extkb, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (rad%extkd2, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (rad%extkd, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (rad%flws, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (rad%fvlai, displs(bidx), ierr)
    blen(bidx) = mf * r1len

    bidx = bidx + 1
    CALL MPI_Get_address (rad%gradis, displs(bidx), ierr)
    blen(bidx) = mf * r1len

    bidx = bidx + 1
    CALL MPI_Get_address (rad%latitude, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (rad%lwabv, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (rad%qcan, displs(bidx), ierr)
    blen(bidx) = mf * nrb * r1len

    bidx = bidx + 1
    CALL MPI_Get_address (rad%qssabs, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (rad%rhocdf, displs(bidx), ierr)
    blen(bidx) = nrb * r1len

    bidx = bidx + 1
    CALL MPI_Get_address (rad%rniso, displs(bidx), ierr)
    blen(bidx) = mf * r1len

    bidx = bidx + 1
    CALL MPI_Get_address (rad%scalex, displs(bidx), ierr)
    blen(bidx) = mf * r1len

    bidx = bidx + 1
    CALL MPI_Get_address (rad%transd, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (rad%trad, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (rad%reffdf, displs(bidx), ierr)
    blen(bidx) = nrb * r1len

    bidx = bidx + 1
    CALL MPI_Get_address (rad%reffbm, displs(bidx), ierr)
    blen(bidx) = nrb * r1len

    bidx = bidx + 1
    CALL MPI_Get_address (rad%extkbm, displs(bidx), ierr)
    blen(bidx) = nrb * r1len

    bidx = bidx + 1
    CALL MPI_Get_address (rad%extkdm, displs(bidx), ierr)
    blen(bidx) = nrb * r1len

    bidx = bidx + 1
    CALL MPI_Get_address (rad%fbeam, displs(bidx), ierr)
    blen(bidx) = nrb * r1len

    bidx = bidx + 1
    CALL MPI_Get_address (rad%cexpkbm, displs(bidx), ierr)
    ! Maciej: cexpkbm is mp*swb
    !    blen(bidx) = nrb * r1len
    blen(bidx) = swb * r1len

    bidx = bidx + 1
    CALL MPI_Get_address (rad%cexpkdm, displs(bidx), ierr)
    ! Maciej: cexpkdm is mp*swb
    !    blen(bidx) = nrb * r1len
    blen(bidx) = swb * r1len

    bidx = bidx + 1
    CALL MPI_Get_address (rad%rhocbm, displs(bidx), ierr)
    ! Maciej: rhocbm is mp*nrb
    !    blen(bidx) = swb * r1len
    blen(bidx) = nrb * r1len

    bidx = bidx + 1
    CALL MPI_Get_address (rad%transb, displs(bidx), ierr)
    blen(bidx) = r1len

    ! ------- sum_flux -----

    bidx = bidx + 1
    CALL MPI_Get_address (sum_flux%sumpn, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (sum_flux%sumrp, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (sum_flux%sumrpw, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (sum_flux%sumrpr, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (sum_flux%sumrs, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (sum_flux%sumrd, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (sum_flux%dsumpn, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (sum_flux%dsumrp, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (sum_flux%dsumrs, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (sum_flux%dsumrd, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (sum_flux%sumxrp, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (sum_flux%sumxrs, displs(bidx), ierr)
    blen(bidx) = r1len

    ! ------- bal ----

    bidx = bidx + 1
    CALL MPI_Get_address (bal%drybal, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (bal%ebal, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (bal%ebal_tot, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (bal%ebal_cncheck, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (bal%ebal_tot_cncheck, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (bal%evap_tot, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (bal%osnowd0, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (bal%precip_tot, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (bal%rnoff_tot, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (bal%wbal, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (bal%wbal_tot, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (bal%wbtot0, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (bal%wetbal, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (bal%owbtot, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (bal%evapc_tot, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (bal%evaps_tot, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (bal%rnof1_tot, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (bal%rnof2_tot, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (bal%snowdc_tot, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (bal%wbal_tot1, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (bal%delwc_tot, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (bal%qasrf_tot, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (bal%qfsrf_tot, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (bal%qssrf_tot, displs(bidx), ierr)
    blen(bidx) = r1len

    ! additional field missing from previous versions;
    ! added when trying to fix a bug in the new mpi code
    ! the order of these new fields follows the order of their
    ! declaration in cable_define_types.F90

    bidx = bidx + 1
    CALL MPI_Get_address (bal%ebaltr, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (bal%ebal_tottr, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (bal%cansto0, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%iantrct, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%tss_p, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%deltss, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%owb1, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%wbtot1, displs(bidx), ierr)
    blen(bidx) = r1len

    !    Maciej: duplicate!
    !    bidx = bidx + 1
    !    CALL MPI_Get_address (ssnow%wbtot1, displs(bidx), ierr)
    !    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%tprecip, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%tevap, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%trnoff, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%totenbal, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%totenbal2, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%fland, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%ifland, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%tilefrac, displs(bidx), ierr)
    blen(bidx) = n_tiles * r1len

    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%qasrf, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%qfsrf, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%qssrf, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (veg%vlaimax, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (rad%albedo_T, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (rad%longitude, displs(bidx), ierr)
    blen(bidx) = r1len

    ! MPI: sanity check
    IF (bidx /= ntyp) THEN
       WRITE (*,*) 'worker ',rank,' invalid number of param_ts fields',bidx,', fix it (20)!'
       CALL MPI_Abort(comm, 54, ierr)
    END IF

    CALL MPI_Type_create_struct (bidx, blen, displs, types, param_ts, ierr)
    CALL MPI_Type_commit (param_ts, ierr)

    CALL MPI_Type_size (param_ts, tsize, ierr)
    CALL MPI_Type_get_extent (param_ts, tmplb, text, ierr)

    WRITE (*,*) 'worker param_ts blocks, size, extent and lb: ',rank,bidx,tsize,text,tmplb

    ! MPI: check whether total size of received data equals total
    ! data sent by all the workers
    CALL MPI_Reduce (tsize, MPI_DATATYPE_NULL, 1, MPI_INTEGER, MPI_SUM, 0, comm, ierr)

    DEALLOCATE(types)
    DEALLOCATE(displs)
    DEALLOCATE(blen)

    ! if anything went wrong the master will mpi_abort
    ! which mpi_recv below is going to catch...

    ! so, now receive all the parameters
    !    CALL MPI_Recv (MPI_BOTTOM, 1, param_ts, 0, 0, comm, stat, ierr)
    !   Maciej: buffered recv + unpac version
    ALLOCATE (rbuf(tsize))
    CALL MPI_Recv (rbuf, tsize, MPI_BYTE, 0, 0, comm, stat, ierr)
    CALL MPI_Get_count (stat, param_ts, rcount, ierr2)

    IF (ierr == MPI_SUCCESS .AND. ierr2 == MPI_SUCCESS .AND. rcount == 1) THEN
       pos = 0
       CALL MPI_Unpack (rbuf, tsize, pos, MPI_BOTTOM, rcount, param_ts, &
            comm, ierr)
       IF (ierr /= MPI_SUCCESS) write(*,*) 'cable param unpack error, rank: ', rank, ierr
    ELSE
       write(*,*) 'cable param recv rank err err2 rcount: ', rank, ierr, ierr2, rcount
    END IF

    DEALLOCATE(rbuf)

    ! finally free the MPI type
    CALL MPI_Type_Free(param_ts, ierr)

    ! all CABLE parameters have been received from the master by now
    RETURN

  END SUBROUTINE worker_cable_params


  ! MPI: creates param_t type for the worker to receive the default casa
  ! parameters from the master process
  ! then receives them
  ! and finally frees the MPI type
  SUBROUTINE worker_casa_params(comm, casabiome, casapool, casaflux, casamet, casabal, phen)

    use mpi
    use cable_def_types_mod
    use casavariable
    use phenvariable

    implicit none

    ! sub arguments
    integer,             intent(in)  :: comm  ! MPI communicator
    ! TODO: have these variables been already allocated?
    type(casa_biome),    intent(out) :: casabiome
    type(casa_pool),     intent(out) :: casapool
    type(casa_flux),     intent(out) :: casaflux
    type(casa_met),      intent(out) :: casamet
    type(casa_balance),  intent(out) :: casabal
    type(phen_variable), intent(out) :: phen

    ! local vars

    ! temp arrays for marshalling all fields into a single struct
    integer, allocatable, dimension(:) :: blen
    integer(kind=MPI_ADDRESS_KIND), allocatable, dimension(:) :: displs
    integer, allocatable, dimension(:) :: types

    ! temp vars for verifying block number and total length of inp_t
    integer(kind=MPI_ADDRESS_KIND) :: text, tmplb
    integer :: tsize

    integer :: stat(MPI_STATUS_SIZE), ierr
    integer :: casa_ts

    integer :: r1len, r2len, i1len, llen ! block lengths
    integer :: bidx ! block index
    integer :: ntyp ! total number of blocks

    integer :: rank, off, ierr2, rcount, pos

    character, dimension(:), allocatable :: rbuf

    off = 1

    call MPI_Comm_rank(comm, rank, ierr)

    if (.not. associated(casabiome%ivt2)) then
       write (*,*) 'worker alloc casa and phen var with m patches: ', rank, mp
       CALL alloc_casavariable(casabiome, mp)
       CALL alloc_casavariable(casapool, mp)
       CALL alloc_casavariable(casaflux, mp)
       CALL alloc_casavariable(casamet, mp)
       CALL alloc_casavariable(casabal, mp)
       call alloc_phenvariable(phen, mp)
       call zero_casavariable(casabiome)
       call zero_casavariable(casapool)
       call zero_casavariable(casaflux)
       call zero_casavariable(casamet)
       call zero_casavariable(casabal)
       call zero_phenvariable(phen)
    end if

    ntyp = ncasaparam

    allocate(blen(ntyp))
    allocate(displs(ntyp))
    allocate(types(ntyp))

    ! default type is byte, to be overriden for multi-D types
    types = MPI_BYTE

    r1len = mp * extr1
    r2len = mp * extr2
    i1len = mp * extid
    llen  = mp * extl

    bidx = 0

    ! ------- casabiome -----

    bidx = bidx + 1
    CALL MPI_Get_address (casabiome%ivt2, displs(bidx), ierr)
    blen(bidx) = mvtype * extid

    bidx = bidx + 1
    CALL MPI_Get_address (casabiome%xkleafcoldmax, displs(bidx), ierr)
    blen(bidx) = mvtype * extr2

    bidx = bidx + 1
    CALL MPI_Get_address (casabiome%xkleafcoldexp, displs(bidx), ierr)
    blen(bidx) = mvtype * extr2

    bidx = bidx + 1
    CALL MPI_Get_address (casabiome%xkleafdrymax, displs(bidx), ierr)
    blen(bidx) = mvtype * extr2

    bidx = bidx + 1
    CALL MPI_Get_address (casabiome%xkleafdryexp, displs(bidx), ierr)
    blen(bidx) = mvtype * extr2

    bidx = bidx + 1
    CALL MPI_Get_address (casabiome%glaimax, displs(bidx), ierr)
    blen(bidx) = mvtype * extr2

    bidx = bidx + 1
    CALL MPI_Get_address (casabiome%glaimin, displs(bidx), ierr)
    blen(bidx) = mvtype * extr2

    bidx = bidx + 1
    CALL MPI_Get_address (casabiome%sla, displs(bidx), ierr)
    blen(bidx) = mvtype * extr2

    bidx = bidx + 1
    CALL MPI_Get_address (casabiome%ratiofrootleaf, displs(bidx), ierr)
    blen(bidx) = mvtype * extr2

    bidx = bidx + 1
    CALL MPI_Get_address (casabiome%kroot, displs(bidx), ierr)
    blen(bidx) = mvtype * extr2

    bidx = bidx + 1
    CALL MPI_Get_address (casabiome%krootlen, displs(bidx), ierr)
    blen(bidx) = mvtype * extr2

    bidx = bidx + 1
    CALL MPI_Get_address (casabiome%rootdepth, displs(bidx), ierr)
    blen(bidx) = mvtype * extr2

    bidx = bidx + 1
    CALL MPI_Get_address (casabiome%kuptake, displs(bidx), ierr)
    blen(bidx) = mvtype * extr2

    bidx = bidx + 1
    CALL MPI_Get_address (casabiome%kminN, displs(bidx), ierr)
    blen(bidx) = mvtype * extr2

    bidx = bidx + 1
    CALL MPI_Get_address (casabiome%KuplabP, displs(bidx), ierr)
    blen(bidx) = mvtype * extr2

    bidx = bidx + 1
    CALL MPI_Get_address (casabiome%kclabrate, displs(bidx), ierr)
    blen(bidx) = mvtype * extr2

    bidx = bidx + 1
    CALL MPI_Get_address (casabiome%xnpmax, displs(bidx), ierr)
    blen(bidx) = mvtype * extr2

    bidx = bidx + 1
    CALL MPI_Get_address (casabiome%q10soil, displs(bidx), ierr)
    blen(bidx) = mvtype * extr2

    bidx = bidx + 1
    CALL MPI_Get_address (casabiome%xkoptlitter, displs(bidx), ierr)
    blen(bidx) = mvtype * extr2

    bidx = bidx + 1
    CALL MPI_Get_address (casabiome%xkoptsoil, displs(bidx), ierr)
    blen(bidx) = mvtype * extr2

    bidx = bidx + 1
    CALL MPI_Get_address (casabiome%maxfinelitter, displs(bidx), ierr)
    blen(bidx) = mvtype * extr2

    bidx = bidx + 1
    CALL MPI_Get_address (casabiome%maxcwd, displs(bidx), ierr)
    blen(bidx) = mvtype * extr2

    bidx = bidx + 1
    CALL MPI_Get_address (casabiome%prodptase, displs(bidx), ierr)
    blen(bidx) = mvtype * extr2

    bidx = bidx + 1
    CALL MPI_Get_address (casabiome%costnpup, displs(bidx), ierr)
    blen(bidx) = mvtype * extr2

    bidx = bidx + 1
    CALL MPI_Get_address (casabiome%xkplab, displs(bidx), ierr)
    blen(bidx) = mso * extr2

    bidx = bidx + 1
    CALL MPI_Get_address (casabiome%xkpsorb, displs(bidx), ierr)
    blen(bidx) = mso * extr2

    bidx = bidx + 1
    CALL MPI_Get_address (casabiome%xkpocc, displs(bidx), ierr)
    blen(bidx) = mso * extr2

    bidx = bidx + 1
    CALL MPI_Get_address (casabiome%nintercept, displs(bidx), ierr)
    blen(bidx) = mvtype * extr2

    bidx = bidx + 1
    CALL MPI_Get_address (casabiome%nslope, displs(bidx), ierr)
    blen(bidx) = mvtype * extr2

    bidx = bidx + 1
    CALL MPI_Get_address (casabiome%plantrate, displs(bidx), ierr)
    blen(bidx) = mvtype * mplant * extr2

    bidx = bidx + 1
    CALL MPI_Get_address (casabiome%rmplant, displs(bidx), ierr)
    blen(bidx) = mvtype * mplant * extr2

    bidx = bidx + 1
    CALL MPI_Get_address (casabiome%fracnpptoP, displs(bidx), ierr)
    blen(bidx) = mvtype * mplant * extr2

    bidx = bidx + 1
    CALL MPI_Get_address (casabiome%fraclignin, displs(bidx), ierr)
    blen(bidx) = mvtype * mplant * extr2

    bidx = bidx + 1
    CALL MPI_Get_address (casabiome%fraclabile, displs(bidx), ierr)
    blen(bidx) = mvtype * mplant * extr2

    bidx = bidx + 1
    CALL MPI_Get_address (casabiome%ratioNCplantmin, displs(bidx), ierr)
    blen(bidx) = mvtype * mplant * extr2

    bidx = bidx + 1
    CALL MPI_Get_address (casabiome%ratioNCplantmax, displs(bidx), ierr)
    blen(bidx) = mvtype * mplant * extr2

    bidx = bidx + 1
    CALL MPI_Get_address (casabiome%ratioPCplantmin, displs(bidx), ierr)
    blen(bidx) = mvtype * mplant * extr2

    bidx = bidx + 1
    CALL MPI_Get_address (casabiome%ratioPCplantmax, displs(bidx), ierr)
    blen(bidx) = mvtype * mplant * extr2

    bidx = bidx + 1
    CALL MPI_Get_address (casabiome%fracLigninplant, displs(bidx), ierr)
    blen(bidx) = mvtype * mplant * extr2

    bidx = bidx + 1
    CALL MPI_Get_address (casabiome%ftransNPtoL, displs(bidx), ierr)
    blen(bidx) = mvtype * mplant * extr2

    bidx = bidx + 1
    CALL MPI_Get_address (casabiome%ftransPPtoL, displs(bidx), ierr)
    blen(bidx) = mvtype * mplant * extr2

    bidx = bidx + 1
    CALL MPI_Get_address (casabiome%litterrate, displs(bidx), ierr)
    blen(bidx) = mvtype * mlitter * extr2

    bidx = bidx + 1
    CALL MPI_Get_address (casabiome%soilrate, displs(bidx), ierr)
    blen(bidx) = mvtype * msoil * extr2

    ! added by ln
    bidx = bidx + 1
    CALL MPI_Get_address (casabiome%ratioNPplantmin, displs(bidx), ierr)
    blen(bidx) = mvtype * mplant * extr2

    bidx = bidx + 1
    CALL MPI_Get_address (casabiome%ratioNPplantmax, displs(bidx), ierr)
    blen(bidx) = mvtype * mplant * extr2

    ! added by vh
    bidx = bidx + 1
    CALL MPI_Get_address (casabiome%la_to_sa, displs(bidx), ierr)
    blen(bidx) = mvtype  * extr2

    bidx = bidx + 1
    CALL MPI_Get_address (casabiome%vcmax_scalar, displs(bidx), ierr)
    blen(bidx) = mvtype  * extr2

    bidx = bidx + 1
    CALL MPI_Get_address (casabiome%disturbance_interval, displs(bidx), ierr)
    blen(bidx) = mvtype  * extr2

    bidx = bidx + 1
    CALL MPI_Get_address (casabiome%DAMM_EnzPool, displs(bidx), ierr)
    blen(bidx) = mvtype * extr2

    bidx = bidx + 1
    CALL MPI_Get_address (casabiome%DAMM_KMO2, displs(bidx), ierr)
    blen(bidx) = mvtype * extr2

    bidx = bidx + 1
    CALL MPI_Get_address (casabiome%DAMM_KMcp, displs(bidx), ierr)
    blen(bidx) = mvtype * extr2

    bidx = bidx + 1
    CALL MPI_Get_address (casabiome%DAMM_Ea, displs(bidx), ierr)
    blen(bidx) = mvtype * extr2

    bidx = bidx + 1
    CALL MPI_Get_address (casabiome%DAMM_alpha, displs(bidx), ierr)
    blen(bidx) = mvtype * extr2

    ! ------ casapool ----

    bidx = bidx + 1
    CALL MPI_Get_address (casapool%Clabile, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casapool%dClabiledt, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casapool%Cplant, displs(bidx), ierr)
    blen(bidx) = mplant * r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casapool%Nplant, displs(bidx), ierr)
    blen(bidx) = mplant * r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casapool%Pplant, displs(bidx), ierr)
    blen(bidx) = mplant * r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casapool%dCplantdt, displs(bidx), ierr)
    blen(bidx) = mplant * r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casapool%dNplantdt, displs(bidx), ierr)
    blen(bidx) = mplant * r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casapool%dPplantdt, displs(bidx), ierr)
    blen(bidx) = mplant * r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casapool%ratioNCplant, displs(bidx), ierr)
    blen(bidx) = mplant * r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casapool%ratioPCplant, displs(bidx), ierr)
    blen(bidx) = mplant * r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casapool%Nsoilmin, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casapool%Psoillab, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casapool%Psoilsorb, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casapool%Psoilocc, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casapool%dNsoilmindt, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casapool%dPsoillabdt, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casapool%dPsoilsorbdt, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casapool%dPsoiloccdt, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casapool%Clitter, displs(bidx), ierr)
    blen(bidx) = mlitter * r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casapool%Nlitter, displs(bidx), ierr)
    blen(bidx) = mlitter * r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casapool%Plitter, displs(bidx), ierr)
    blen(bidx) = mlitter * r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casapool%dClitterdt, displs(bidx), ierr)
    blen(bidx) = mlitter * r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casapool%dNlitterdt, displs(bidx), ierr)
    blen(bidx) = mlitter * r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casapool%dPlitterdt, displs(bidx), ierr)
    blen(bidx) = mlitter * r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casapool%ratioNClitter, displs(bidx), ierr)
    blen(bidx) = mlitter * r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casapool%ratioPClitter, displs(bidx), ierr)
    blen(bidx) = mlitter * r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casapool%Csoil, displs(bidx), ierr)
    blen(bidx) = msoil * r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casapool%Nsoil, displs(bidx), ierr)
    blen(bidx) = msoil * r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casapool%Psoil, displs(bidx), ierr)
    blen(bidx) = msoil * r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casapool%dCsoildt, displs(bidx), ierr)
    blen(bidx) = msoil * r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casapool%dNsoildt, displs(bidx), ierr)
    blen(bidx) = msoil * r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casapool%dPsoildt, displs(bidx), ierr)
    blen(bidx) = msoil * r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casapool%ratioNCsoil, displs(bidx), ierr)
    blen(bidx) = msoil * r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casapool%ratioPCsoil, displs(bidx), ierr)
    blen(bidx) = msoil * r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casapool%ratioNCsoilnew, displs(bidx), ierr)
    blen(bidx) = msoil * r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casapool%ratioNCsoilmin, displs(bidx), ierr)
    blen(bidx) = msoil * r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casapool%ratioNCsoilmax, displs(bidx), ierr)
    blen(bidx) = msoil * r2len

    ! added by LN
    bidx = bidx + 1
    CALL MPI_Get_address (casapool%ratioNPplant, displs(bidx), ierr)
    blen(bidx) = mplant * r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casapool%ratioNPlitter, displs(bidx), ierr)
    !    blen(bidx) = mplant * r2len
    ! Maciej
    blen(bidx) = mlitter * r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casapool%ratioNPsoil, displs(bidx), ierr)
    !    blen(bidx) = mplant * r2len
    ! Maciej
    blen(bidx) = msoil * r2len

    ! ------- casaflux ----

    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%Cgpp, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%Cnpp, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%Crp, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%Crgplant, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%Nminfix, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%Nminuptake, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%Plabuptake, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%Clabloss, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%fracClabile, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%fracCalloc, displs(bidx), ierr)
    blen(bidx) = mplant * r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%fracNalloc, displs(bidx), ierr)
    blen(bidx) = mplant * r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%fracPalloc, displs(bidx), ierr)
    blen(bidx) = mplant * r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%Crmplant, displs(bidx), ierr)
    blen(bidx) = mplant * r2len

    bidx = bidx + 1
    CALL MPI_Get_address(casaflux%kplant, displs(bidx), ierr)
    blen(bidx) = mplant * r2len

    bidx = bidx + 1
    CALL MPI_Get_address(casaflux%kplant_fire, displs(bidx), ierr)
    blen(bidx) = mplant * r2len

    bidx = bidx + 1
    CALL MPI_Get_address(casaflux%kplant_tot, displs(bidx), ierr)
    blen(bidx) = mplant * r2len

    ! 3D
    bidx = bidx + 1
    CALL MPI_Get_address(casaflux%fromPtoL, displs(bidx), ierr)
    blen(bidx) = mplant * mlitter * r2len

    bidx = bidx + 1
    CALL MPI_Get_address(casaflux%fromPtoL_fire, displs(bidx), ierr)
    blen(bidx) = mplant * mlitter * r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%Cnep, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%Crsoil, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%Nmindep, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%Nminloss, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%Nminleach, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%Nupland, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%Nlittermin, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%Nsmin, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%Nsimm, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%Nsnet, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%fNminloss, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%fNminleach, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%Pdep, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%Pwea, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%Pleach, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%Ploss, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%Pupland, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%Plittermin, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%Psmin, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%Psimm, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%Psnet, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%fPleach, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%kplab, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%kpsorb, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%kpocc, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%kmlabP, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%Psorbmax, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%frac_sapwood, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%sapwood_area, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%fHarvest(off), displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%fCrop(off), displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address(casaflux%klitter, displs(bidx), ierr)
    blen(bidx) = mlitter * r2len

    bidx = bidx + 1
    CALL MPI_Get_address(casaflux%klitter_fire, displs(bidx), ierr)
    blen(bidx) = mlitter * r2len

    bidx = bidx + 1
    CALL MPI_Get_address(casaflux%klitter_tot, displs(bidx), ierr)
    blen(bidx) = mlitter * r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%ksoil, displs(bidx), ierr)
    blen(bidx) = msoil * r2len

    ! 3D
    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%fromLtoS, displs(bidx), ierr)
    blen(bidx) = msoil * mlitter * r2len

    ! 3D
    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%fromStoS, displs(bidx), ierr)
    blen(bidx) = msoil * msoil * r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%fromLtoCO2, displs(bidx), ierr)
    blen(bidx) = mlitter * r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%fromStoCO2, displs(bidx), ierr)
    blen(bidx) = msoil * r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%fluxCtolitter, displs(bidx), ierr)
    blen(bidx) = mlitter * r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%fluxNtolitter, displs(bidx), ierr)
    blen(bidx) = mlitter * r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%fluxPtolitter, displs(bidx), ierr)
    blen(bidx) = mlitter * r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%fluxCtosoil, displs(bidx), ierr)
    blen(bidx) = msoil * r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%fluxNtosoil, displs(bidx), ierr)
    blen(bidx) = msoil * r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%fluxPtosoil, displs(bidx), ierr)
    blen(bidx) = msoil * r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%fluxCtoCO2, displs(bidx), ierr)
    blen(bidx) = r2len

    ! 13C
    bidx = bidx + 1
    CALL MPI_Get_address(casaflux%FluxFromPtoL, displs(bidx), ierr)
    blen(bidx) = mplant * mlitter * r2len

    bidx = bidx + 1
    CALL MPI_Get_address(casaflux%FluxFromLtoS, displs(bidx), ierr)
    blen(bidx) = mlitter * msoil * r2len

    bidx = bidx + 1
    CALL MPI_Get_address(casaflux%FluxFromStoS, displs(bidx), ierr)
    blen(bidx) = msoil * msoil * r2len

    bidx = bidx + 1
    CALL MPI_Get_address(casaflux%FluxFromPtoCO2, displs(bidx), ierr)
    blen(bidx) = mplant * r2len

    bidx = bidx + 1
    CALL MPI_Get_address(casaflux%FluxFromLtoCO2, displs(bidx), ierr)
    blen(bidx) = mlitter * r2len

    bidx = bidx + 1
    CALL MPI_Get_address(casaflux%FluxFromStoCO2, displs(bidx), ierr)
    blen(bidx) = msoil * r2len

    bidx = bidx + 1
    CALL MPI_Get_address(casaflux%FluxFromPtoHarvest, displs(bidx), ierr)
    blen(bidx) = r2len

    ! ------- casamet ----

    bidx = bidx + 1
    CALL MPI_Get_address (casamet%glai, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casamet%Tairk, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casamet%precip, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casamet%tsoilavg, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casamet%moistavg, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casamet%btran, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casamet%lnonwood, displs(bidx), ierr)
    blen(bidx) = I1LEN

    bidx = bidx + 1
    CALL MPI_Get_address (casamet%Tsoil, displs(bidx), ierr)
    blen(bidx) = ms * r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casamet%moist, displs(bidx), ierr)
    blen(bidx) = ms * r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casamet%iveg2, displs(bidx), ierr)
    blen(bidx) = I1LEN

    bidx = bidx + 1
    CALL MPI_Get_address (casamet%ijgcm, displs(bidx), ierr)
    ! Maciej: ijgcm is INTEGER
    blen(bidx) = i1len

    bidx = bidx + 1
    CALL MPI_Get_address (casamet%isorder, displs(bidx), ierr)
    ! Maciej: isorder is INTEGER
    blen(bidx) = i1len

    bidx = bidx + 1
    CALL MPI_Get_address (casamet%lat, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casamet%lon, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casamet%areacell, displs(bidx), ierr)
    blen(bidx) = r2len

    ! ------- casabal ----

    bidx = bidx + 1
    CALL MPI_Get_address (casabal%FCgppyear, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casabal%FCnppyear, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casabal%FCrmleafyear, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casabal%FCrmwoodyear, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casabal%FCrmrootyear, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casabal%FCrgrowyear, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casabal%FCrpyear, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casabal%FCrsyear, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casabal%FCneeyear, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casabal%FNdepyear, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casabal%FNfixyear, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casabal%FNsnetyear, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casabal%FNupyear, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casabal%FNleachyear, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casabal%FNlossyear, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casabal%FPweayear, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casabal%FPdustyear, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casabal%FPsnetyear, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casabal%FPupyear, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casabal%FPleachyear, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casabal%FPlossyear, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casabal%glaimon, displs(bidx), ierr)
    blen(bidx) = 12 * r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casabal%glaimonx, displs(bidx), ierr)
    blen(bidx) = 12 * r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casabal%cplantlast, displs(bidx), ierr)
    blen(bidx) = mplant * r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casabal%nplantlast, displs(bidx), ierr)
    blen(bidx) = mplant * r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casabal%pplantlast, displs(bidx), ierr)
    blen(bidx) = mplant * r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casabal%clitterlast, displs(bidx), ierr)
    blen(bidx) = mlitter * r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casabal%nlitterlast, displs(bidx), ierr)
    blen(bidx) = mlitter * r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casabal%plitterlast, displs(bidx), ierr)
    blen(bidx) = mlitter * r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casabal%csoillast, displs(bidx), ierr)
    blen(bidx) = msoil * r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casabal%nsoillast, displs(bidx), ierr)
    blen(bidx) = msoil * r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casabal%psoillast, displs(bidx), ierr)
    blen(bidx) = msoil * r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casabal%nsoilminlast, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casabal%psoillablast, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casabal%psoilsorblast, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casabal%psoilocclast, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casabal%cbalance, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casabal%nbalance, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casabal%pbalance, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casabal%sumcbal, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casabal%sumnbal, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casabal%sumpbal, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casabal%clabilelast, displs(bidx), ierr)
    blen(bidx) = r2len

    ! ------- phen -------

    bidx = bidx + 1
    CALL MPI_Get_address (phen%phase, displs(bidx), ierr)
    blen(bidx) = I1LEN

    bidx = bidx + 1
    CALL MPI_Get_address (phen%TKshed, displs(bidx), ierr)
    blen(bidx) = mvtype * extr2

    bidx = bidx + 1
    CALL MPI_Get_address (phen%doyphase, displs(bidx), ierr)
    blen(bidx) = mphase * I1LEN

    bidx = bidx + 1
    CALL MPI_Get_address (phen%phen(off), displs(bidx), ierr)
    blen(bidx) =  r1len

    bidx = bidx + 1
    CALL MPI_Get_address (phen%aphen(off), displs(bidx), ierr)
    blen(bidx) = r1len


    bidx = bidx + 1
    CALL MPI_Get_address (phen%phasespin(off,1), displs(bidx), ierr)
    blen(bidx) = mdyear*I1LEN

    bidx = bidx + 1
    CALL MPI_Get_address (phen%doyphasespin_1(off,1), displs(bidx), ierr)
    blen(bidx) =  mdyear*I1LEN

    bidx = bidx + 1
    CALL MPI_Get_address (phen%doyphasespin_2(off,1), displs(bidx), ierr)
    blen(bidx) =  mdyear*I1LEN

    bidx = bidx + 1
    CALL MPI_Get_address (phen%doyphasespin_3(off,1), displs(bidx), ierr)
    blen(bidx) =  mdyear*I1LEN

    bidx = bidx + 1
    CALL MPI_Get_address (phen%doyphasespin_4(off,1), displs(bidx), ierr)
    blen(bidx) =  mdyear*I1LEN


    ! MPI: sanity check
    IF (bidx /= ntyp) THEN
       WRITE(*,*) 'worker ', rank, ' invalid number of casa_ts param fields ', bidx, ', fix it (21)!'
       CALL MPI_Abort(comm, 55, ierr)
    END IF

    CALL MPI_Type_create_struct(bidx, blen, displs, types, casa_ts, ierr)
    CALL MPI_Type_commit(casa_ts, ierr)

    CALL MPI_Type_size(casa_ts, tsize, ierr)
    CALL MPI_Type_get_extent(casa_ts, tmplb, text, ierr)

    WRITE(*,*) 'worker casa_ts param blocks, size, extent and lb: ', rank, bidx, tsize, text, tmplb

    ! MPI: check whether total size of received data equals total
    ! data sent by all the workers
    CALL MPI_Reduce(tsize, MPI_DATATYPE_NULL, 1, MPI_INTEGER, MPI_SUM, 0, comm, ierr)

    DEALLOCATE(types)
    DEALLOCATE(displs)
    DEALLOCATE(blen)

    ! if anything went wrong the master will mpi_abort
    ! which mpi_recv below is going to catch...

    CALL MPI_Barrier(comm, ierr)

    ! so, now receive all the parameters
    !    CALL MPI_Recv (MPI_BOTTOM, 1, casa_ts, 0, 0, comm, stat, ierr)

    !   Maciej: buffered recv + unpac version
    ALLOCATE(rbuf(tsize))
    CALL MPI_Recv(rbuf, tsize, MPI_BYTE, 0, 0, comm, stat, ierr)
    CALL MPI_Get_count(stat, casa_ts, rcount, ierr2)

    IF (ierr == MPI_SUCCESS .AND. ierr2 == MPI_SUCCESS .AND. rcount == 1) THEN
       pos = 0
       CALL MPI_Unpack(rbuf, tsize, pos, MPI_BOTTOM, rcount, casa_ts, &
            comm, ierr)
       IF (ierr /= MPI_SUCCESS) write(*,*) 'casa params unpack error, rank: ',rank,ierr
    ELSE
       write(*,*) 'casa params recv rank err err2 rcount: ', rank, ierr, ierr2, rcount
    END IF

    DEALLOCATE(rbuf)

    ! finally free the MPI type
    CALL MPI_Type_Free(casa_ts, ierr)

    ! all casa parameters have been received from the master by now
    RETURN

  END SUBROUTINE worker_casa_params


  ! MPI: creates inp_t type to receive input data from the master
  SUBROUTINE worker_intype (comm,met,veg)

    use mpi

    USE cable_def_types_mod

    IMPLICIT NONE

    ! Arguments
    INTEGER,INTENT(IN) :: comm
    TYPE(met_type),INTENT(IN):: met ! meteorological data
    TYPE(veg_parameter_type),INTENT(IN) :: veg ! LAI retrieved from file

    ! Local variables

    ! temp arrays for marshalling all fields into a single struct
    INTEGER, ALLOCATABLE, DIMENSION(:) :: blocks
    INTEGER(KIND=MPI_ADDRESS_KIND), ALLOCATABLE, DIMENSION(:) :: displs
    INTEGER, ALLOCATABLE, DIMENSION(:) :: types

    ! temp vars for verifying block number and total length of inp_t
    INTEGER(KIND=MPI_ADDRESS_KIND) :: text, tmplb
    INTEGER :: tsize

    INTEGER :: r1len, r2len, I1LEN, llen ! block lengths
    INTEGER :: bidx ! block index
    INTEGER :: ntyp ! total number of blocks
    INTEGER :: ierr

    INTEGER :: rank

    CALL MPI_Comm_rank (comm, rank, ierr)

    r1len = mp * extr1
    r2len = mp * extr2
    I1LEN = mp * extid
    llen = mp * extl

    ! max total number of fields to receive: met + veg fields
    ! ntyp = 10 + 1
    ntyp = ninput

    ALLOCATE (blocks(ntyp))
    ALLOCATE (displs(ntyp))
    ALLOCATE (types(ntyp))

    ! met fields
    ! TODO: don't add optional field when not required
    ! (exists% flags)
    bidx = 0

    bidx = bidx + 1
    CALL MPI_Get_address (met%fsd, displs(bidx), ierr)
    blocks(bidx) = swb * r1len

    bidx = bidx + 1
    CALL MPI_Get_address (met%tk, displs(bidx), ierr)
    blocks(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (met%pmb, displs(bidx), ierr)
    blocks(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (met%qv, displs(bidx), ierr)
    blocks(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (met%ua, displs(bidx), ierr)
    blocks(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (met%precip, displs(bidx), ierr)
    blocks(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (met%precip_sn, displs(bidx), ierr)
    blocks(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (met%fld, displs(bidx), ierr)
    blocks(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (met%ca, displs(bidx), ierr)
    blocks(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (met%coszen, displs(bidx), ierr)
    blocks(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (met%Ndep, displs(bidx), ierr)
    blocks(bidx) = r1len


    ! veg fields

    bidx = bidx + 1
    CALL MPI_Get_address (veg%vlai, displs(bidx), ierr)
    blocks(bidx) = r1len

    ! additional field missing from previous versions;
    ! added when trying to fix a bug in the new mpi code
    ! the order of these new fields follows the order of their
    ! declaration in cable_define_types.F90

    bidx = bidx + 1
    CALL MPI_Get_address (met%year, displs(bidx), ierr)
    blocks(bidx) = I1LEN

    bidx = bidx + 1
    CALL MPI_Get_address (met%moy, displs(bidx), ierr)
    blocks(bidx) = I1LEN

    bidx = bidx + 1
    CALL MPI_Get_address (met%doy, displs(bidx), ierr)
    blocks(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (met%hod, displs(bidx), ierr)
    blocks(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (met%u10, displs(bidx), ierr)
    blocks(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (met%rhum, displs(bidx), ierr)
    blocks(bidx) = r1len


    ! MPI: sanity check
    IF (bidx /= ntyp) THEN
       WRITE (*,*) 'worker ',rank,': invalid intype nmat, nvec or n3d constant, fix it (22)!'
       CALL MPI_Abort(comm, 56, ierr)
    END IF


    ! marshall all fields into a single MPI derived datatype

    ! all variables are contiguous blocks of memory so just send them
    ! as blocks of bytes
    types = MPI_BYTE

    CALL MPI_Type_create_struct (bidx, blocks, displs, types, inp_t, ierr)
    CALL MPI_Type_commit (inp_t, ierr)

    CALL MPI_Type_size (inp_t, tsize, ierr)
    CALL MPI_Type_get_extent (inp_t, tmplb, text, ierr)

    WRITE (*,*) 'worker ',rank,': intype struct blocks, size, extent and lb: ', &
         bidx,tsize,text,tmplb

    ! MPI: check whether total size of received data equals total
    ! data sent by all the workers
    CALL MPI_Reduce (tsize, MPI_DATATYPE_NULL, 1, MPI_INTEGER, MPI_SUM, 0, comm, ierr)

    DEALLOCATE(types)
    DEALLOCATE(displs)
    DEALLOCATE(blocks)

    RETURN

  END SUBROUTINE worker_intype

  ! MPI: creates send_t type to send the results to the master
  !
  ! list of fields that master needs to receive for use in write_output:
  !
  ! air%          rlam
  !
  ! canopy%       delwc, fe, fev, fh, fhs, fhv, fevw, fevc, fes, fnee, fpn, frday,
  !               frp, frs, ga, through, spill, tv, cansto,
  !
  ! met%          precip, precip_sn, fld, fsd, tk, pmb, qv, ua, ca,
  !
  ! rad%          albedo, qcan, qssabs, transd, flws
  !
  ! soil%         zse,
  !
  ! ssnow%        wb, snowd, osnowd, runoff, cls, rnof1, rnof2, tgg, tggsn, sdepth, isflag
  !
  ! veg%          vlai
  !
  ! Total: 47
  SUBROUTINE worker_outtype(comm,met,canopy,ssnow,rad,bal,air,soil,veg)

    use mpi

    USE cable_def_types_mod

    IMPLICIT NONE

    INTEGER :: comm ! MPI communicator to talk to the workers

    TYPE(met_type),            INTENT(IN)    :: met
    TYPE(canopy_type),         INTENT(IN)    :: canopy
    TYPE(soil_snow_type),      INTENT(IN)    :: ssnow
    TYPE(radiation_type),      INTENT(IN)    :: rad
    TYPE(balances_type),       INTENT(INOUT) :: bal
    TYPE(air_type),            INTENT(IN)    :: air
    TYPE(soil_parameter_type), INTENT(IN)    :: soil ! soil parameters
    TYPE(veg_parameter_type),  INTENT(IN)    :: veg  ! vegetation parameters

    ! MPI: temp arrays for marshalling all types into a struct
    INTEGER, ALLOCATABLE, DIMENSION(:) :: blocks
    INTEGER(KIND=MPI_ADDRESS_KIND), ALLOCATABLE, DIMENSION(:) :: displs
    INTEGER, ALLOCATABLE, DIMENSION(:) :: types
    INTEGER :: ntyp ! number of worker's types

    ! MPI: block lengths and strides for hvector representing matrices
    INTEGER :: r1len, r2len, I1LEN, llen

    INTEGER :: rank, off, cnt
    INTEGER :: bidx, ierr

    INTEGER :: tsize
    INTEGER(KIND=MPI_ADDRESS_KIND) :: text, tmplb

    CALL MPI_Comm_rank (comm, rank, ierr)

    ! MPI: calculate the sizes/extents of Fortran types used by
    ! CABLE
    CALL find_extents

    ! MPI: allocate temp vectors used for marshalling
    ntyp = n3d + nmat + nvec
    ALLOCATE (blocks(ntyp))
    ALLOCATE (displs(ntyp))
    ALLOCATE (types(ntyp))

    ! MPI: should work because worker versions of CABLE variables
    ! are allocated with starting index set to patch0
    !off = wpatch%patch0
    !cnt = wpatch%npatch
    ! MPI: new version, all arrays indices run 1:mp
    off = 1
    cnt = mp

    r1len = cnt * extr1
    r2len = cnt * extr2
    I1LEN = cnt * extid
    llen  = cnt * extl

    ! ------------- 3D arrays -------------

    ! rad 3D
    ! TODO: REAL(r_1) : rad%qcan(landunits,mf,nrb)
    ! CALL MPI_Type_create_hvector (mf*nrb, r1len, r1stride, MPI_BYTE, &
    !  &            m3d_t(1, rank), ierr)
    bidx = 1
    CALL MPI_Get_address (rad%qcan(off,1,1), displs(bidx), ierr)
    blocks(bidx) = r1len * mf * nrb

    ! ------------- 2D arrays -------------

    ! MPI: an hvector type for each vector, maddr contains displacements
    ! for bundling these hvectors into the struct later
    ! block length is number of patches/worker * type extent
    ! stride is global number of patches * type extent
    ! repeat/no of blocks is the 2nd rank

    ! met 2D
    bidx = bidx + 1
    CALL MPI_Get_address (met%fsd(off,1), displs(bidx), ierr)
    blocks(bidx) = r1len * swb

    ! canopy 2D
    !midx = 1
    ! REAL(r_1)
    !CALL MPI_Get_address (canopy%rwater(off,1), maddr(midx), ierr) ! 1
    !CALL MPI_Type_create_hvector (ms, r1len, r1stride, MPI_BYTE, &
    !  &            mat_t(midx, rank), ierr)

    ! TODO: skip, used for restart but not output
    !  bidx = bidx + 1
    !  CALL MPI_Get_address (canopy%rwater(off,1), displs(bidx), ierr)
    !  blocks(bidx) = r1len * ms

    ! midx = midx + 1
    ! REAL(r_2)
    ! CALL MPI_Get_address (canopy%evapfbl(off,1), maddr(midx), ierr) ! 2
    !CALL MPI_Type_create_hvector (ms, r2len, r2stride, MPI_BYTE, &
    !  &            mat_t(midx, rank), ierr)

    ! TODO: skip, used for restart but not output
    bidx = bidx + 1
    CALL MPI_Get_address (canopy%evapfbl(off,1), displs(bidx), ierr)
    ! MPI: gol124: changed to r1 when Bernard ported to CABLE_r491
    blocks(bidx) = r1len * ms

    bidx = bidx + 1
    CALL MPI_Get_address (canopy%gswx(off,1), displs(bidx), ierr)
    blocks(bidx) = r1len * mf

    bidx = bidx + 1
    CALL MPI_Get_address (canopy%zetar(off,1), displs(bidx), ierr)
    blocks(bidx) = r1len * niter

    ! 13C
    ! done as canopy%gswx
    bidx = bidx + 1
    CALL MPI_Get_address(canopy%An(off,1), displs(bidx), ierr)
    blocks(bidx) = r2len * mf

    bidx = bidx + 1
    CALL MPI_Get_address(canopy%Rd(off,1), displs(bidx), ierr)
    blocks(bidx) = r2len * mf

    bidx = bidx + 1
    CALL MPI_Get_address(canopy%vcmax(off,1), displs(bidx), ierr)
    blocks(bidx) = r2len * mf

    bidx = bidx + 1
    CALL MPI_Get_address(canopy%gammastar(off,1), displs(bidx), ierr)
    blocks(bidx) = r2len * mf

    bidx = bidx + 1
    CALL MPI_Get_address(canopy%gsc(off,1), displs(bidx), ierr)
    blocks(bidx) = r2len * mf

    bidx = bidx + 1
    CALL MPI_Get_address(canopy%gbc(off,1), displs(bidx), ierr)
    blocks(bidx) = r2len * mf

    bidx = bidx + 1
    CALL MPI_Get_address(canopy%gac(off,1), displs(bidx), ierr)
    blocks(bidx) = r2len * mf

    bidx = bidx + 1
    CALL MPI_Get_address(canopy%ci(off,1), displs(bidx), ierr)
    blocks(bidx) = r2len * mf

    ! ssnow 2D
    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%dtmlt(off,1), displs(bidx), ierr)
    blocks(bidx) = r1len * 3

    !midx = midx + 1
    ! REAL(r_1)
    ! CALL MPI_Get_address (ssnow%albsoilsn(off,1), maddr(midx), ierr) ! 3
    ! CALL MPI_Type_create_hvector (nrb, r1len, r1stride, MPI_BYTE, &
    !  &            mat_t(midx, rank), ierr)
    !midx = midx + 1
    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%albsoilsn(off,1), displs(bidx), ierr)
    blocks(bidx) = r1len * nrb

    ! REAL(r_2)
    !CALL MPI_Get_address (ssnow%gammzz(off,1), maddr(midx), ierr) ! 4
    !CALL MPI_Type_create_hvector (ms, r2len, r2stride, MPI_BYTE, &
    !  &            mat_t(midx, rank), ierr)
    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%gammzz(off,1), displs(bidx), ierr)
    blocks(bidx) = r2len * ms

    !midx = midx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (ssnow%sconds(off,1), maddr(midx), ierr) ! 5
    !CALL MPI_Type_create_hvector (msn, r1len, r1stride, MPI_BYTE, &
    !  &            mat_t(midx, rank), ierr)
    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%sconds(off,1), displs(bidx), ierr)
    blocks(bidx) = r1len * msn

    !midx = midx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (ssnow%sdepth(off,1), maddr(midx), ierr) ! 6
    !CALL MPI_Type_create_hvector (msn, r1len, r1stride, MPI_BYTE, &
    !  &            mat_t(midx, rank), ierr)
    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%sdepth(off,1), displs(bidx), ierr)
    blocks(bidx) = r1len * msn

    !midx = midx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (ssnow%smass(off,1), maddr(midx), ierr) ! 7
    !CALL MPI_Type_create_hvector (msn, r1len, r1stride, MPI_BYTE, &
    !  &            mat_t(midx, rank), ierr)
    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%smass(off,1), displs(bidx), ierr)
    blocks(bidx) = r1len * msn

    !midx = midx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (ssnow%dtmlt(off,1), maddr(midx), ierr) ! 8
    !CALL MPI_Type_create_hvector (msn, r1len, r1stride, MPI_BYTE, &
    !  &            mat_t(midx, rank), ierr)
    ! MPI: r1134 does not know about this field, comment out
    !bidx = bidx + 1
    !CALL MPI_Get_address (ssnow%dtmlt(off,1), displs(bidx), ierr)
    !blocks(bidx) = r1len * msn

    !midx = midx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (ssnow%ssdn(off,1), maddr(midx), ierr) ! 9
    !CALL MPI_Type_create_hvector (msn, r1len, r1stride, MPI_BYTE, &
    !  &            mat_t(midx, rank), ierr)
    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%ssdn(off,1), displs(bidx), ierr)
    blocks(bidx) = r1len * msn

    !midx = midx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (ssnow%tgg(off,1), maddr(midx), ierr) ! 10
    !CALL MPI_Type_create_hvector (ms, r1len, r1stride, MPI_BYTE, &
    !  &            mat_t(midx, rank), ierr)
    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%tgg(off,1), displs(bidx), ierr)
    blocks(bidx) = r1len * ms

    !midx = midx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (ssnow%tggsn(off,1), maddr(midx), ierr) ! 11
    !CALL MPI_Type_create_hvector (msn, r1len, r1stride, MPI_BYTE, &
    !  &            mat_t(midx, rank), ierr)
    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%tggsn(off,1), displs(bidx), ierr)
    blocks(bidx) = r1len * msn

    !midx = midx + 1
    ! REAL(r_2)
    !CALL MPI_Get_address (ssnow%wb(off,1), maddr(midx), ierr) ! 12
    !CALL MPI_Type_create_hvector (ms, r2len, r2stride, MPI_BYTE, &
    !  &            mat_t(midx, rank), ierr)
    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%wb(off,1), displs(bidx), ierr)
    blocks(bidx) = r2len * ms

    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%evapfbl(off,1), displs(bidx), ierr)
    blocks(bidx) = r1len * ms

    !midx = midx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (ssnow%wbfice(off,1), maddr(midx), ierr) ! 13
    !CALL MPI_Type_create_hvector (ms, r1len, r1stride, MPI_BYTE, &
    !  &            mat_t(midx, rank), ierr)
    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%wbfice(off,1), displs(bidx), ierr)
    blocks(bidx) = r2len * ms

    !midx = midx + 1
    ! REAL(r_2)
    !CALL MPI_Get_address (ssnow%wbice(off,1), maddr(midx), ierr) ! 14
    !CALL MPI_Type_create_hvector (ms, r2len, r2stride, MPI_BYTE, &
    !  &            mat_t(midx, rank), ierr)
    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%wbice(off,1), displs(bidx), ierr)
    blocks(bidx) = r2len * ms

    !midx = midx + 1
    ! REAL(r_2)
    !CALL MPI_Get_address (ssnow%wblf(off,1), maddr(midx), ierr) ! 15
    !CALL MPI_Type_create_hvector (ms, r2len, r2stride, MPI_BYTE, &
    !  &            mat_t(midx, rank), ierr)
    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%wblf(off,1), displs(bidx), ierr)
    blocks(bidx) = r2len * ms

    ! additional  for sli
    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%S(off,1), displs(bidx), ierr)
    blocks(bidx) = r2len * ms

    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%Tsoil(off,1), displs(bidx), ierr)
    blocks(bidx) = r2len * ms

    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%thetai(off,1), displs(bidx), ierr)
    blocks(bidx) = r2len * ms

    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%snowliq(off,1), displs(bidx), ierr)
    blocks(bidx) = r2len * msn

    bidx = bidx + 1
    CALL MPI_Get_address(ssnow%sconds(off,1), displs(bidx), ierr)
    blocks(bidx) = r1len * msn
    ! end additional for sli

    ! rad 2D
    bidx = bidx + 1
    CALL MPI_Get_address (rad%fbeam(off,1), displs(bidx), ierr)
    blocks(bidx) = r1len * nrb

    !midx = midx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (rad%albedo(off,1), maddr(midx), ierr) ! 16
    !CALL MPI_Type_create_hvector (nrb, r1len, r1stride, MPI_BYTE, &
    !  &            mat_t(midx, rank), ierr)
    bidx = bidx + 1
    CALL MPI_Get_address (rad%albedo(off,1), displs(bidx), ierr)
    blocks(bidx) = r1len * nrb

    !midx = midx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (rad%fvlai(off,1), maddr(midx), ierr) ! 17
    !CALL MPI_Type_create_hvector (mf, r1len, r1stride, MPI_BYTE, &
    !  &            mat_t(midx, rank), ierr)
    bidx = bidx + 1
    CALL MPI_Get_address (rad%fvlai(off,1), displs(bidx), ierr)
    blocks(bidx) = r1len * mf

    !midx = midx + 1
    ! REAL(r_2)
    !CALL MPI_Get_address (rad%gradis(off,1), maddr(midx), ierr) ! 18
    !CALL MPI_Type_create_hvector (mf, r2len, r2stride, MPI_BYTE, &
    !  &            mat_t(midx, rank), ierr)
    bidx = bidx + 1
    CALL MPI_Get_address (rad%gradis(off,1), displs(bidx), ierr)
    blocks(bidx) = r1len * mf

    !midx = midx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (rad%rhocdf(off,1), maddr(midx), ierr) ! 19
    !CALL MPI_Type_create_hvector (nrb, r1len, r1stride, MPI_BYTE, &
    !  &            mat_t(midx, rank), ierr)
    bidx = bidx + 1
    CALL MPI_Get_address (rad%rhocdf(off,1), displs(bidx), ierr)
    blocks(bidx) = r1len * nrb

    !midx = midx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (rad%rniso(off,1), maddr(midx), ierr) ! 20
    !CALL MPI_Type_create_hvector (mf, r1len, r1stride, MPI_BYTE, &
    !  &            mat_t(midx, rank), ierr)
    bidx = bidx + 1
    CALL MPI_Get_address (rad%rniso(off,1), displs(bidx), ierr)
    blocks(bidx) = r1len * mf

    !midx = midx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (rad%scalex(off,1), maddr(midx), ierr) ! 21
    !CALL MPI_Type_create_hvector (mf, r1len, r1stride, MPI_BYTE, &
    !  &            mat_t(midx, rank), ierr)
    bidx = bidx + 1
    CALL MPI_Get_address (rad%scalex(off,1), displs(bidx), ierr)
    blocks(bidx) = r1len * mf

    !midx = midx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (rad%reffdf(off,1), maddr(midx), ierr) ! 22
    !CALL MPI_Type_create_hvector (nrb, r1len, r1stride, MPI_BYTE, &
    !  &            mat_t(midx, rank), ierr)
    bidx = bidx + 1
    CALL MPI_Get_address (rad%reffdf(off,1), displs(bidx), ierr)
    blocks(bidx) = r1len * nrb

    !midx = midx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (rad%reffbm(off,1), maddr(midx), ierr) ! 23
    !CALL MPI_Type_create_hvector (nrb, r1len, r1stride, MPI_BYTE, &
    !  &            mat_t(midx, rank), ierr)
    bidx = bidx + 1
    CALL MPI_Get_address (rad%reffbm(off,1), displs(bidx), ierr)
    blocks(bidx) = r1len * nrb

    !midx = midx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (rad%extkbm(off,1), maddr(midx), ierr) ! 24
    !CALL MPI_Type_create_hvector (nrb, r1len, r1stride, MPI_BYTE, &
    !  &            mat_t(midx, rank), ierr)
    bidx = bidx + 1
    CALL MPI_Get_address (rad%extkbm(off,1), displs(bidx), ierr)
    blocks(bidx) = r1len * nrb

    !midx = midx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (rad%extkdm(off,1), maddr(midx), ierr) ! 25
    !CALL MPI_Type_create_hvector (nrb, r1len, r1stride, MPI_BYTE, &
    !  &            mat_t(midx, rank), ierr)
    bidx = bidx + 1
    CALL MPI_Get_address (rad%extkdm(off,1), displs(bidx), ierr)
    blocks(bidx) = r1len * nrb

    !midx = midx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (rad%cexpkbm(off,1), maddr(midx), ierr) ! 26
    !CALL MPI_Type_create_hvector (nrb, r1len, r1stride, MPI_BYTE, &
    !  &            mat_t(midx, rank), ierr)
    bidx = bidx + 1
    CALL MPI_Get_address (rad%cexpkbm(off,1), displs(bidx), ierr)
    ! Maciej: cexpkbm is mp*swb
    !    blocks(bidx) = r1len * nrb
    blocks(bidx) = r1len * swb

    !midx = midx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (rad%cexpkdm(off,1), maddr(midx), ierr) ! 27
    !CALL MPI_Type_create_hvector (nrb, r1len, r1stride, MPI_BYTE, &
    !  &            mat_t(midx, rank), ierr)
    bidx = bidx + 1
    CALL MPI_Get_address (rad%cexpkdm(off,1), displs(bidx), ierr)
    ! Maciej: cexpkdm is mp*swb
    !    blocks(bidx) = r1len * nrb
    blocks(bidx) = r1len * swb

    bidx = bidx + 1
    CALL MPI_Get_address (rad%rhocbm(off,1), displs(bidx), ierr)
    ! Maciej: rhocbm is mp*nrb
    !    blocks(bidx) = r1len * swb
    blocks(bidx) = r1len * nrb


    ! air 2D - all fields 1D - skipped

    ! soil 2D
    !midx = midx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (soil%albsoil(off,1), maddr(midx), ierr) ! 28
    !CALL MPI_Type_create_hvector (nrb, r1len, r1stride, MPI_BYTE, &
    !  &            mat_t(midx, rank), ierr)
    bidx = bidx + 1
    CALL MPI_Get_address (soil%albsoil(off,1), displs(bidx), ierr)
    blocks(bidx) = r1len * nrb


    ! veg 2D
    bidx = bidx + 1
    CALL MPI_Get_address (veg%refl(off,1), displs(bidx), ierr)
    blocks(bidx) = r1len * 2

    bidx = bidx + 1
    CALL MPI_Get_address (veg%taul(off,1), displs(bidx), ierr)
    blocks(bidx) = r1len * 2

    !midx = midx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (veg%froot(off,1), maddr(midx), ierr) ! 29
    !CALL MPI_Type_create_hvector (ms, r1len, r1stride, MPI_BYTE, &
    !  &            mat_t(midx, rank), ierr)
    bidx = bidx + 1
    CALL MPI_Get_address (veg%froot(off,1), displs(bidx), ierr)
    blocks(bidx) = r1len * ms



    ! ------------- 1D arrays -------------

    ! met
    !vidx = 1
    ! REAL(r_1)
    !CALL MPI_Get_address (met%ca(off), vaddr(vidx), ierr) ! 1
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (met%ca(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! INTEGER(i_d)
    !CALL MPI_Get_address (met%year(off), vaddr(vidx), ierr) ! 2
    !blen(vidx) = cnt * extid
    ! gol124: not output, removed
    !bidx = bidx + 1
    !CALL MPI_Get_address (met%year(off), displs(bidx), ierr)
    !blocks(bidx) = I1LEN

    !vidx = vidx + 1
    ! INTEGER(i_d)
    !CALL MPI_Get_address (met%moy(off), vaddr(vidx), ierr) ! 3
    !blen(vidx) = cnt * extid
    ! gol124: not output, removed
    !bidx = bidx + 1
    !CALL MPI_Get_address (met%moy(off), displs(bidx), ierr)
    !blocks(bidx) = I1LEN

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (met%doy(off), vaddr(vidx), ierr) ! 4
    !blen(vidx) = cnt * extr1
    ! gol124: not output, removed
    !bidx = bidx + 1
    !CALL MPI_Get_address (met%doy(off), displs(bidx), ierr)
    !blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (met%hod(off), vaddr(vidx), ierr) ! 5
    !blen(vidx) = cnt * extr1
    ! gol124: not output, removed
    !bidx = bidx + 1
    !CALL MPI_Get_address (met%hod(off), displs(bidx), ierr)
    !blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (met%fsd(off), vaddr(vidx), ierr)
    !blen(vidx) = cnt * extr1
    ! MPI: gol124: changed to 2D and move up when Bernard
    ! ported to CABLE_r491
    !bidx = bidx + 1
    !CALL MPI_Get_address (met%fsd(off,1), displs(bidx), ierr)
    !blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (met%fld(off), vaddr(vidx), ierr)
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (met%fld(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (met%precip(off), vaddr(vidx), ierr)
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (met%precip(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (met%precip_sn(off), vaddr(vidx), ierr)
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (met%precip_sn(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (met%tk(off), vaddr(vidx), ierr)
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (met%tk(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (met%tvair(off), vaddr(vidx), ierr)
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (met%tvair(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (met%tvrad(off), vaddr(vidx), ierr)
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (met%tvrad(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (met%pmb(off), vaddr(vidx), ierr)
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (met%pmb(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (met%ua(off), vaddr(vidx), ierr)
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (met%ua(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (met%qv(off), vaddr(vidx), ierr)
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (met%qv(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (met%qvair(off), vaddr(vidx), ierr)
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (met%qvair(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (met%da(off), vaddr(vidx), ierr)
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (met%da(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (met%dva(off), vaddr(vidx), ierr)
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (met%dva(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (met%coszen(off), vaddr(vidx), ierr) ! 19
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (met%coszen(off), displs(bidx), ierr)
    blocks(bidx) = r1len


    ! canopy
    bidx = bidx + 1
    CALL MPI_Get_address (canopy%fess(off), displs(bidx), ierr)
    blocks(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (canopy%fesp(off), displs(bidx), ierr)
    blocks(bidx) = r2len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (canopy%cansto(off), vaddr(vidx), ierr) ! 20
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (canopy%cansto(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (canopy%cduv(off), vaddr(vidx), ierr) ! 21
    !blen(vidx) = cnt * extr1

    ! TODO: skip, used for restart but not output
    bidx = bidx + 1
    CALL MPI_Get_address (canopy%cduv(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (canopy%delwc(off), vaddr(vidx), ierr) ! 22
    !blen(vidx) = cnt * extr1

    bidx = bidx + 1
    CALL MPI_Get_address (canopy%delwc(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (canopy%dewmm(off), vaddr(vidx), ierr) ! 23
    !blen(vidx) = cnt * extr1

    ! TODO: skip, used for restart but not output
    bidx = bidx + 1
    CALL MPI_Get_address (canopy%dewmm(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_2)
    !CALL MPI_Get_address (canopy%dgdtg(off), vaddr(vidx), ierr) ! 24
    !blen(vidx) = cnt * extr2

    ! TODO: skip, used for restart but not output
    bidx = bidx + 1
    CALL MPI_Get_address (canopy%dgdtg(off), displs(bidx), ierr)
    blocks(bidx) = r2len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (canopy%fe(off), vaddr(vidx), ierr) ! 25
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (canopy%fe(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (canopy%fh(off), vaddr(vidx), ierr) ! 26
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (canopy%fh(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (canopy%fpn(off), vaddr(vidx), ierr) ! 27
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (canopy%fpn(off), displs(bidx), ierr)
    blocks(bidx) = r1len


    bidx = bidx + 1
    CALL MPI_Get_address (canopy%A_sh, displs(bidx), ierr)
    blocks(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (canopy%A_sl, displs(bidx), ierr)
    blocks(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (canopy%A_slC, displs(bidx), ierr)
    blocks(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (canopy%A_shC, displs(bidx), ierr)
    blocks(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (canopy%A_slJ, displs(bidx), ierr)
    blocks(bidx) = r2len


    bidx = bidx + 1
    CALL MPI_Get_address (canopy%A_shJ, displs(bidx), ierr)
    blocks(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (canopy%GPP_sh, displs(bidx), ierr)
    blocks(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (canopy%GPP_sl, displs(bidx), ierr)
    blocks(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (canopy%eta_A_cs, displs(bidx), ierr)
    blocks(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (canopy%dAdcs, displs(bidx), ierr)
    blocks(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (canopy%cs, displs(bidx), ierr)
    blocks(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (canopy%eta_GPP_cs, displs(bidx), ierr)
    blocks(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (canopy%eta_fevc_cs, displs(bidx), ierr)
    blocks(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (canopy%dlf, displs(bidx), ierr)
    blocks(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (veg%vcmax_shade, displs(bidx), ierr)
    blocks(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (veg%vcmax_sun, displs(bidx), ierr)
    blocks(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (veg%ejmax_shade, displs(bidx), ierr)
    blocks(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (veg%ejmax_sun, displs(bidx), ierr)
    blocks(bidx) = r1len


    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (canopy%frp(off), vaddr(vidx), ierr) ! 28
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (canopy%frp(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (canopy%frpw(off), vaddr(vidx), ierr) ! 29
    !blen(vidx) = cnt * extr1

    ! TODO: skip, used for restart but not output
    bidx = bidx + 1
    CALL MPI_Get_address (canopy%frpw(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (canopy%frpr(off), vaddr(vidx), ierr) ! 30
    !blen(vidx) = cnt * extr1

    ! TODO: skip, used for restart but not output
    bidx = bidx + 1
    CALL MPI_Get_address (canopy%frpr(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (canopy%frs(off), vaddr(vidx), ierr) ! 31
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (canopy%frs(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (canopy%fnee(off), vaddr(vidx), ierr) ! 32
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (canopy%fnee(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (canopy%frday(off), vaddr(vidx), ierr) ! 33
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (canopy%frday(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (canopy%fnv(off), vaddr(vidx), ierr) ! 34
    !blen(vidx) = cnt * extr1

    ! TODO: skip, used for restart but not output
    bidx = bidx + 1
    CALL MPI_Get_address (canopy%fnv(off), displs(bidx), ierr)
    blocks(bidx) = r1len


    ! gol124: MPI: DONE until here!

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (canopy%fev(off), vaddr(vidx), ierr) ! 35
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (canopy%fev(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_2)
    !CALL MPI_Get_address (canopy%fevc(off), vaddr(vidx), ierr) ! 36
    !blen(vidx) = cnt * extr2
    bidx = bidx + 1
    CALL MPI_Get_address (canopy%fevc(off), displs(bidx), ierr)
    blocks(bidx) = r2len

    !vidx = vidx + 1
    ! REAL(r_2)
    !CALL MPI_Get_address (canopy%fevw(off), vaddr(vidx), ierr) ! 37
    !blen(vidx) = cnt * extr2
    bidx = bidx + 1
    CALL MPI_Get_address (canopy%fevw(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_2)
    !CALL MPI_Get_address (canopy%potev_c(off), vaddr(vidx), ierr) ! 38
    !blen(vidx) = cnt * extr2
    !  bidx = bidx + 1
    !  CALL MPI_Get_address (canopy%potev_c(off), displs(bidx), ierr)
    !  blocks(bidx) = r2len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (canopy%fhv(off), vaddr(vidx), ierr) ! 39
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (canopy%fhv(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_2)
    !CALL MPI_Get_address (canopy%fhvw(off), vaddr(vidx), ierr) ! 40
    !blen(vidx) = cnt * extr2
    bidx = bidx + 1
    CALL MPI_Get_address (canopy%fhvw(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (canopy%fns(off), vaddr(vidx), ierr) ! 41
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (canopy%fns(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (canopy%fes(off), vaddr(vidx), ierr) ! 42
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (canopy%fes(off), displs(bidx), ierr)
    blocks(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (canopy%fes_cor(off), displs(bidx), ierr)
    blocks(bidx) = r2len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (canopy%fhs(off), vaddr(vidx), ierr) ! 43
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (canopy%fhs(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (canopy%fhs_cor(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (canopy%fwet(off), vaddr(vidx), ierr) ! 44
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (canopy%fwet(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (canopy%epot(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (canopy%fnpp(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (canopy%fevw_pot(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (canopy%gswx_T(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (canopy%cdtq(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (canopy%wetfac_cs(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (canopy%ga(off), vaddr(vidx), ierr) ! 45
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (canopy%ga(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (canopy%ghflux(off), vaddr(vidx), ierr) ! 46
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (canopy%ghflux(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (canopy%precis(off), vaddr(vidx), ierr) ! 47
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (canopy%precis(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (canopy%qscrn(off), vaddr(vidx), ierr) ! 48
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (canopy%qscrn(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (canopy%rnet(off), vaddr(vidx), ierr) ! 49
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (canopy%rnet(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (canopy%segg(off), vaddr(vidx), ierr) ! 50
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (canopy%segg(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (canopy%sghflux(off), vaddr(vidx), ierr) ! 51
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (canopy%sghflux(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (canopy%spill(off), vaddr(vidx), ierr) ! 52
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (canopy%spill(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (canopy%through(off), vaddr(vidx), ierr) ! 53
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (canopy%through(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (canopy%tscrn(off), vaddr(vidx), ierr) ! 54
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (canopy%tscrn(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (canopy%tv(off), vaddr(vidx), ierr) ! 55
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (canopy%tv(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (canopy%us(off), vaddr(vidx), ierr) ! 56
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (canopy%us(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (canopy%uscrn(off), vaddr(vidx), ierr) ! 57
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (canopy%uscrn(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (canopy%vlaiw(off), vaddr(vidx), ierr) ! 58
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (canopy%vlaiw(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (canopy%rghlai(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (canopy%wcint(off), vaddr(vidx), ierr) ! 59
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (canopy%wcint(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (canopy%fwsoil(off), displs(bidx), ierr)
    blocks(bidx) = r2len

    ! 13C
    ! LOGICAL
    ! done as veg%deciduous
    bidx = bidx + 1
    CALL MPI_Get_address(canopy%isc3(off), displs(bidx), ierr)
    blocks(bidx) = llen

    ! MPI: 2D vars moved above
    ! rwater
    ! evapfbl

    ! ssnow
    ! MPI: 2D vars moved above
    ! albsoilsn
    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%pudsto(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%pudsmx(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (ssnow%cls(off), vaddr(vidx), ierr) ! 60
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%cls(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (ssnow%dfn_dtg(off), vaddr(vidx), ierr) ! 61
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%dfn_dtg(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (ssnow%dfh_dtg(off), vaddr(vidx), ierr) ! 62
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%dfh_dtg(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%dfe_ddq(off), displs(bidx), ierr) ! +1
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (ssnow%ddq_dtg(off), vaddr(vidx), ierr) ! 63
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%ddq_dtg(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (ssnow%evapsn(off), vaddr(vidx), ierr) ! 64
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%evapsn(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (ssnow%fwtop(off), vaddr(vidx), ierr) ! 65
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%fwtop(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%fwtop1(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%fwtop2(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%fwtop3(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    ! MPI: 2D vars moved above
    ! gammzz
    !vidx = vidx + 1
    ! INTEGER(i_d)
    !CALL MPI_Get_address (ssnow%isflag(off), vaddr(vidx), ierr) ! 66
    !blen(vidx) = cnt * extid
    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%isflag(off), displs(bidx), ierr)
    blocks(bidx) = I1LEN

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (ssnow%osnowd(off), vaddr(vidx), ierr) ! 67
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%osnowd(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (ssnow%potev(off), vaddr(vidx), ierr) ! 68
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%potev(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_2)
    !CALL MPI_Get_address (soil%pwb_min(off), vaddr(vidx), ierr) ! 69
    !blen(vidx) = cnt * extr2
    bidx = bidx + 1
    CALL MPI_Get_address (soil%pwb_min(off), displs(bidx), ierr)
    blocks(bidx) = r2len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (ssnow%runoff(off), vaddr(vidx), ierr) ! 70
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%runoff(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (ssnow%rnof1(off), vaddr(vidx), ierr) ! 71
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%rnof1(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (ssnow%rnof2(off), vaddr(vidx), ierr) ! 72
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%rnof2(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (ssnow%rtsoil(off), vaddr(vidx), ierr) ! 73
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%rtsoil(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    ! MPI: 2D vars moved above
    ! sconds
    ! sdepth
    ! smass
    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (ssnow%snage(off), vaddr(vidx), ierr) ! 74
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%snage(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (ssnow%snowd(off), vaddr(vidx), ierr) ! 75
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%snowd(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (ssnow%smelt(off), vaddr(vidx), ierr) ! 76
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%smelt(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    ! MPI: 2D vars moved above
    ! dtmlt
    ! ssdn
    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (ssnow%ssdnn(off), vaddr(vidx), ierr) ! 77
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%ssdnn(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    ! MPI: 2D vars moved above
    ! tgg
    ! tggsn
    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (ssnow%tss(off), vaddr(vidx), ierr) ! 78
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%tss(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (ssnow%otss(off), vaddr(vidx), ierr) ! 79
    !blen(vidx) = cnt * extr1
    ! MPI: r1134 does not know about this field, comment out
    !bidx = bidx + 1
    !CALL MPI_Get_address (ssnow%otss(off), displs(bidx), ierr)
    !blocks(bidx) = r1len

    ! MPI: 2D vars moved above
    ! wb
    ! wbfice
    ! wbice
    ! wblf
    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (ssnow%wbtot(off), vaddr(vidx), ierr) ! 90
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%wbtot(off), displs(bidx), ierr)
    blocks(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%wb_lake(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%sinfil(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%qstss(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (ssnow%wetfac(off), vaddr(vidx), ierr) ! 91
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%wetfac(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    ! MPI: TODO: maybe not needed for transfer to master?
    !CALL MPI_Get_address (ssnow%owetfac(off), vaddr(vidx), ierr) ! 92
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%owetfac(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%t_snwlr(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%tggav(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%otss(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%otss_0(off), displs(bidx), ierr)
    blocks(bidx) = r1len


    ! rad
    ! MPI: 2D vars moved above
    ! albedo
    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (rad%extkb(off), vaddr(vidx), ierr) ! 93
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (rad%extkb(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (rad%extkd2(off), vaddr(vidx), ierr) ! 94
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (rad%extkd2(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (rad%extkd(off), vaddr(vidx), ierr) ! 95
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (rad%extkd(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (rad%flws(off), vaddr(vidx), ierr) ! 96
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (rad%flws(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    ! MPI: 2D vars moved above
    ! fvlai
    ! gradis
    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (rad%latitude(off), vaddr(vidx), ierr) ! 97
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (rad%latitude(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (rad%lwabv(off), vaddr(vidx), ierr) !98
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (rad%lwabv(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    ! MPI: 3D vars moved above
    ! qcan
    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (rad%qssabs(off), vaddr(vidx), ierr) !99
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (rad%qssabs(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    ! MPI: 2D vars moved above
    ! rhocdf
    ! rniso
    ! scalex
    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (rad%transd(off), vaddr(vidx), ierr) ! 100
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (rad%transd(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (rad%trad(off), vaddr(vidx), ierr) ! 101
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (rad%trad(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (rad%transb(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    ! MPI: 2D vars moved above
    ! reffdf
    ! reffbm
    ! extkbm
    ! extkdm
    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (rad%fbeam(off), vaddr(vidx), ierr) ! 102
    !blen(vidx) = cnt * extr1
    ! MPI: gol124: changed to 2D and moved up when Bernard
    ! ported to CABLE_r491
    !bidx = bidx + 1
    !CALL MPI_Get_address (rad%fbeam(off,1), displs(bidx), ierr)
    !blocks(bidx) = r1len

    ! MPI: 2D vars moved above
    ! cexpkbm
    ! cexpkdm

    ! bal
    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (bal%drybal(off), vaddr(vidx), ierr) ! 103
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (bal%drybal(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (bal%ebal(off), vaddr(vidx), ierr) ! 104
    !blen(vidx) = cnt * extr1
    ! MPI: remove ebal from exchanged data, calculate temp val on the master
    !bidx = bidx + 1
    !CALL MPI_Get_address (bal%ebal(off), displs(bidx), ierr)
    !blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (bal%ebal_tot(off), vaddr(vidx), ierr) ! 105
    !blen(vidx) = cnt * extr1
    ! MPI: remove ebal_tot from exchanged data, calculate val on the master
    !bidx = bidx + 1
    !CALL MPI_Get_address (bal%ebal_tot(off), displs(bidx), ierr)
    !blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (bal%seb(off), vaddr(vidx), ierr) ! 106
    !blen(vidx) = cnt * extr1
    ! MPI: remove seb from exchanged data, calculate temp val on the master
    !bidx = bidx + 1
    !CALL MPI_Get_address (bal%seb(off), displs(bidx), ierr)
    !blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (bal%seb_tot(off), vaddr(vidx), ierr) ! 107
    !blen(vidx) = cnt * extr1
    ! MPI: remove seb_tot from exchanged data, calculate val on the master
    !bidx = bidx + 1
    !CALL MPI_Get_address (bal%seb_tot(off), displs(bidx), ierr)
    !blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (bal%evap_tot(off), vaddr(vidx), ierr) ! 108
    !blen(vidx) = cnt * extr1
    ! MPI: remove evap_tot from exchanged data
    !bidx = bidx + 1
    !CALL MPI_Get_address (bal%evap_tot(off), displs(bidx), ierr)
    !blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (bal%osnowd0(off), vaddr(vidx), ierr) ! 109
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (bal%osnowd0(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (bal%precip_tot(off), vaddr(vidx), ierr) ! 110
    !blen(vidx) = cnt * extr1
    ! MPI: remove wbal from exchanged data
    !bidx = bidx + 1
    !CALL MPI_Get_address (bal%precip_tot(off), displs(bidx), ierr)
    !blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (bal%rnoff_tot(off), vaddr(vidx), ierr) ! 111
    !blen(vidx) = cnt * extr1
    ! MPI: remove wbal from exchanged data
    !bidx = bidx + 1
    !CALL MPI_Get_address (bal%rnoff_tot(off), displs(bidx), ierr)
    !blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (bal%wbal(off), vaddr(vidx), ierr) ! 112
    !blen(vidx) = cnt * extr1
    ! MPI: remove wbal from exchanged data
    ! bidx = bidx + 1
    ! CALL MPI_Get_address (bal%wbal(off), displs(bidx), ierr)
    ! blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (bal%wbal_tot(off), vaddr(vidx), ierr) ! 113
    !blen(vidx) = cnt * extr1
    ! MPI: remove wbal_tot from exchanged data
    !bidx = bidx + 1
    !CALL MPI_Get_address (bal%wbal_tot(off), displs(bidx), ierr)
    !blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (bal%wbtot0(off), vaddr(vidx), ierr) ! 114
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (bal%wbtot0(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (bal%wetbal(off), vaddr(vidx), ierr) ! 115
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (bal%wetbal(off), displs(bidx), ierr)
    blocks(bidx) = r1len


    ! air
    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (air%rho(off), vaddr(vidx), ierr) ! 116
    !blen(vidx) = cnt * extr1

    ! TODO: skip, used for restart but not output
    bidx = bidx + 1
    CALL MPI_Get_address (air%rho(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (air%volm(off), vaddr(vidx), ierr) ! 117
    !blen(vidx) = cnt * extr1

    ! TODO: skip, used for restart but not output
    bidx = bidx + 1
    CALL MPI_Get_address (air%volm(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (air%rlam(off), vaddr(vidx), ierr) ! 118
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (air%rlam(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (air%qsat(off), vaddr(vidx), ierr) ! 119
    !blen(vidx) = cnt * extr1

    ! TODO: skip, used for restart but not output
    bidx = bidx + 1
    CALL MPI_Get_address (air%qsat(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (air%epsi(off), vaddr(vidx), ierr) ! 120
    !blen(vidx) = cnt * extr1

    ! TODO: skip, used for restart but not output
    bidx = bidx + 1
    CALL MPI_Get_address (air%epsi(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (air%visc(off), vaddr(vidx), ierr) ! 121
    !blen(vidx) = cnt * extr1

    ! TODO: skip, used for restart but not output
    bidx = bidx + 1
    CALL MPI_Get_address (air%visc(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (air%psyc(off), vaddr(vidx), ierr) ! 122
    !blen(vidx) = cnt * extr1

    ! TODO: skip, used for restart but not output
    bidx = bidx + 1
    CALL MPI_Get_address (air%psyc(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (air%dsatdk(off), vaddr(vidx), ierr) ! 123
    !blen(vidx) = cnt * extr1

    ! TODO: skip, used for restart but not output
    bidx = bidx + 1
    CALL MPI_Get_address (air%dsatdk(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (air%cmolar(off), vaddr(vidx), ierr) ! 124
    !blen(vidx) = cnt * extr1

    ! TODO: skip, used for restart but not output
    bidx = bidx + 1
    CALL MPI_Get_address (air%cmolar(off), displs(bidx), ierr)
    blocks(bidx) = r1len


    ! soil
    ! MPI: 2D vars moved above
    ! albsoil
    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (soil%bch(off), vaddr(vidx), ierr) ! 125
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (soil%bch(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (soil%c3(off), vaddr(vidx), ierr) ! 126
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (soil%c3(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (soil%clay(off), vaddr(vidx), ierr) ! 127
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (soil%clay(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (soil%cnsd(off), vaddr(vidx), ierr) ! 128
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (soil%cnsd(off), displs(bidx), ierr)
    blocks(bidx) = r2len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (soil%css(off), vaddr(vidx), ierr) ! 129
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (soil%css(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (soil%hsbh(off), vaddr(vidx), ierr) ! 130
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (soil%hsbh(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (soil%hyds(off), vaddr(vidx), ierr) ! 131
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (soil%hyds(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! INTEGER(i_d)
    !CALL MPI_Get_address (soil%i2bp3(off), vaddr(vidx), ierr) ! 132
    !blen(vidx) = cnt * extid
    bidx = bidx + 1
    CALL MPI_Get_address (soil%i2bp3(off), displs(bidx), ierr)
    ! Maciej: i2bp3 is REAL
    !    blocks(bidx) = I1LEN
    blocks(bidx) = R1LEN

    !vidx = vidx + 1
    ! INTEGER(i_d)
    !CALL MPI_Get_address (soil%ibp2(off), vaddr(vidx), ierr) ! 133
    !blen(vidx) = cnt * extid
    bidx = bidx + 1
    CALL MPI_Get_address (soil%ibp2(off), displs(bidx), ierr)
    ! Maciej: ibp2 is REAL
    !    blocks(bidx) = I1LEN
    blocks(bidx) = R1LEN

    !vidx = vidx + 1
    ! INTEGER(i_d)
    !CALL MPI_Get_address (soil%isoilm(off), vaddr(vidx), ierr) ! 134
    !blen(vidx) = cnt * extid
    bidx = bidx + 1
    CALL MPI_Get_address (soil%isoilm(off), displs(bidx), ierr)
    blocks(bidx) = I1LEN

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (soil%rhosoil(off), vaddr(vidx), ierr) ! 135
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (soil%rhosoil(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (veg%rs20(off), vaddr(vidx), ierr) ! 136
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (veg%rs20(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (soil%sand(off), vaddr(vidx), ierr) ! 137
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (soil%sand(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (soil%sfc(off), vaddr(vidx), ierr) ! 138
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (soil%sfc(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (soil%silt(off), vaddr(vidx), ierr) ! 139
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (soil%silt(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (soil%ssat(off), vaddr(vidx), ierr) ! 140
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (soil%ssat(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (soil%sucs(off), vaddr(vidx), ierr) ! 141
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (soil%sucs(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (soil%swilt(off), vaddr(vidx), ierr) ! 142
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (soil%swilt(off), displs(bidx), ierr)
    blocks(bidx) = r1len


    ! veg
    !vidx = vidx + 1
    ! INTEGER(i_d)
    !CALL MPI_Get_address (veg%iveg(off), vaddr(vidx), ierr) ! 143
    !blen(vidx) = cnt * extid
    bidx = bidx + 1
    CALL MPI_Get_address (veg%iveg(off), displs(bidx), ierr)
    blocks(bidx) = I1LEN

    !vidx = vidx + 1
    ! INTEGER(i_d)
    !CALL MPI_Get_address (veg%meth(off), vaddr(vidx), ierr) ! 144
    !blen(vidx) = cnt * extid
    bidx = bidx + 1
    CALL MPI_Get_address (veg%meth(off), displs(bidx), ierr)
    ! Maciej: veg%meth is REAL
    !    blocks(bidx) = I1LEN
    blocks(bidx) = R1LEN

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (veg%vlai(off), vaddr(vidx), ierr) ! 145
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (veg%vlai(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    ! MPI: 2D vars moved above
    ! froot
    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (veg%canst1(off), vaddr(vidx), ierr) ! 146
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (veg%canst1(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (veg%ejmax(off), vaddr(vidx), ierr) ! 147
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (veg%ejmax(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (veg%frac4(off), vaddr(vidx), ierr) ! 148
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (veg%frac4(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (veg%wai(off), vaddr(vidx), ierr) ! 149
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (veg%wai(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (veg%vegcf(off), vaddr(vidx), ierr) ! 150
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (veg%vegcf(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (veg%tminvj(off), vaddr(vidx), ierr) ! 151
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (veg%tminvj(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (veg%tmaxvj(off), vaddr(vidx), ierr) ! 152
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (veg%tmaxvj(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (veg%vbeta(off), vaddr(vidx), ierr) ! 153
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (veg%vbeta(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (veg%xalbnir(off), vaddr(vidx), ierr) ! 154
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (veg%xalbnir(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (veg%hc(off), vaddr(vidx), ierr) ! 155
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (veg%hc(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (veg%shelrb(off), vaddr(vidx), ierr) ! 156
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (veg%shelrb(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (veg%vcmax(off), vaddr(vidx), ierr) ! 157
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (veg%vcmax(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (veg%xfang(off), vaddr(vidx), ierr) ! 158
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (veg%xfang(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (veg%dleaf(off), vaddr(vidx), ierr) ! 159
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (veg%dleaf(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (veg%rp20(off), vaddr(vidx), ierr) ! 160
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (veg%rp20(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (veg%rpcoef(off), vaddr(vidx), ierr) ! 161
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (veg%rpcoef(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! REAL(r_1)
    !CALL MPI_Get_address (veg%extkn(off), vaddr(vidx), ierr) ! 162
    !blen(vidx) = cnt * extr1
    bidx = bidx + 1
    CALL MPI_Get_address (veg%extkn(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    !vidx = vidx + 1
    ! LOGICAL
    !CALL MPI_Get_address (veg%deciduous(off), vaddr(vidx), ierr) ! 163
    !blen(vidx) = cnt * extl
    bidx = bidx + 1
    CALL MPI_Get_address (veg%deciduous(off), displs(bidx), ierr)
    blocks(bidx) = llen

    ! additional for SLI
    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%Tsurface(off), displs(bidx), ierr)
    blocks(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%h0(off), displs(bidx), ierr)
    blocks(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%delwcol(off), displs(bidx), ierr)
    blocks(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%evap(off), displs(bidx), ierr)
    blocks(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%nsnow(off), displs(bidx), ierr)
    blocks(bidx) = i1len

    bidx = bidx + 1
    CALL MPI_Get_address (ssnow%nsteps(off), displs(bidx), ierr)
    blocks(bidx) = r2len
    ! end additional for SLI


    ! MPI: sanity check
    IF (bidx /= ntyp) THEN
       WRITE (*,*) 'worker ',rank,': invalid outtype nmat, nvec or n3d constant, fix it (23)!'
       CALL MPI_Abort(comm, 57, ierr)
    END IF

    types = MPI_BYTE

    CALL MPI_Type_create_struct (bidx, blocks, displs, types, send_t, ierr)
    CALL MPI_Type_commit (send_t, ierr)

    CALL MPI_Type_size (send_t, tsize, ierr)
    CALL MPI_Type_get_extent (send_t, tmplb, text, ierr)

    WRITE (*,*) 'worker ',rank,': struct blocks, size, extent and lb: ',bidx,tsize,text,tmplb

    ! MPI: check whether total size of received data equals total
    ! data sent by all the workers
    !mcd287  CALL MPI_Reduce (tsize, tsize, 1, MPI_INTEGER, MPI_SUM, 0, comm, ierr)
    CALL MPI_Reduce (tsize, MPI_DATATYPE_NULL, 1, MPI_INTEGER, MPI_SUM, 0, comm, ierr)

    DEALLOCATE(types)
    DEALLOCATE(displs)
    DEALLOCATE(blocks)

    RETURN

  END SUBROUTINE worker_outtype

  ! not used
  ! SUBROUTINE worker_time_update (met, kend, dels)

  !   USE cable_common_module, ONLY: ktau_gl
  !   USE cable_def_types_mod
  !   USE cable_IO_vars_module

  !   IMPLICIT NONE

  !   TYPE(met_type), INTENT(INOUT) :: met
  !   INTEGER, INTENT(IN) :: kend ! number of time steps in simulation
  !   REAL, INTENT(IN) :: dels ! time step size

  !   INTEGER :: i

  !   DO i=1,mland ! over all land points/grid cells
  !      ! First set timing variables:
  !      ! All timing details below are initially written to the first patch
  !      ! of each gridcell, then dumped to all patches for the gridcell.
  !      IF(ktau_gl==1) THEN ! initialise...
  !         SELECT CASE(time_coord)
  !         CASE('LOC')! i.e. use local time by default
  !            ! hour-of-day = starting hod
  !            met%hod(landpt(i)%cstart) = shod
  !            met%doy(landpt(i)%cstart) = sdoy
  !            met%moy(landpt(i)%cstart) = smoy
  !            met%year(landpt(i)%cstart) = syear
  !         CASE('GMT')! use GMT
  !            ! hour-of-day = starting hod + offset from GMT time:
  !            met%hod(landpt(i)%cstart) = shod + (longitude(i)/180.0)*12.0
  !            ! Note above that all met%* vars have dim mp,
  !            ! while longitude and latitude have dimension mland.
  !            met%doy(landpt(i)%cstart) = sdoy
  !            met%moy(landpt(i)%cstart) = smoy
  !            met%year(landpt(i)%cstart) = syear
  !         CASE DEFAULT
  !            write(*,*) 'Unknown time coordinate! (SUBROUTINE get_met_data)'
  !            stop 9
  !         END SELECT
  !      ELSE
  !         ! increment hour-of-day by time step size:
  !         met%hod(landpt(i)%cstart) = met%hod(landpt(i)%cstart) + dels/3600.0
  !      END IF
  !      !
  !      IF(met%hod(landpt(i)%cstart)<0.0) THEN ! may be -ve since longitude
  !         ! has range [-180,180]
  !         ! Reduce day-of-year by one and ammend hour-of-day:
  !         met%doy(landpt(i)%cstart) = met%doy(landpt(i)%cstart) - 1
  !         met%hod(landpt(i)%cstart) = met%hod(landpt(i)%cstart) + 24.0
  !         ! If a leap year AND we're using leap year timing:
  !         IF(((MOD(syear,4)==0.AND.MOD(syear,100)/=0).OR. &
  !              (MOD(syear,4)==0.AND.MOD(syear,400)==0)).AND.leaps) THEN
  !            SELECT CASE(INT(met%doy(landpt(i)%cstart)))
  !            CASE(0) ! ie Dec previous year
  !               met%moy(landpt(i)%cstart) = 12
  !               met%year(landpt(i)%cstart) = met%year(landpt(i)%cstart) - 1
  !               met%doy(landpt(i)%cstart) = 365 ! prev year not leap year as this is
  !            CASE(31) ! Jan
  !               met%moy(landpt(i)%cstart) = 1
  !            CASE(60) ! Feb
  !               met%moy(landpt(i)%cstart) = 2
  !            CASE(91) ! Mar
  !               met%moy(landpt(i)%cstart) = 3
  !            CASE(121)
  !               met%moy(landpt(i)%cstart) = 4
  !            CASE(152)
  !               met%moy(landpt(i)%cstart) = 5
  !            CASE(182)
  !               met%moy(landpt(i)%cstart) = 6
  !            CASE(213)
  !               met%moy(landpt(i)%cstart) = 7
  !            CASE(244)
  !               met%moy(landpt(i)%cstart) = 8
  !            CASE(274)
  !               met%moy(landpt(i)%cstart) = 9
  !            CASE(305)
  !               met%moy(landpt(i)%cstart) = 10
  !            CASE(335)
  !               met%moy(landpt(i)%cstart) = 11
  !            END SELECT
  !         ELSE ! not a leap year or not using leap year timing
  !            SELECT CASE(INT(met%doy(landpt(i)%cstart)))
  !            CASE(0) ! ie Dec previous year
  !               met%moy(landpt(i)%cstart) = 12
  !               met%year(landpt(i)%cstart) = met%year(landpt(i)%cstart) - 1
  !               ! If previous year is a leap year
  !               IF((MOD(syear,4)==0.AND.MOD(syear,100)/=0).OR. &
  !                    (MOD(syear,4)==0.AND.MOD(syear,400)==0)) THEN
  !                  met%doy(landpt(i)%cstart) = 366
  !               ELSE
  !                  met%doy(landpt(i)%cstart) = 365
  !               END IF
  !            CASE(31) ! Jan
  !               met%moy(landpt(i)%cstart) = 1
  !            CASE(59) ! Feb
  !               met%moy(landpt(i)%cstart) = 2
  !            CASE(90)
  !               met%moy(landpt(i)%cstart) = 3
  !            CASE(120)
  !               met%moy(landpt(i)%cstart) = 4
  !            CASE(151)
  !               met%moy(landpt(i)%cstart) = 5
  !            CASE(181)
  !               met%moy(landpt(i)%cstart) = 6
  !            CASE(212)
  !               met%moy(landpt(i)%cstart) = 7
  !            CASE(243)
  !               met%moy(landpt(i)%cstart) = 8
  !            CASE(273)
  !               met%moy(landpt(i)%cstart) = 9
  !            CASE(304)
  !               met%moy(landpt(i)%cstart) = 10
  !            CASE(334)
  !               met%moy(landpt(i)%cstart) = 11
  !            END SELECT
  !         END IF ! if leap year or not
  !      ELSE IF(met%hod(landpt(i)%cstart)>=24.0) THEN
  !         ! increment or GMT adj has shifted day
  !         ! Adjust day-of-year and hour-of-day:
  !         met%doy(landpt(i)%cstart) = met%doy(landpt(i)%cstart) + 1
  !         met%hod(landpt(i)%cstart) = met%hod(landpt(i)%cstart) - 24.0
  !         ! If a leap year AND we're using leap year timing:
  !         IF(((MOD(syear,4)==0.AND.MOD(syear,100)/=0).OR. &
  !              (MOD(syear,4)==0.AND.MOD(syear,400)==0)).AND.leaps) THEN
  !            SELECT CASE(INT(met%doy(landpt(i)%cstart)))
  !            CASE(32) ! Feb
  !               met%moy(landpt(i)%cstart) = 2
  !            CASE(61) ! Mar
  !               met%moy(landpt(i)%cstart) = 3
  !            CASE(92)
  !               met%moy(landpt(i)%cstart) = 4
  !            CASE(122)
  !               met%moy(landpt(i)%cstart) = 5
  !            CASE(153)
  !               met%moy(landpt(i)%cstart) = 6
  !            CASE(183)
  !               met%moy(landpt(i)%cstart) = 7
  !            CASE(214)
  !               met%moy(landpt(i)%cstart) = 8
  !            CASE(245)
  !               met%moy(landpt(i)%cstart) = 9
  !            CASE(275)
  !               met%moy(landpt(i)%cstart) = 10
  !            CASE(306)
  !               met%moy(landpt(i)%cstart) = 11
  !            CASE(336)
  !               met%moy(landpt(i)%cstart) = 12
  !            CASE(367)! end of year; increment
  !               met%year(landpt(i)%cstart) = met%year(landpt(i)%cstart) + 1
  !               met%moy(landpt(i)%cstart) = 1
  !               met%doy(landpt(i)%cstart) = 1
  !            END SELECT
  !            ! ELSE IF not leap year and Dec 31st, increment year
  !         ELSE
  !            SELECT CASE(INT(met%doy(landpt(i)%cstart)))
  !            CASE(32) ! Feb
  !               met%moy(landpt(i)%cstart) = 2
  !            CASE(60) ! Mar
  !               met%moy(landpt(i)%cstart) = 3
  !            CASE(91)
  !               met%moy(landpt(i)%cstart) = 4
  !            CASE(121)
  !               met%moy(landpt(i)%cstart) = 5
  !            CASE(152)
  !               met%moy(landpt(i)%cstart) = 6
  !            CASE(182)
  !               met%moy(landpt(i)%cstart) = 7
  !            CASE(213)
  !               met%moy(landpt(i)%cstart) = 8
  !            CASE(244)
  !               met%moy(landpt(i)%cstart) = 9
  !            CASE(274)
  !               met%moy(landpt(i)%cstart) = 10
  !            CASE(305)
  !               met%moy(landpt(i)%cstart) = 11
  !            CASE(335)
  !               met%moy(landpt(i)%cstart) = 12
  !            CASE(366)! end of year; increment
  !               met%year(landpt(i)%cstart) = met%year(landpt(i)%cstart) + 1
  !               met%moy(landpt(i)%cstart) = 1
  !               met%doy(landpt(i)%cstart) = 1
  !            END SELECT
  !         END IF ! if leap year or not
  !      END IF ! if increment has pushed hod to a different day
  !      ! Now copy these values to all veg/soil patches in the current grid cell:
  !      met%hod(landpt(i)%cstart:landpt(i)%cend) = met%hod(landpt(i)%cstart)
  !      met%doy(landpt(i)%cstart:landpt(i)%cend) = met%doy(landpt(i)%cstart)
  !      met%moy(landpt(i)%cstart:landpt(i)%cend) = met%moy(landpt(i)%cstart)
  !      met%year(landpt(i)%cstart:landpt(i)%cend) = met%year(landpt(i)%cstart)
  !   ENDDO

  !   RETURN

  ! END SUBROUTINE worker_time_update

  ! creates MPI types for sending casa results back to the master at
  ! the end of the simulation
  !
  ! changes from old mpi version:
  !  phen: removed out because casa_poolout in this version
  !  is no longer writing phen%phase
  !

  !
  SUBROUTINE worker_casa_type(comm, casapool, casaflux, casamet, casabal, phen)

    use mpi

    USE cable_def_types_mod
    USE casadimension
    USE casavariable
    !  gol124: commented out because casa_poolout in this version
    !  is no longer writing phen%phase
    USE phenvariable

    IMPLICIT NONE

    ! subroutine arguments
    INTEGER :: comm ! MPI communicator to talk to the workers
    TYPE(casa_pool),     INTENT(INOUT) :: casapool
    TYPE(casa_flux),     INTENT(INOUT) :: casaflux
    TYPE(casa_met),      INTENT(INOUT) :: casamet
    TYPE(casa_balance),  INTENT(INOUT) :: casabal
    TYPE(phen_variable), INTENT(INOUT) :: phen

    ! local variables

    ! MPI: temp arrays for marshalling all types into a struct
    INTEGER, ALLOCATABLE, DIMENSION(:) :: blocks
    INTEGER(KIND=MPI_ADDRESS_KIND), ALLOCATABLE, DIMENSION(:) :: displs
    INTEGER, ALLOCATABLE, DIMENSION(:) :: types
    INTEGER :: ntyp ! number of worker's types

    ! MPI: block lengths and strides for hvector representing matrices
    INTEGER :: r1len, r2len, I1LEN, llen

    INTEGER :: off, cnt
    INTEGER :: bidx, ierr

    INTEGER :: tsize
    INTEGER(KIND=MPI_ADDRESS_KIND) :: text, tmplb

    ! MPI: allocate temp vectors used for marshalling
    ntyp = ncasa_mat + ncasa_vec + (nphen-1)
    ALLOCATE (blocks(ntyp))
    ALLOCATE (displs(ntyp))
    ALLOCATE (types(ntyp))

    !off = wpatch%patch0
    !cnt = wpatch%npatch
    off = 1
    cnt = mp

    r1len = cnt * extr1
    r2len = cnt * extr2
    I1LEN = cnt * extid
    llen  = cnt * extl

    bidx = 0

    ! ------------- 2D arrays -------------

    bidx = bidx + 1
    CALL MPI_Get_address (casapool%cplant(off,1), displs(bidx), ierr)
    blocks(bidx) = r2len * mplant

    bidx = bidx + 1
    CALL MPI_Get_address (casapool%clitter(off,1), displs(bidx), ierr)
    blocks(bidx) = r2len * mlitter

    bidx = bidx + 1
    CALL MPI_Get_address (casapool%csoil(off,1), displs(bidx), ierr)
    blocks(bidx) = r2len * msoil

    bidx = bidx + 1
    CALL MPI_Get_address (casapool%nplant(off,1), displs(bidx), ierr)
    blocks(bidx) = r2len * mplant

    bidx = bidx + 1
    CALL MPI_Get_address (casapool%nlitter(off,1), displs(bidx), ierr)
    blocks(bidx) = r2len * mlitter

    bidx = bidx + 1
    CALL MPI_Get_address (casapool%nsoil(off,1), displs(bidx), ierr)
    blocks(bidx) = r2len * msoil

    bidx = bidx + 1
    CALL MPI_Get_address (casapool%pplant(off,1), displs(bidx), ierr)
    blocks(bidx) = r2len * mplant

    bidx = bidx + 1
    CALL MPI_Get_address (casapool%plitter(off,1), displs(bidx), ierr)
    blocks(bidx) = r2len * mlitter

    bidx = bidx + 1
    CALL MPI_Get_address (casapool%psoil(off,1), displs(bidx), ierr)
    blocks(bidx) = r2len * msoil

    bidx = bidx + 1
    CALL MPI_Get_address (casapool%ratioNCplant(off,1), displs(bidx), ierr)
    blocks(bidx) = r2len * mplant

    bidx = bidx + 1
    CALL MPI_Get_address (casapool%ratioPCplant(off,1), displs(bidx), ierr)
    blocks(bidx) = r2len * mplant

    bidx = bidx + 1
    CALL MPI_Get_address (casapool%ratioNClitter(off,1), displs(bidx), ierr)
    blocks(bidx) = r2len * mlitter

    bidx = bidx + 1
    CALL MPI_Get_address (casapool%ratioPClitter(off,1), displs(bidx), ierr)
    blocks(bidx) = r2len * mlitter

    bidx = bidx + 1
    CALL MPI_Get_address (casapool%ratioNCsoil(off,1), displs(bidx), ierr)
    blocks(bidx) = r2len * msoil

    bidx = bidx + 1
    CALL MPI_Get_address (casapool%ratioPCsoil(off,1), displs(bidx), ierr)
    blocks(bidx) = r2len * msoil

    bidx = bidx + 1
    CALL MPI_Get_address (phen%doyphase(off,1), displs(bidx), ierr)
    blocks(bidx) = mphase*I1LEN

    bidx = bidx + 1
    CALL MPI_Get_address (phen%phasespin(off,1), displs(bidx), ierr)
    blocks(bidx) = mdyear*I1LEN

    bidx = bidx + 1
    CALL MPI_Get_address (phen%doyphasespin_1(off,1), displs(bidx), ierr)
    blocks(bidx) = mdyear*I1LEN

    bidx = bidx + 1
    CALL MPI_Get_address (phen%doyphasespin_2(off,1), displs(bidx), ierr)
    blocks(bidx) = mdyear*I1LEN

    bidx = bidx + 1
    CALL MPI_Get_address (phen%doyphasespin_3(off,1), displs(bidx), ierr)
    blocks(bidx) = mdyear*I1LEN

    bidx = bidx + 1
    CALL MPI_Get_address (phen%doyphasespin_4(off,1), displs(bidx), ierr)
    blocks(bidx) = mdyear*I1LEN

    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%Cplant_turnover(off,1), displs(bidx), ierr)
    blocks(bidx) = mplant*r2LEN

    !TRUNK from here no (off,1)
    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%fracCalloc(off,1), displs(bidx), ierr)
    blocks(bidx) = mplant * r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%fracNalloc(off,1), displs(bidx), ierr)
    blocks(bidx) = mplant * r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%fracPalloc(off,1), displs(bidx), ierr)
    blocks(bidx) = mplant * r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%Crmplant(off,1), displs(bidx), ierr)
    blocks(bidx) = mplant * r2len

    bidx = bidx + 1
    CALL MPI_Get_address(casaflux%kplant(off,1), displs(bidx), ierr)
    blocks(bidx) = mplant * r2len

    bidx = bidx + 1
    CALL MPI_Get_address(casaflux%kplant_fire(off,1), displs(bidx), ierr)
    blocks(bidx) = mplant * r2len

    ! 13C
    bidx = bidx + 1
    CALL MPI_Get_address(casaflux%FluxFromPtoCO2(off,1), displs(bidx), ierr)
    blocks(bidx) = mplant * r2len

    bidx = bidx + 1
    CALL MPI_Get_address(casaflux%FluxFromLtoCO2(off,1), displs(bidx), ierr)
    blocks(bidx) = mlitter * r2len

    bidx = bidx + 1
    CALL MPI_Get_address(casaflux%FluxFromStoCO2(off,1), displs(bidx), ierr)
    blocks(bidx) = msoil * r2len

    bidx = bidx + 1
    CALL MPI_Get_address(casaflux%kplant_tot(off,1), displs(bidx), ierr)
    blocks(bidx) = mplant * r2len

    bidx = bidx + 1
    CALL MPI_Get_address(casaflux%klitter_tot(off,1), displs(bidx), ierr)
    blocks(bidx) = mlitter * r2len

    bidx = bidx + 1
    CALL MPI_Get_address(casaflux%fluxfromPtoCO2_fire(off,1), displs(bidx), ierr)
    blocks(bidx) = mplant * r2len

    bidx = bidx + 1
    CALL MPI_Get_address(casaflux%fluxfromLtoCO2_fire(off,1), displs(bidx), ierr)
    blocks(bidx) = mlitter * r2len

    ! ------------- 3D arrays -------------
    !TRUNK and also no (off,1,1)
    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%fromPtoL(off,1,1), displs(bidx), ierr)
    blocks(bidx) = mplant * mlitter * r2len

    bidx = bidx + 1
    CALL MPI_Get_address(casaflux%klitter(off,1), displs(bidx), ierr)
    blocks(bidx) = mlitter * r2len

    bidx = bidx + 1
    CALL MPI_Get_address(casaflux%klitter_fire(off,1), displs(bidx), ierr)
    blocks(bidx) = mlitter * r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%ksoil(off,1), displs(bidx), ierr)
    blocks(bidx) = msoil * r2len

    ! 3D
    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%fromLtoS(off,1,1), displs(bidx), ierr)
    blocks(bidx) = msoil * mlitter * r2len

    ! 3D
    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%fromStoS(off,1,1), displs(bidx), ierr)
    blocks(bidx) = msoil * msoil * r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%fromLtoCO2(off,1), displs(bidx), ierr)
    blocks(bidx) = mlitter * r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%fromStoCO2(off,1), displs(bidx), ierr)
    blocks(bidx) = msoil * r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%fluxCtolitter(off,1), displs(bidx), ierr)
    blocks(bidx) = mlitter * r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%fluxNtolitter(off,1), displs(bidx), ierr)
    blocks(bidx) = mlitter * r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%fluxPtolitter(off,1), displs(bidx), ierr)
    blocks(bidx) = mlitter * r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%fluxCtosoil(off,1), displs(bidx), ierr)
    blocks(bidx) = msoil * r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%fluxNtosoil(off,1), displs(bidx), ierr)
    blocks(bidx) = msoil * r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%fluxPtosoil(off,1), displs(bidx), ierr)
    blocks(bidx) = msoil * r2len

    ! 13C
    bidx = bidx + 1
    CALL MPI_Get_address(casaflux%FluxFromPtoL(off,1,1), displs(bidx), ierr)
    blocks(bidx) = mplant * mlitter * r2len

    bidx = bidx + 1
    CALL MPI_Get_address(casaflux%FluxFromLtoS(off,1,1), displs(bidx), ierr)
    blocks(bidx) = mlitter * msoil * r2len

    bidx = bidx + 1
    CALL MPI_Get_address(casaflux%FluxFromStoS(off,1,1), displs(bidx), ierr)
    blocks(bidx) = msoil * msoil * r2len

    bidx = bidx + 1
    CALL MPI_Get_address(casaflux%fromPtoL_fire(off,1,1), displs(bidx), ierr)
    blocks(bidx) = mlitter * mplant * r2len

    ! ------------- 1D vectors -------------

    bidx = bidx + 1
    CALL MPI_Get_address (casamet%glai(off), displs(bidx), ierr)
    blocks(bidx) = r2len

    !  gol124: commented out because casa_poolout in this version
    !  is no longer writing phen%phase
    bidx = bidx + 1
    CALL MPI_Get_address (phen%phase(off), displs(bidx), ierr)
    blocks(bidx) = I1LEN

    bidx = bidx + 1
    CALL MPI_Get_address (phen%phen(off), displs(bidx), ierr)
    blocks(bidx) =  r1len

    bidx = bidx + 1
    CALL MPI_Get_address (phen%aphen(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (casapool%clabile(off), displs(bidx), ierr)
    blocks(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casapool%Ctot(off), displs(bidx), ierr)
    blocks(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casapool%Ctot_0(off), displs(bidx), ierr)
    blocks(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casapool%nsoilmin(off), displs(bidx), ierr)
    blocks(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casapool%psoillab(off), displs(bidx), ierr)
    blocks(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casapool%psoilsorb(off), displs(bidx), ierr)
    blocks(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casapool%psoilocc(off), displs(bidx), ierr)
    blocks(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%psorbmax(off), displs(bidx), ierr)
    blocks(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casabal%sumcbal(off), displs(bidx), ierr)
    blocks(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casabal%sumnbal(off), displs(bidx), ierr)
    blocks(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casabal%sumpbal(off), displs(bidx), ierr)
    blocks(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casabal%FCgppyear(off), displs(bidx), ierr)
    blocks(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casabal%FCnppyear(off), displs(bidx), ierr)
    blocks(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casabal%FCrmleafyear(off), displs(bidx), ierr)
    blocks(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casabal%FCrmwoodyear(off), displs(bidx), ierr)
    blocks(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casabal%FCrmrootyear(off), displs(bidx), ierr)
    blocks(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casabal%FCrgrowyear(off), displs(bidx), ierr)
    blocks(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casabal%FCrpyear(off), displs(bidx), ierr)
    blocks(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casabal%FCrsyear(off), displs(bidx), ierr)
    blocks(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casabal%FCneeyear(off), displs(bidx), ierr)
    blocks(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casabal%FNdepyear(off), displs(bidx), ierr)
    blocks(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casabal%FNfixyear(off), displs(bidx), ierr)
    blocks(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casabal%FNsnetyear(off), displs(bidx), ierr)
    blocks(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casabal%FNupyear(off), displs(bidx), ierr)
    blocks(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casabal%FNleachyear(off), displs(bidx), ierr)
    blocks(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casabal%FNlossyear(off), displs(bidx), ierr)
    blocks(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casabal%FPweayear(off), displs(bidx), ierr)
    blocks(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casabal%FPdustyear(off), displs(bidx), ierr)
    blocks(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casabal%FPsnetyear(off), displs(bidx), ierr)
    blocks(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casabal%FPupyear(off), displs(bidx), ierr)
    blocks(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casabal%FPleachyear(off), displs(bidx), ierr)
    blocks(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casabal%FPlossyear(off), displs(bidx), ierr)
    blocks(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%Cgpp(off), displs(bidx), ierr)
    blocks(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%Cnpp(off), displs(bidx), ierr)
    blocks(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%Crp(off), displs(bidx), ierr)
    blocks(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%Crgplant(off), displs(bidx), ierr)
    blocks(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%Nminfix(off), displs(bidx), ierr)
    blocks(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%Nminuptake(off), displs(bidx), ierr)
    blocks(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%Plabuptake(off), displs(bidx), ierr)
    blocks(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%Clabloss(off), displs(bidx), ierr)
    blocks(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%fracClabile(off), displs(bidx), ierr)
    blocks(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%Cnep(off), displs(bidx), ierr)
    blocks(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%Crsoil(off), displs(bidx), ierr)
    blocks(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%Nmindep(off), displs(bidx), ierr)
    blocks(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%Nminloss(off), displs(bidx), ierr)
    blocks(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%Nminleach(off), displs(bidx), ierr)
    blocks(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%Nupland(off), displs(bidx), ierr)
    blocks(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%Nlittermin(off), displs(bidx), ierr)
    blocks(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%Nsmin(off), displs(bidx), ierr)
    blocks(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%Nsimm(off), displs(bidx), ierr)
    blocks(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%Nsnet(off), displs(bidx), ierr)
    blocks(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%frac_sapwood(off), displs(bidx), ierr)
    blocks(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%sapwood_area(off), displs(bidx), ierr)
    blocks(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%Cplant_turnover_disturbance(off), displs(bidx), ierr)
    blocks(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%Cplant_turnover_crowding(off), displs(bidx), ierr)
    blocks(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%Cplant_turnover_resource_limitation(off), displs(bidx), ierr)
    blocks(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%fHarvest(off), displs(bidx), ierr)
    blocks(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%fCrop(off), displs(bidx), ierr)
    blocks(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address(casaflux%FluxFromPtoHarvest(off), displs(bidx), ierr)
    blocks(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address(casaflux%fluxCtoCO2_plant_fire(off), displs(bidx), ierr)
    blocks(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address(casaflux%fluxCtoCO2_litter_fire(off), displs(bidx), ierr)
    blocks(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address(casaflux%fluxNtoAtm_fire(off), displs(bidx), ierr)
    blocks(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address(casaflux%stemnpp(off), displs(bidx), ierr)
    blocks(bidx) = r2len

    !this is equivalent MPI code to parts of #9238
    bidx = bidx + 1
    CALL MPI_Get_address(casaflux%potstemnpp(off), displs(bidx), ierr)
    blocks(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address(casaflux%fNMinloss(off), displs(bidx), ierr)
    blocks(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address(casaflux%FluxCtoco2(off), displs(bidx), ierr)
    blocks(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%FluxCtohwp(off), displs(bidx), ierr)
    blocks(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%FluxCtoclear(off), displs(bidx), ierr)
    blocks(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casapool%dClabiledt(off), displs(bidx), ierr)
    blocks(bidx) = r2len

    ! MPI: sanity check
    IF (bidx /= ntyp) THEN
       WRITE(*,*) 'worker: invalid number of casa fields, fix it (24)!'
       WRITE(*,*) 'ntyp: ', ntyp, 'bidx: ', bidx
       CALL MPI_Abort(comm, 58, ierr)
    END IF

    types = MPI_BYTE

    CALL MPI_Type_create_struct(bidx, blocks, displs, types, casa_t, ierr)
    CALL MPI_Type_commit(casa_t, ierr)

    CALL MPI_Type_size(casa_t, tsize, ierr)
    CALL MPI_Type_get_extent(casa_t, tmplb, text, ierr)

    WRITE(*,*) 'casa type struct blocks, size, extent and lb: ', bidx, tsize, text, tmplb

    ! MPI: check whether total size of received data equals total
    ! data sent by all the workers
    CALL MPI_Reduce(tsize, MPI_DATATYPE_NULL, 1, MPI_INTEGER, MPI_SUM, 0, comm, ierr)

    DEALLOCATE(types)
    DEALLOCATE(displs)
    DEALLOCATE(blocks)

    RETURN

  END SUBROUTINE worker_casa_type



  SUBROUTINE worker_climate_types (comm, climate, ktauday)

    use mpi

    USE cable_def_types_mod, ONLY: climate_type, alloc_cbm_var, mp, zero_cbm_var
    ! USE cable_climate_mod, ONLY: climate_init

    IMPLICIT NONE

    TYPE(climate_type), INTENT(INOUT):: climate
    INTEGER, INTENT(IN) :: comm, ktauday

    ! MPI: temp arrays for marshalling all types into a struct
    INTEGER, ALLOCATABLE, DIMENSION(:) :: blocks
    INTEGER(KIND=MPI_ADDRESS_KIND), ALLOCATABLE, DIMENSION(:) :: displs
    INTEGER, ALLOCATABLE, DIMENSION(:) :: types
    INTEGER :: ntyp ! number of worker's types

    ! MPI: block lengths and strides for hvector representing matrices
    INTEGER :: r1len, i1len

    INTEGER :: tsize, totalrecv
    INTEGER(KIND=MPI_ADDRESS_KIND) :: text, tmplb

    INTEGER :: rank, off
    INTEGER :: bidx, ierr, ny, nd, ndq, nsd
    INTEGER :: stat(MPI_STATUS_SIZE), ierr2, rcount, pos


    CHARACTER, DIMENSION(:), ALLOCATABLE :: rbuf

    !TRUNK no call to alloc_cbm_var
    CALL alloc_cbm_var(climate,mp,ktauday)
    call zero_cbm_var(climate)

    !TRUNK but call to climate_init
    !CALL climate_init (climate)
    ! MPI: allocate temp vectors used for marshalling
    ntyp = nclimate

    ALLOCATE (blocks(ntyp))
    ALLOCATE (displs(ntyp))
    ALLOCATE (types(ntyp))
    off = 1

    ! counter to sum total number of bytes receives from all workers
    totalrecv = 0

    r1len = mp * extr1
    i1len = mp * extid

    bidx = 0

    ! ------------- 2D arrays -------------
    ny = climate%nyear_average
    nd = climate%nday_average
    ndq = 91
    nsd = ktauday * 5  ! sub-diurnal time-steps for leaf-level variables

    ! print*, 'worker, nd ny mp nsd', nd, ny,mp, nsd
    bidx = bidx + 1
    CALL MPI_Get_address (climate%mtemp_min_20(off,1), displs(bidx), ierr)
    blocks(bidx) = ny*r1len
    types(bidx)  = MPI_BYTE

    bidx = bidx + 1
    CALL MPI_Get_address (climate%mtemp_max_20(off,1), displs(bidx), ierr)
    blocks(bidx) = ny*r1len
    types(bidx)  = MPI_BYTE

    bidx = bidx + 1
    CALL MPI_Get_address (climate%alpha_PT_20(off,1), displs(bidx), ierr)
    blocks(bidx) = ny*r1len
    types(bidx)  = MPI_BYTE

    bidx = bidx + 1
    CALL MPI_Get_address (climate%dtemp_31(off,1), displs(bidx), ierr)
    blocks(bidx) = nd*r1len
    types(bidx)  = MPI_BYTE

    bidx = bidx + 1
    CALL MPI_Get_address (climate%dtemp_91(off,1), displs(bidx), ierr)
    blocks(bidx) = ndq*r1len
    types(bidx)  = MPI_BYTE

    bidx = bidx + 1
    CALL MPI_Get_address (climate%dmoist_31(off,1), displs(bidx), ierr)
    blocks(bidx) = nd*r1len
    types(bidx)  = MPI_BYTE

    bidx = bidx + 1
    CALL MPI_Get_address (climate%APAR_leaf_sun(off,1), displs(bidx), ierr)
    blocks(bidx) = nsd*r1len
    types(bidx)  = MPI_BYTE

    bidx = bidx + 1
    CALL MPI_Get_address (climate%APAR_leaf_shade(off,1), displs(bidx), ierr)
    blocks(bidx) = nsd*r1len
    types(bidx)  = MPI_BYTE

    bidx = bidx + 1
    CALL MPI_Get_address (climate%Dleaf_sun(off,1), displs(bidx), ierr)
    blocks(bidx) = nsd*r1len
    types(bidx)  = MPI_BYTE

    bidx = bidx + 1
    CALL MPI_Get_address (climate%Dleaf_shade(off,1), displs(bidx), ierr)
    blocks(bidx) = nsd*r1len
    types(bidx)  = MPI_BYTE

    bidx = bidx + 1
    CALL MPI_Get_address (climate%Tleaf_sun(off,1), displs(bidx), ierr)
    blocks(bidx) = nsd*r1len
    types(bidx)  = MPI_BYTE

    bidx = bidx + 1
    CALL MPI_Get_address (climate%Tleaf_shade(off,1), displs(bidx), ierr)
    blocks(bidx) = nsd*r1len
    types(bidx)  = MPI_BYTE

    bidx = bidx + 1
    CALL MPI_Get_address (climate%cs_sun(off,1), displs(bidx), ierr)
    blocks(bidx) = nsd*r1len
    types(bidx)  = MPI_BYTE

    bidx = bidx + 1
    CALL MPI_Get_address (climate%cs_shade(off,1), displs(bidx), ierr)
    blocks(bidx) = nsd*r1len
    types(bidx)  = MPI_BYTE

    bidx = bidx + 1
    CALL MPI_Get_address (climate%scalex_sun(off,1), displs(bidx), ierr)
    blocks(bidx) = nsd*r1len
    types(bidx)  = MPI_BYTE

    bidx = bidx + 1
    CALL MPI_Get_address (climate%scalex_shade(off,1), displs(bidx), ierr)
    blocks(bidx) = nsd*r1len
    types(bidx)  = MPI_BYTE

    bidx = bidx + 1
    CALL MPI_Get_address (climate%fwsoil(off,1), displs(bidx), ierr)
    blocks(bidx) = nsd*r1len
    types(bidx)  = MPI_BYTE

    bidx = bidx + 1
    CALL MPI_Get_address (climate%dmoist_min_20(off,1), displs(bidx), ierr)
    blocks(bidx) = ny*r1len
    types(bidx)  = MPI_BYTE

    bidx = bidx + 1
    CALL MPI_Get_address (climate%dmoist_max_20(off,1), displs(bidx), ierr)
    blocks(bidx) = ny*r1len
    types(bidx)  = MPI_BYTE

    bidx = bidx + 1
    CALL MPI_Get_address (climate%aprecip_20(off,1), displs(bidx), ierr)
    blocks(bidx) = ny*r1len
    types(bidx)  = MPI_BYTE

    ! ------------- 1D vectors -------------

    bidx = bidx + 1
    CALL MPI_Get_address (climate%chilldays(off), displs(bidx), ierr)
    blocks(bidx) = i1len
    types(bidx)  = MPI_BYTE

    bidx = bidx + 1
    CALL MPI_Get_address (climate%iveg(off), displs(bidx), ierr)
    blocks(bidx) = i1len
    types(bidx)  = MPI_BYTE

    bidx = bidx + 1
    CALL MPI_Get_address (climate%biome(off), displs(bidx), ierr)
    blocks(bidx) = i1len
    types(bidx)  = MPI_BYTE

    bidx = bidx + 1
    CALL MPI_Get_address (climate%GMD(off), displs(bidx), ierr)
    blocks(bidx) = i1len
    types(bidx)  = MPI_BYTE

    bidx = bidx + 1
    CALL MPI_Get_address (climate%dtemp(off), displs(bidx), ierr)
    blocks(bidx) = r1len
    types(bidx)  = MPI_BYTE

    bidx = bidx + 1
    CALL MPI_Get_address (climate%dmoist(off), displs(bidx), ierr)
    blocks(bidx) = r1len
    types(bidx)  = MPI_BYTE

    bidx = bidx + 1
    CALL MPI_Get_address (climate%mtemp(off), displs(bidx), ierr)
    blocks(bidx) = r1len
    types(bidx)  = MPI_BYTE

    bidx = bidx + 1
    CALL MPI_Get_address (climate%qtemp(off), displs(bidx), ierr)
    blocks(bidx) = r1len
    types(bidx)  = MPI_BYTE

    bidx = bidx + 1
    CALL MPI_Get_address (climate%mmoist(off), displs(bidx), ierr)
    blocks(bidx) = r1len
    types(bidx)  = MPI_BYTE

    bidx = bidx + 1
    CALL MPI_Get_address (climate%mtemp_min(off), displs(bidx), ierr)
    blocks(bidx) = r1len
    types(bidx)  = MPI_BYTE

    bidx = bidx + 1
    CALL MPI_Get_address (climate%mtemp_max(off), displs(bidx), ierr)
    blocks(bidx) = r1len
    types(bidx)  = MPI_BYTE

    bidx = bidx + 1
    CALL MPI_Get_address (climate%qtemp_max(off), displs(bidx), ierr)
    blocks(bidx) = r1len
    types(bidx)  = MPI_BYTE

    bidx = bidx + 1
    CALL MPI_Get_address (climate%qtemp_max_last_year(off), displs(bidx), ierr)
    blocks(bidx) = r1len
    types(bidx)  = MPI_BYTE

    bidx = bidx + 1
    CALL MPI_Get_address (climate%mtemp_min20(off), displs(bidx), ierr)
    blocks(bidx) = r1len
    types(bidx)  = MPI_BYTE

    bidx = bidx + 1
    CALL MPI_Get_address (climate%mtemp_max20(off), displs(bidx), ierr)
    blocks(bidx) = r1len
    types(bidx)  = MPI_BYTE

    bidx = bidx + 1
    CALL MPI_Get_address (climate%atemp_mean(off), displs(bidx), ierr)
    blocks(bidx) = r1len
    types(bidx)  = MPI_BYTE

    bidx = bidx + 1
    CALL MPI_Get_address (climate%agdd5(off), displs(bidx), ierr)
    blocks(bidx) = r1len
    types(bidx)  = MPI_BYTE

    bidx = bidx + 1
    CALL MPI_Get_address (climate%gdd5(off), displs(bidx), ierr)
    blocks(bidx) = r1len
    types(bidx)  = MPI_BYTE

    bidx = bidx + 1
    CALL MPI_Get_address (climate%agdd0(off), displs(bidx), ierr)
    blocks(bidx) = r1len
    types(bidx)  = MPI_BYTE

    bidx = bidx + 1
    CALL MPI_Get_address (climate%gdd0(off), displs(bidx), ierr)
    blocks(bidx) = r1len
    types(bidx)  = MPI_BYTE

    bidx = bidx + 1
    CALL MPI_Get_address (climate%alpha_PT(off), displs(bidx), ierr)
    blocks(bidx) = r1len
    types(bidx)  = MPI_BYTE

    bidx = bidx + 1
    CALL MPI_Get_address (climate%evap_PT(off), displs(bidx), ierr)
    blocks(bidx) = r1len
    types(bidx)  = MPI_BYTE

    bidx = bidx + 1
    CALL MPI_Get_address (climate%aevap(off), displs(bidx), ierr)
    blocks(bidx) = r1len
    types(bidx)  = MPI_BYTE

    bidx = bidx + 1
    CALL MPI_Get_address (climate%frec(off), displs(bidx), ierr)
    blocks(bidx) = r1len
    types(bidx)  = MPI_BYTE

    bidx = bidx + 1
    CALL MPI_Get_address (climate%gdd0_rec(off), displs(bidx), ierr)
    blocks(bidx) = r1len
    types(bidx)  = MPI_BYTE

    bidx = bidx + 1
    CALL MPI_Get_address (climate%fdorm(off), displs(bidx), ierr)
    blocks(bidx) = r1len
    types(bidx)  = MPI_BYTE

    bidx = bidx + 1
    CALL MPI_Get_address (climate%dmoist_min20(off), displs(bidx), ierr)
    blocks(bidx) = r1len
    types(bidx)  = MPI_BYTE

    bidx = bidx + 1
    CALL MPI_Get_address (climate%dmoist_max20(off), displs(bidx), ierr)
    blocks(bidx) = r1len
    types(bidx)  = MPI_BYTE

    bidx = bidx + 1
    CALL MPI_Get_address (climate%fapar_ann_max(off), displs(bidx), ierr)
    blocks(bidx) = r1len
    types(bidx)  = MPI_BYTE

    bidx = bidx + 1
    CALL MPI_Get_address (climate%fapar_ann_max_last_year(off), displs(bidx), ierr)
    blocks(bidx) = r1len
    types(bidx)  = MPI_BYTE

    bidx = bidx + 1
    CALL MPI_Get_address (climate%modis_igbp(off), displs(bidx), ierr)
    blocks(bidx) = i1len
    types(bidx)  = MPI_BYTE

    bidx = bidx + 1
    CALL MPI_Get_address (climate%AvgAnnMaxFAPAR(off), displs(bidx), ierr)
    blocks(bidx) = r1len
    types(bidx)  = MPI_BYTE

    bidx = bidx + 1
    CALL MPI_Get_address (climate%DSLR(off), displs(bidx), ierr)
    blocks(bidx) = i1len
    types(bidx)  = MPI_BYTE

    bidx = bidx + 1
    CALL MPI_Get_address (climate%aprecip_av20(off), displs(bidx), ierr)
    blocks(bidx) = r1len
    types(bidx)  = MPI_BYTE

    bidx = bidx + 1
    CALL MPI_Get_address (climate%NDAY_Nesterov(off), displs(bidx), ierr)
    blocks(bidx) = i1len
    types(bidx)  = MPI_BYTE

    bidx = bidx + 1
    CALL MPI_Get_address (climate%dmoist_min(off), displs(bidx), ierr)
    blocks(bidx) = r1len
    types(bidx)  = MPI_BYTE

    bidx = bidx + 1
    CALL MPI_Get_address (climate%dmoist_max(off), displs(bidx), ierr)
    blocks(bidx) = r1len
    types(bidx)  = MPI_BYTE

    bidx = bidx + 1
    CALL MPI_Get_address (climate%alpha_PT20(off), displs(bidx), ierr)
    blocks(bidx) = r1len
    types(bidx)  = MPI_BYTE

    bidx = bidx + 1
    CALL MPI_Get_address (climate%dtemp_min(off), displs(bidx), ierr)
    blocks(bidx) = r1len
    types(bidx)  = MPI_BYTE

    bidx = bidx + 1
    CALL MPI_Get_address (climate%dtemp_max(off), displs(bidx), ierr)
    blocks(bidx) = r1len
    types(bidx)  = MPI_BYTE

    bidx = bidx + 1
    CALL MPI_Get_address (climate%drhum(off), displs(bidx), ierr)
    blocks(bidx) = r1len
    types(bidx)  = MPI_BYTE

    bidx = bidx + 1
    CALL MPI_Get_address (climate%du10_max(off), displs(bidx), ierr)
    blocks(bidx) = r1len
    types(bidx)  = MPI_BYTE

    bidx = bidx + 1
    CALL MPI_Get_address (climate%dprecip(off), displs(bidx), ierr)
    blocks(bidx) = r1len
    types(bidx)  = MPI_BYTE

    bidx = bidx + 1
    CALL MPI_Get_address (climate%aprecip(off), displs(bidx), ierr)
    blocks(bidx) = r1len
    types(bidx)  = MPI_BYTE

    bidx = bidx + 1
    CALL MPI_Get_address (climate%last_precip(off), displs(bidx), ierr)
    blocks(bidx) = r1len
    types(bidx)  = MPI_BYTE

    bidx = bidx + 1
    CALL MPI_Get_address (climate%KBDI(off), displs(bidx), ierr)
    blocks(bidx) = r1len
    types(bidx)  = MPI_BYTE

    bidx = bidx + 1
    CALL MPI_Get_address (climate%FFDI(off), displs(bidx), ierr)
    blocks(bidx) = r1len
    types(bidx)  = MPI_BYTE

    bidx = bidx + 1
    CALL MPI_Get_address (climate%D_MacArthur(off), displs(bidx), ierr)
    blocks(bidx) = r1len
    types(bidx)  = MPI_BYTE

    bidx = bidx + 1
    CALL MPI_Get_address (climate%Nesterov_Current(off), displs(bidx), ierr)
    blocks(bidx) = r1len
    types(bidx)  = MPI_BYTE

    bidx = bidx + 1
    CALL MPI_Get_address (climate%Nesterov_ann_max(off), displs(bidx), ierr)
    blocks(bidx) = r1len
    types(bidx)  = MPI_BYTE

    bidx = bidx + 1
    CALL MPI_Get_address (climate%Nesterov_ann_max_last_year(off), displs(bidx), ierr)
    blocks(bidx) = r1len
    types(bidx)  = MPI_BYTE

    bidx = bidx + 1
    CALL MPI_Get_address (climate%Nesterov_ann_running_max(off), displs(bidx), ierr)
    blocks(bidx) = r1len
    types(bidx)  = MPI_BYTE

    ! ------------- scalars  -------------

    bidx = bidx + 1
    CALL MPI_Get_address (climate%nyears, displs(bidx), ierr)
    blocks(bidx) = extid
    types(bidx)  = MPI_BYTE

    bidx = bidx + 1
    CALL MPI_Get_address (climate%doy, displs(bidx), ierr)
    blocks(bidx) = extid
    types(bidx)  = MPI_BYTE


!!$types = MPI_BYTE
    ! MPI: sanity check
    IF (bidx /= ntyp) THEN
       WRITE (*,*) 'worker: invalid number of climate fields, fix it (25)!'
       CALL MPI_Abort (comm, 59, ierr)
    END IF


    CALL MPI_Type_create_struct (bidx, blocks, displs, types, climate_t, ierr)
    CALL MPI_Type_commit (climate_t, ierr)

    CALL MPI_Type_size (climate_t, tsize, ierr)
    CALL MPI_Type_get_extent (climate_t, tmplb, text, ierr)

    CALL MPI_Reduce (tsize, MPI_DATATYPE_NULL, 1, MPI_INTEGER, MPI_SUM, 0, comm, ierr)

    ! so, now receive all the parameters
    !    CALL MPI_Recv (MPI_BOTTOM, 1, climate_t, 0, 0, comm, stat, ierr)

    !   Maciej: buffered recv + unpac version
    ALLOCATE (rbuf(tsize))
    CALL MPI_Recv (rbuf, tsize, MPI_BYTE, 0, 0, comm, stat, ierr)
    CALL MPI_Get_count (stat, climate_t, rcount, ierr2)

    IF (ierr == MPI_SUCCESS .AND. ierr2 == MPI_SUCCESS .AND. rcount == 1) THEN
       pos = 0
       CALL MPI_Unpack (rbuf, tsize, pos, MPI_BOTTOM, rcount, climate_t, &
            comm, ierr)
       IF (ierr /= MPI_SUCCESS) write(*,*) 'climate unpack error, rank: ', rank, ierr
    ELSE
       write(*,*) 'climate recv rank err err2 rcount: ', rank, ierr, ierr2, rcount
    END IF

    DEALLOCATE(rbuf)

    DEALLOCATE(types)
    DEALLOCATE(displs)
    DEALLOCATE(blocks)

    RETURN

  END SUBROUTINE worker_climate_types
  !!$
  ! MPI: creates restart_t type to send to the master the fields
  ! that are only required for the restart file but not included in the
  ! results sent at the end of each time step
  SUBROUTINE worker_restart_type (comm, canopy, air)

    use mpi

    USE cable_def_types_mod

    IMPLICIT NONE

    INTEGER :: comm

    TYPE(canopy_type), INTENT(IN) :: canopy
    TYPE (air_type),INTENT(IN)     :: air

    ! MPI: temp arrays for marshalling all types into a struct
    INTEGER, ALLOCATABLE, DIMENSION(:) :: blocks
    INTEGER(KIND=MPI_ADDRESS_KIND), ALLOCATABLE, DIMENSION(:) :: displs
    INTEGER, ALLOCATABLE, DIMENSION(:) :: types
    INTEGER :: ntyp ! number of worker's types

    ! MPI: block lengths and strides for hvector representing matrices
    INTEGER :: r1len, r2len, I1LEN, llen

    INTEGER :: rank, off, cnt
    INTEGER :: bidx, ierr

    INTEGER :: tsize
    INTEGER(KIND=MPI_ADDRESS_KIND) :: text, tmplb

    CALL MPI_Comm_rank (comm, rank, ierr)

    ! MPI: allocate temp vectors used for marshalling
    ntyp = nrestart
    ALLOCATE (blocks(ntyp))
    ALLOCATE (displs(ntyp))
    ALLOCATE (types(ntyp))

    off = 1
    cnt = mp
    bidx = 0

    r1len = cnt * extr1
    r2len = cnt * extr2
    I1LEN = cnt * extid
    llen = cnt * extl

    !  bidx = bidx + 1
    !  CALL MPI_Get_address (canopy%rwater(off,1), displs(bidx), ierr)
    !  blocks(bidx) = r1len * ms

    bidx = bidx + 1
    CALL MPI_Get_address (canopy%evapfbl(off,1), displs(bidx), ierr)
    ! MPI: gol124: changed to r1 when Bernard ported to CABLE_r491
    blocks(bidx) = r1len * ms

    bidx = bidx + 1
    CALL MPI_Get_address (canopy%cduv(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (canopy%dewmm(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (canopy%dgdtg(off), displs(bidx), ierr)
    blocks(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (canopy%frpw(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (canopy%frpr(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (canopy%fnv(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (air%rho(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (air%volm(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (air%qsat(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (air%epsi(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (air%visc(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (air%psyc(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (air%dsatdk(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address (air%cmolar(off), displs(bidx), ierr)
    blocks(bidx) = r1len

    ! MPI: sanity check
    IF (bidx /= ntyp) THEN
       WRITE (*,*) 'invalid nrestart constant, fix it (26)!'
       CALL MPI_Abort (comm, 60, ierr)
    END IF

    types = MPI_BYTE

    CALL MPI_Type_create_struct (bidx, blocks, displs, types, restart_t, ierr)
    CALL MPI_Type_commit (restart_t, ierr)

    CALL MPI_Type_size (restart_t, tsize, ierr)
    CALL MPI_Type_get_extent (restart_t, tmplb, text, ierr)

    WRITE (*,*) 'restart struct blocks, size, extent and lb: ',rank,bidx,tsize,text,tmplb

    ! MPI: check whether total size of received data equals total
    ! data sent by all the workers
    !mcd287  CALL MPI_Reduce (tsize, tsize, 1, MPI_INTEGER, MPI_SUM, 0, comm, ierr)
    ! write(*,*) 'b4 reduce wk', tsize, MPI_DATATYPE_NULL, 1, MPI_INTEGER, MPI_SUM, 0, comm, ierr
    ! flush(6)
    ! flush(wlogn)
    CALL MPI_Reduce (tsize, MPI_DATATYPE_NULL, 1, MPI_INTEGER, MPI_SUM, 0, comm, ierr)

    DEALLOCATE(types)
    DEALLOCATE(displs)
    DEALLOCATE(blocks)

    RETURN

  END SUBROUTINE worker_restart_type

  SUBROUTINE worker_casa_dump_types(comm, casamet, casaflux, phen, climate, c13o2flux)

    use mpi

    use casadimension,       only: icycle
    use casavariable,        only: casa_met, casa_flux, mplant
    use cable_def_types_mod, only: climate_type
    use phenvariable
    ! 13C
    use cable_common_module, only: cable_user
    use cable_c13o2_def,     only: c13o2_flux

    IMPLICIT NONE

    ! sub arguments
    INTEGER,             INTENT(IN)    :: comm  ! MPI communicator
    TYPE(casa_met),      INTENT(INOUT) :: casamet
    TYPE(casa_flux),     INTENT(INOUT) :: casaflux
    TYPE(phen_variable), INTENT(INOUT) :: phen
    TYPE(climate_type),  INTENT(INOUT) :: climate
    ! 13C
    type(c13o2_flux),    intent(inout) :: c13o2flux

    ! local vars

    ! temp arrays for marshalling all fields into a single struct
    INTEGER, ALLOCATABLE, DIMENSION(:) :: blen
    INTEGER(KIND=MPI_ADDRESS_KIND), ALLOCATABLE, DIMENSION(:) :: displs
    INTEGER, ALLOCATABLE, DIMENSION(:) :: types

    ! temp vars for verifying block number and total length of inp_t
    INTEGER(KIND=MPI_ADDRESS_KIND) :: text, tmplb
    INTEGER :: tsize

    INTEGER :: ierr

    INTEGER :: r1len, r2len, I1LEN ! block lengths
    INTEGER :: bidx ! block index
    INTEGER :: ntyp ! total number of blocks

    INTEGER :: rank

    CALL MPI_Comm_rank(comm, rank, ierr)

    ntyp = ncdumprw + icycle - 1
    ! 13C
    if (cable_user%c13o2) ntyp = ntyp + 2
    !call_blaze=0 if blaze off, >0 if blaze on to some extent
    if (cable_user%call_blaze) ntyp = ntyp + 11

    ALLOCATE(blen(ntyp))
    ALLOCATE(displs(ntyp))
    ALLOCATE(types(ntyp))

    ! default type is byte, to be overriden for multi-D types
    types = MPI_BYTE

    r1len = mp * extr1
    r2len = mp * extr2
    i1len = mp * extid

    bidx = 0

    ! ------- casamet ----

    bidx = bidx + 1
    CALL MPI_Get_address(casamet%Tairk, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address(casamet%Tsoil, displs(bidx), ierr)
    blen(bidx) = ms * r2len

    bidx = bidx + 1
    CALL MPI_Get_address(casamet%moist, displs(bidx), ierr)
    blen(bidx) = ms * r2len

    bidx = bidx + 1
    CALL MPI_Get_address(casaflux%Cgpp, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address(casaflux%Crmplant, displs(bidx), ierr)
    blen(bidx) = mplant * r2len

    ! ------- phen ----

    bidx = bidx + 1
    CALL MPI_Get_address(phen%phase, displs(bidx), ierr)
    blen(bidx) = i1len

    bidx = bidx + 1
    CALL MPI_Get_address(phen%doyphase, displs(bidx), ierr)
    blen(bidx) = mphase * i1len

    ! ------- climate (for acclimation of autotrophic resp) ----

    bidx = bidx + 1
    CALL MPI_Get_address(climate%qtemp_max_last_year, displs(bidx), ierr)
    blen(bidx) = r1len

    bidx = bidx + 1
    CALL MPI_Get_address(climate%frec, displs(bidx), ierr)
    blen(bidx) =r1len

    ! ------- climate (for BLAZE) ----
    ! call_blaze=0 if blaze off, >0 if blaze on to some extent
    if (cable_user%call_blaze>0) then
       bidx = bidx + 1
       CALL MPI_Get_address(climate%dprecip, displs(bidx), ierr)
       blen(bidx) = r1len

       bidx = bidx + 1
       CALL MPI_Get_address(climate%aprecip_av20, displs(bidx), ierr)
       blen(bidx) = r1len

       bidx = bidx + 1
       CALL MPI_Get_address(climate%du10_max, displs(bidx), ierr)
       blen(bidx) = r1len

       bidx = bidx + 1
       CALL MPI_Get_address(climate%drhum, displs(bidx), ierr)
       blen(bidx) = r1len

       bidx = bidx + 1
       CALL MPI_Get_address(climate%dtemp_max, displs(bidx), ierr)
       blen(bidx) = r1len

       bidx = bidx + 1
       CALL MPI_Get_address(climate%dtemp_min, displs(bidx), ierr)
       blen(bidx) = r1len

       bidx = bidx + 1
       CALL MPI_Get_address(climate%KBDI, displs(bidx), ierr)
       blen(bidx) = r1len

       bidx = bidx + 1
       CALL MPI_Get_address(climate%D_MacArthur, displs(bidx), ierr)
       blen(bidx) = r1len

       bidx = bidx + 1
       CALL MPI_Get_address(climate%FFDI, displs(bidx), ierr)
       blen(bidx) = r1len

       bidx = bidx + 1
       CALL MPI_Get_address(climate%DSLR, displs(bidx), ierr)
       blen(bidx) = i1len

       bidx = bidx + 1
       CALL MPI_Get_address(climate%last_precip, displs(bidx), ierr)
       blen(bidx) = r1len
    endif

    ! ------- casaflux - N and P deposition ---

    if (icycle>1) then
       bidx = bidx + 1
       CALL MPI_Get_address(casaflux%Nmindep, displs(bidx), ierr)
       blen(bidx) = r2len
    endif

    if (icycle>2) then
       bidx = bidx + 1
       CALL MPI_Get_address(casaflux%Pdep, displs(bidx), ierr)
       blen(bidx) = r2len
    endif

    ! ------- c13o2 ----

    if (cable_user%c13o2) then
       bidx = bidx + 1
       CALL MPI_Get_address(c13o2flux%cAn12, displs(bidx), ierr)
       blen(bidx) = r2len

       bidx = bidx + 1
       CALL MPI_Get_address(c13o2flux%cAn, displs(bidx), ierr)
       blen(bidx) = r2len
    endif

    ! MPI: sanity check
    IF (bidx /= ntyp) THEN
       WRITE(*,*) 'worker ', rank, ' invalid number of casa_dump_t param fields ', bidx, ', fix it (27)!'
       CALL MPI_Abort(comm, 61, ierr)
    END IF

    CALL MPI_Type_create_struct(bidx, blen, displs, types, casa_dump_t, ierr)
    CALL MPI_Type_commit(casa_dump_t, ierr)

    CALL MPI_Type_size(casa_dump_t, tsize, ierr)
    CALL MPI_Type_get_extent(casa_dump_t, tmplb, text, ierr)

    WRITE (*,*) 'worker casa_dump_t param blocks, size, extent and lb: ', rank, &
         bidx, tsize, text, tmplb

    ! MPI: check whether total size of received data equals total
    ! data sent by all the workers
    ! write(*,*) 'b27 reduce wk', tsize, MPI_DATATYPE_NULL, 1, MPI_INTEGER, MPI_SUM, 0, comm, ierr
    ! flush(6)
    CALL MPI_Reduce(tsize, MPI_DATATYPE_NULL, 1, MPI_INTEGER, MPI_SUM, 0, comm, ierr)

    DEALLOCATE(types)
    DEALLOCATE(displs)
    DEALLOCATE(blen)

    ! ! if anything went wrong the master will mpi_abort
    ! ! which mpi_recv below is going to catch...
    ! ! so, now receive all the parameters
    ! CALL MPI_Recv (MPI_BOTTOM, 1, casa_dump_t, 0, 0, comm, stat, ierr)
    !
    ! ! finally free the MPI type
    ! CALL MPI_Type_Free (casa_dump_t, ierr)

    ! all casa parameters have been received from the master by now

  END SUBROUTINE worker_casa_dump_types


  SUBROUTINE worker_casa_LUC_types(comm, casapool, casabal, casaflux)

    use mpi
    USE casavariable, ONLY: casa_pool, casa_balance, casa_flux, mplant, mlitter, msoil

    IMPLICIT NONE

    ! sub arguments
    INTEGER,            INTENT(IN) :: comm  ! MPI communicator
    TYPE(casa_pool),    INTENT(IN) :: casapool
    TYPE(casa_balance), INTENT(IN) :: casabal
    TYPE(casa_flux),    INTENT(IN) :: casaflux

    ! local vars

    ! temp arrays for marshalling all fields into a single struct
    INTEGER, ALLOCATABLE, DIMENSION(:) :: blen
    INTEGER(KIND=MPI_ADDRESS_KIND), ALLOCATABLE, DIMENSION(:) :: displs
    INTEGER, ALLOCATABLE, DIMENSION(:) :: types

    ! temp vars for verifying block number and total length of inp_t
    INTEGER(KIND=MPI_ADDRESS_KIND) :: text, tmplb
    INTEGER :: tsize

    INTEGER :: ierr

    INTEGER :: r1len, r2len, I1LEN ! block lengths
    INTEGER :: bidx ! block index
    INTEGER :: ntyp ! total number of blocks

    INTEGER :: rank, off

    CALL MPI_Comm_rank(comm, rank, ierr)

    ntyp = nLUCrw

    ALLOCATE(blen(ntyp))
    ALLOCATE(displs(ntyp))
    ALLOCATE(types(ntyp))

    ! default type is byte, to be overriden for multi-D types
    types = MPI_BYTE

    r1len = mp * extr1
    r2len = mp * extr2
    i1len = mp * extid

    off  = 1

    bidx = 0

    bidx = bidx + 1
    CALL MPI_Get_address (casapool%cplant(off,1), displs(bidx), ierr)
    blen(bidx) = r2len * mplant

    bidx = bidx + 1
    CALL MPI_Get_address (casapool%clitter(off,1), displs(bidx), ierr)
    blen(bidx) = r2len * mlitter

    bidx = bidx + 1
    CALL MPI_Get_address (casapool%csoil(off,1), displs(bidx), ierr)
    blen(bidx) = r2len * msoil

    bidx = bidx + 1
    CALL MPI_Get_address (casapool%nplant(off,1), displs(bidx), ierr)
    blen(bidx) = r2len * mplant

    bidx = bidx + 1
    CALL MPI_Get_address (casapool%nlitter(off,1), displs(bidx), ierr)
    blen(bidx) = r2len * mlitter

    bidx = bidx + 1
    CALL MPI_Get_address (casapool%nsoil(off,1), displs(bidx), ierr)
    blen(bidx) = r2len * msoil

    bidx = bidx + 1
    CALL MPI_Get_address (casapool%pplant(off,1), displs(bidx), ierr)
    blen(bidx) = r2len * mplant

    bidx = bidx + 1
    CALL MPI_Get_address (casapool%plitter(off,1), displs(bidx), ierr)
    blen(bidx) = r2len * mlitter

    bidx = bidx + 1
    CALL MPI_Get_address (casapool%psoil(off,1), displs(bidx), ierr)
    blen(bidx) = r2len * msoil

    bidx = bidx + 1
    CALL MPI_Get_address (casabal%cplantlast(off,1), displs(bidx), ierr)
    blen(bidx) = r2len * mplant

    bidx = bidx + 1
    CALL MPI_Get_address (casabal%clitterlast(off,1), displs(bidx), ierr)
    blen(bidx) = r2len * mlitter

    bidx = bidx + 1
    CALL MPI_Get_address (casabal%csoillast(off,1), displs(bidx), ierr)
    blen(bidx) = r2len * msoil

    bidx = bidx + 1
    CALL MPI_Get_address (casapool%Nsoilmin(off), displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casapool%clabile(off), displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casapool%ctot(off), displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casabal%FCneeyear(off), displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casabal%clabilelast(off), displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%fHarvest(off), displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%cHarvest(off), displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%nHarvest(off), displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%fcrop(off), displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%FluxCtohwp(off), displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%FluxCtoclear(off), displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    CALL MPI_Get_address (casaflux%CtransferLUC(off), displs(bidx), ierr)
    blen(bidx) = r2len

    ! MPI: sanity check
    IF (bidx /= ntyp) THEN
       WRITE (*,*) 'worker ',rank,' invalid number of casa_LUC_t param fields ',bidx,', fix it (28)!'
       CALL MPI_Abort (comm, 62, ierr)
    END IF

    CALL MPI_Type_create_struct (bidx, blen, displs, types, casa_LUC_t, ierr)
    CALL MPI_Type_commit (casa_LUC_t, ierr)

    CALL MPI_Type_size (casa_LUC_t, tsize, ierr)
    CALL MPI_Type_get_extent (casa_LUC_t, tmplb, text, ierr)

    WRITE (*,*) 'worker casa_LUC_t param blocks, size, extent and lb: ',rank, &
         bidx,tsize,text,tmplb

    ! MPI: check whether total size of received data equals total
    ! data sent by all the workers
    CALL MPI_Reduce (tsize, MPI_DATATYPE_NULL, 1, MPI_INTEGER, MPI_SUM, 0, comm, ierr)

    DEALLOCATE(types)
    DEALLOCATE(displs)
    DEALLOCATE(blen)

    ! ! if anything went wrong the master will mpi_abort
    ! ! which mpi_recv below is going to catch...
    ! ! so, now receive all the parameters
    ! CALL MPI_Recv (MPI_BOTTOM, 1, casa_dump_t, 0, 0, comm, stat, ierr)

    ! ! finally free the MPI type
    ! Call MPI_Type_Free (casa_dump_t, ierr)

    ! all casa parameters have been received from the master by now

  END SUBROUTINE worker_casa_LUC_types


  SUBROUTINE worker_pop_types(comm, veg, pop)

    use mpi
    USE POP_mpi
    USE POPmodule,          ONLY: POP_INIT
    USE POP_types,          ONLY: pop_type
    USE cable_def_types_mod,ONLY: veg_parameter_type
    USE cable_common_module,ONLY: cable_user

    IMPLICIT NONE

    INTEGER,         INTENT(IN)    :: comm
    TYPE (pop_type), INTENT(INOUT) :: pop
    TYPE (veg_parameter_type),INTENT(IN) :: veg

    INTEGER, DIMENSION(:), Allocatable :: Iwood, ainv
    INTEGER :: mp_pop, stat(MPI_STATUS_SIZE), ierr

    INTEGER :: rank, inv

    CALL MPI_Comm_rank (comm, rank, ierr)

    ! Get POP relevant info from Master
    CALL MPI_Recv ( mp_pop, 1, MPI_INTEGER, 0, 0, comm, stat, ierr )
    write(*,*) 'worker iwood to allocate', rank, mp_pop, mp
    !write(*,*) 'worker mppop', rank, mp_pop
    !ALLOCATE( POP%Iwood( mp_pop ) )
    ALLOCATE( Iwood( mp_pop ) )
    write(*,*) 'worker iwood allocated', rank, mp_pop
    !CALL MPI_Recv ( POP%Iwood, mp_pop, MPI_INTEGER, 0, 0, comm, stat, ierr )
    CALL MPI_Recv( Iwood, mp_pop, MPI_INTEGER, 0, 0, comm, stat, ierr )
    !write(*,*),'worker Iwood', rank, POP%Iwood
    ! Maciej
    IF (ANY (Iwood < 1) .OR. ANY (Iwood > mp)) THEN
       write(*,*) 'worker iwood values outside valid ranges', rank
       inv = COUNT(Iwood < 1)
       IF (inv .gt. 0) THEN
          write(*,*) 'no of values below 1: ', inv
          ALLOCATE (ainv(inv))
          ainv = PACK (Iwood, Iwood .lt. 1)
          write (*,*) 'values below 1: ', ainv
          DEALLOCATE(ainv)
       END IF
       inv = COUNT(Iwood > mp)
       IF (inv .gt. 0) THEN
          write(*,*) 'no of values above mp ', mp, inv
          ALLOCATE(ainv(inv))
          ainv = PACK(Iwood, Iwood .gt. mp)
          write (*,*) 'values above mp: ', ainv
          DEALLOCATE (ainv)
       END IF

    END IF
    ! maciej: unnecessary, pop_init starts with a call to alloc_pop
    ! CALL ALLOC_POP ( POP, mp_pop )
    !CALL POP_INIT( pop,veg%disturbance_interval,mp_pop,POP%Iwood )
    ! maciej: veg%disturbance_levels received earlier in worker_cable_params
    CALL POP_INIT( pop,veg%disturbance_interval,mp_pop,Iwood )

    DEALLOCATE (Iwood)

    ! Maciej: pop_t used also for sending results back to master, even if from zero
    CALL create_pop_gridcell_type (pop_t, comm)

    IF (.NOT. CABLE_USER%POP_fromZero ) THEN
       write(*,*) 'rank receiving pop_grid from master', rank
       CALL MPI_Recv( POP%pop_grid(1), mp_pop, pop_t, 0, 0, comm, stat, ierr )
       CALL MPI_Recv( POP%it_pop(1), mp_pop, MPI_INTEGER, 0, 0, comm, stat, ierr )
    END IF

  END SUBROUTINE worker_pop_types


  ! 13C
  ! MPI: creates param_t type for the worker to receive the default casa parameters from the master process
  ! then receives them
  ! and finally frees the MPI type
  subroutine worker_c13o2_flux_params(comm, c13o2flux)

    use mpi
    ! use mpi,                 only: &
    !      MPI_ADDRESS_KIND, MPI_STATUS_SIZE, MPI_Comm_rank, MPI_BYTE, MPI_Get_address, &
    !      MPI_Abort, MPI_Type_create_struct, MPI_Type_commit, MPI_Type_size, &
    !      MPI_Type_get_extent, MPI_Reduce, MPI_DATATYPE_NULL, MPI_INTEGER, MPI_Sum, &
    !      MPI_Barrier, MPI_Recv, MPI_Get_count, MPI_SUCCESS, MPI_Unpack, MPI_BOTTOM, MPI_Type_Free
    use cable_def_types_mod, only: mp, mf
    use cable_c13o2_def,     only: c13o2_flux, c13o2_alloc_flux, c13o2_zero_flux

    implicit none

    integer,          intent(in)  :: comm  ! mpi communicator
    type(c13o2_flux), intent(out) :: c13o2flux

    ! local vars

    ! temp arrays for marshalling all fields into a single struct
    integer, allocatable, dimension(:) :: blen
    integer(kind=MPI_ADDRESS_KIND), allocatable, dimension(:) :: displs
    integer, allocatable, dimension(:) :: types

    ! temp vars for verifying block number and total length of inp_t
    integer(kind=MPI_ADDRESS_KIND) :: text, tmplb
    integer :: tsize

    integer :: stat(MPI_STATUS_SIZE), ierr
    integer :: c13o2_flux_ts

    integer :: r1len, r2len, i1len, llen ! block lengths
    integer :: bidx ! block index
    integer :: ntyp ! total number of blocks

    integer :: rank, off, ierr2, rcount, pos

    character, dimension(:), allocatable :: rbuf

    off = 1

    call MPI_Comm_rank(comm, rank, ierr)

    if (.not. associated(c13o2flux%ca)) then
       write(*,*) 'worker alloc c13o2_flux with m patches: ', rank, mp
       call c13o2_alloc_flux(c13o2flux, mp)
       call c13o2_zero_flux(c13o2flux)
    end if

    ntyp = nc13o2_flux

    allocate(blen(ntyp))
    allocate(displs(ntyp))
    allocate(types(ntyp))

    ! default type is byte, to be overriden for multi-D types
    types = MPI_BYTE

    r1len = mp * extr1
    r2len = mp * extr2
    i1len = mp * extid
    llen  = mp * extl

    bidx = 0

    ! -------

    bidx = bidx + 1
    call MPI_Get_address(c13o2flux%ca, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    call MPI_Get_address(c13o2flux%cAn12, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    call MPI_Get_address(c13o2flux%cAn, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    call MPI_Get_address(c13o2flux%RAn, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    call MPI_Get_address(c13o2flux%Vstarch, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    call MPI_Get_address(c13o2flux%Rstarch, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    call MPI_Get_address(c13o2flux%An, displs(bidx), ierr)
    blen(bidx) = mf * r2len

    bidx = bidx + 1
    call MPI_Get_address(c13o2flux%Disc, displs(bidx), ierr)
    blen(bidx) = mf * r2len

    bidx = bidx + 1
    call MPI_Get_address(c13o2flux%Rsucrose, displs(bidx), ierr)
    blen(bidx) = mf * r2len

    bidx = bidx + 1
    call MPI_Get_address(c13o2flux%Rphoto, displs(bidx), ierr)
    blen(bidx) = mf * r2len

    ! -------

    ! MPI: sanity check
    if (bidx /= ntyp) then
       write(*,*) 'worker ', rank, ' invalid number of c13o2_flux_ts param fields ', bidx, ', fix it (29)!'
       call MPI_Abort(comm, 63, ierr)
    end if

    call MPI_Type_create_struct(bidx, blen, displs, types, c13o2_flux_ts, ierr)
    call MPI_Type_commit(c13o2_flux_ts, ierr)

    call MPI_Type_size(c13o2_flux_ts, tsize, ierr)
    call MPI_Type_get_extent(c13o2_flux_ts, tmplb, text, ierr)

    write(*,*) 'worker c13o2_flux_ts param blocks, size, extent and lb: ', rank, bidx, tsize, text, tmplb

    ! MPI: check whether total size of received data equals total data sent by all the workers
    call MPI_Reduce(tsize, MPI_DATATYPE_NULL, 1, MPI_INTEGER, MPI_Sum, 0, comm, ierr)

    deallocate(types)
    deallocate(displs)
    deallocate(blen)

    ! if anything went wrong the master will mpi_abort
    ! which mpi_recv below is going to catch...
    call MPI_Barrier(comm, ierr)

    ! so, now receive all the parameters
    ! CALL MPI_Recv (MPI_BOTTOM, 1, c13o2_flux_ts, 0, 0, comm, stat, ierr)

    ! Maciej: buffered recv + unpac version
    allocate(rbuf(tsize))
    call MPI_Recv(rbuf, tsize, MPI_BYTE, 0, 0, comm, stat, ierr)
    call MPI_Get_count(stat, c13o2_flux_ts, rcount, ierr2)

    if (ierr == MPI_SUCCESS .AND. ierr2 == MPI_SUCCESS .AND. rcount == 1) THEN
       pos = 0
       call MPI_Unpack(rbuf, tsize, pos, MPI_BOTTOM, rcount, c13o2_flux_ts, comm, ierr)
       if (ierr /= MPI_SUCCESS) write(*,*) 'c13o2_flux params unpack error, rank: ',rank,ierr
    ELSE
       write(*,*) 'c13o2_flux params recv rank err err2 rcount: ', rank, ierr, ierr2, rcount
    END IF

    DEALLOCATE(rbuf)

    ! finally free the MPI type
    call MPI_Type_Free(c13o2_flux_ts, ierr)

    ! all c13o2_flux parameters have been received from the master by now
    return

  end subroutine worker_c13o2_flux_params


  subroutine worker_c13o2_pool_params(comm, c13o2pools)

    use mpi
    ! use mpi,                 only: &
    !      MPI_ADDRESS_KIND, MPI_STATUS_SIZE, MPI_Comm_rank, MPI_BYTE, MPI_Get_address, &
    !      MPI_Abort, MPI_Type_create_struct, MPI_Type_commit, MPI_Type_size, &
    !      MPI_Type_get_extent, MPI_Reduce, MPI_DATATYPE_NULL, MPI_INTEGER, MPI_Sum, &
    !      MPI_Barrier, MPI_Recv, MPI_Get_count, MPI_SUCCESS, MPI_Unpack, MPI_BOTTOM, MPI_Type_Free
    use cable_def_types_mod, only: mp
    use casadimension,       only: mplant, mlitter, msoil
    use cable_c13o2_def,     only: c13o2_pool, c13o2_alloc_pools, c13o2_zero_pools

    implicit none

    integer,          intent(in)  :: comm  ! mpi communicator
    type(c13o2_pool), intent(out) :: c13o2pools

    ! local vars

    ! temp arrays for marshalling all fields into a single struct
    integer, allocatable, dimension(:) :: blen
    integer(kind=MPI_ADDRESS_KIND), allocatable, dimension(:) :: displs
    integer, allocatable, dimension(:) :: types

    ! temp vars for verifying block number and total length of inp_t
    integer(kind=MPI_ADDRESS_KIND) :: text, tmplb
    integer :: tsize

    integer :: stat(MPI_STATUS_SIZE), ierr
    integer :: c13o2_pool_ts

    integer :: r1len, r2len, i1len, llen ! block lengths
    integer :: bidx ! block index
    integer :: ntyp ! total number of blocks

    integer :: rank, off, ierr2, rcount, pos

    character, dimension(:), allocatable :: rbuf

    off = 1

    call MPI_Comm_rank(comm, rank, ierr)

    if (.not. associated(c13o2pools%cplant)) then
       write(*,*) 'worker alloc c13o2_pool with m patches: ', rank, mp
       call c13o2_alloc_pools(c13o2pools, mp)
       call c13o2_zero_pools(c13o2pools)
    end if

    ntyp = nc13o2_pool

    allocate(blen(ntyp))
    allocate(displs(ntyp))
    allocate(types(ntyp))

    ! default type is byte, to be overriden for multi-D types
    types = MPI_BYTE

    r1len = mp * extr1
    r2len = mp * extr2
    i1len = mp * extid
    llen  = mp * extl

    bidx = 0

    ! -------

    bidx = bidx + 1
    call MPI_Get_address(c13o2pools%clabile, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    call MPI_Get_address(c13o2pools%charvest, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    call MPI_Get_address(c13o2pools%cplant, displs(bidx), ierr)
    blen(bidx) = mplant * r2len

    bidx = bidx + 1
    call MPI_Get_address(c13o2pools%clitter, displs(bidx), ierr)
    blen(bidx) = mlitter * r2len

    bidx = bidx + 1
    call MPI_Get_address(c13o2pools%csoil, displs(bidx), ierr)
    blen(bidx) = msoil * r2len

    ! -------

    ! MPI: sanity check
    if (bidx /= ntyp) then
       write(*,*) 'worker ', rank, ' invalid number of c13o2_pool_ts param fields ', bidx, ', fix it (30)!'
       call MPI_Abort(comm, 64, ierr)
    end if

    call MPI_Type_create_struct(bidx, blen, displs, types, c13o2_pool_ts, ierr)
    call MPI_Type_commit(c13o2_pool_ts, ierr)

    call MPI_Type_size(c13o2_pool_ts, tsize, ierr)
    call MPI_Type_get_extent(c13o2_pool_ts, tmplb, text, ierr)

    write(*,*) 'worker c13o2_pool_ts param blocks, size, extent and lb: ', rank, bidx, tsize, text, tmplb

    ! MPI: check whether total size of received data equals total data sent by all the workers
    call MPI_Reduce(tsize, MPI_DATATYPE_NULL, 1, MPI_INTEGER, MPI_Sum, 0, comm, ierr)

    deallocate(types)
    deallocate(displs)
    deallocate(blen)

    ! if anything went wrong the master will mpi_abort
    ! which mpi_recv below is going to catch...
    call MPI_Barrier(comm, ierr)

    ! so, now receive all the parameters
    ! call MPI_Recv(MPI_BOTTOM, 1, c13o2_pool_ts, 0, 0, comm, stat, ierr)

    ! Maciej: buffered recv + unpac version
    allocate(rbuf(tsize))
    call MPI_Recv(rbuf, tsize, MPI_BYTE, 0, 0, comm, stat, ierr)
    call MPI_Get_count(stat, c13o2_pool_ts, rcount, ierr2)

    if (ierr == MPI_SUCCESS .AND. ierr2 == MPI_SUCCESS .AND. rcount == 1) THEN
       pos = 0
       call MPI_Unpack(rbuf, tsize, pos, MPI_BOTTOM, rcount, c13o2_pool_ts, comm, ierr)
       if (ierr /= MPI_SUCCESS) write(*,*) 'c13o2_pool params unpack error, rank: ',rank,ierr
    else
       write(*,*) 'c13o2_pool params recv rank err err2 rcount: ', rank, ierr, ierr2, rcount
    end if

    deallocate(rbuf)

    ! finally free the MPI type
    call MPI_Type_Free(c13o2_pool_ts, ierr)

    ! all c13o2_pool parameters have been received from the master by now
    return

  end subroutine worker_c13o2_pool_params


  subroutine worker_c13o2_luc_params(comm, c13o2luc)

    use mpi
    ! use mpi,                 only: &
    !      MPI_ADDRESS_KIND, MPI_STATUS_SIZE, MPI_Comm_rank, MPI_BYTE, MPI_Get_address, &
    !      MPI_Abort, MPI_Type_create_struct, MPI_Type_commit, MPI_Type_size, &
    !      MPI_Type_get_extent, MPI_Reduce, MPI_DATATYPE_NULL, MPI_INTEGER, MPI_Sum, &
    !      MPI_Barrier, MPI_Recv, MPI_Get_count, MPI_SUCCESS, MPI_Unpack, MPI_BOTTOM, MPI_Type_Free
    use cable_def_types_mod, only: mland
    ! use cable_def_types_mod, only: mp
    use cable_c13o2_def,     only: c13o2_luc, c13o2_alloc_luc, c13o2_zero_luc

    implicit none

    integer,         intent(in)  :: comm  ! mpi communicator
    type(c13o2_luc), intent(out) :: c13o2luc

    ! local vars

    ! temp arrays for marshalling all fields into a single struct
    integer, allocatable, dimension(:) :: blen
    integer(kind=MPI_ADDRESS_KIND), allocatable, dimension(:) :: displs
    integer, allocatable, dimension(:) :: types

    ! temp vars for verifying block number and total length of inp_t
    integer(kind=MPI_ADDRESS_KIND) :: text, tmplb
    integer :: tsize

    integer :: stat(MPI_STATUS_SIZE), ierr
    integer :: c13o2_luc_ts

    integer :: r1len, r2len, i1len, llen ! block lengths
    integer :: bidx ! block index
    integer :: ntyp ! total number of blocks

    integer :: rank, off, ierr2, rcount, pos

    character, dimension(:), allocatable :: rbuf

    off = 1

    call MPI_Comm_rank(comm, rank, ierr)

    if (.not. associated(c13o2luc%charvest)) then
       write(*,*) 'worker alloc c13o2_luc with m land points: ', rank, mland
       call c13o2_alloc_luc(c13o2luc, mland)
       ! write(*,*) 'worker alloc c13o2_luc with m land points: ', rank, mp
       ! call c13o2_alloc_luc(c13o2luc, mp)
       call c13o2_zero_luc(c13o2luc)
    end if

    ntyp = nc13o2_luc

    allocate(blen(ntyp))
    allocate(displs(ntyp))
    allocate(types(ntyp))

    ! default type is byte, to be overriden for multi-D types
    types = MPI_BYTE

    r1len = mland * extr1
    r2len = mland * extr2
    i1len = mland * extid
    llen  = mland * extl
    ! r1len = mp * extr1
    ! r2len = mp * extr2
    ! i1len = mp * extid
    ! llen  = mp * extl

    bidx = 0

    ! -------

    bidx = bidx + 1
    call MPI_Get_address(c13o2luc%cagric, displs(bidx), ierr)
    blen(bidx) = r2len

    bidx = bidx + 1
    call MPI_Get_address(c13o2luc%charvest, displs(bidx), ierr)
    blen(bidx) = c13o2luc%nharvest * r2len

    bidx = bidx + 1
    call MPI_Get_address(c13o2luc%cclearance, displs(bidx), ierr)
    blen(bidx) = c13o2luc%nclearance * r2len

    ! -------

    ! MPI: sanity check
    if (bidx /= ntyp) then
       write(*,*) 'worker ', rank, ' invalid number of c13o2_luc_ts param fields ', bidx, ', fix it (31)!'
       call MPI_Abort(comm, 65, ierr)
    end if

    call MPI_Type_create_struct(bidx, blen, displs, types, c13o2_luc_ts, ierr)
    call MPI_Type_commit(c13o2_luc_ts, ierr)

    call MPI_Type_size(c13o2_luc_ts, tsize, ierr)
    call MPI_Type_get_extent(c13o2_luc_ts, tmplb, text, ierr)

    write(*,*) 'worker c13o2_luc_ts param blocks, size, extent and lb: ', rank, bidx, tsize, text, tmplb

    ! MPI: check whether total size of received data equals total data sent by all the workers
    call MPI_Reduce(tsize, MPI_DATATYPE_NULL, 1, MPI_INTEGER, MPI_Sum, 0, comm, ierr)

    deallocate(types)
    deallocate(displs)
    deallocate(blen)

    ! if anything went wrong the master will mpi_abort
    ! which mpi_recv below is going to catch...
    call MPI_Barrier(comm, ierr)

    ! so, now receive all the parameters
    ! CALL MPI_Recv (MPI_BOTTOM, 1, c13o2_luc_ts, 0, 0, comm, stat, ierr)

    ! Maciej: buffered recv + unpac version
    allocate(rbuf(tsize))
    call MPI_Recv(rbuf, tsize, MPI_BYTE, 0, 0, comm, stat, ierr)
    call MPI_Get_count(stat, c13o2_luc_ts, rcount, ierr2)

    if (ierr == MPI_SUCCESS .AND. ierr2 == MPI_SUCCESS .AND. rcount == 1) THEN
       pos = 0
       call MPI_Unpack(rbuf, tsize, pos, MPI_BOTTOM, rcount, c13o2_luc_ts, comm, ierr)
       if (ierr /= MPI_SUCCESS) write(*,*) 'c13o2_luc params unpack error, rank: ',rank,ierr
    ELSE
       write(*,*) 'c13o2_luc params recv rank err err2 rcount: ', rank, ierr, ierr2, rcount
    END IF

    DEALLOCATE(rbuf)

    ! finally free the MPI type
    call MPI_Type_Free(c13o2_luc_ts, ierr)

    ! all c13o2_luc parameters have been received from the master by now
    return

  end subroutine worker_c13o2_luc_params


  ! creates MPI types for sending c13o2 results back to the master at the end of the simulation
  subroutine worker_c13o2_flux_type(comm, c13o2flux)

    use mpi
    ! use mpi,                 only: &
    !      MPI_BYTE, MPI_ADDRESS_KIND, MPI_Get_address, MPI_Abort, MPI_Type_create_struct, &
    !      MPI_Type_commit, MPI_Type_size, MPI_Type_get_extent, MPI_Reduce, &
    !      MPI_DATATYPE_NULL, MPI_INTEGER, MPI_Sum
    use cable_def_types_mod, only: mp, mf
    use cable_c13o2_def,     only: c13o2_flux

    implicit none

    ! subroutine arguments
    integer,          intent(in)    :: comm ! MPI communicator to talk to the workers
    type(c13o2_flux), intent(inout) :: c13o2flux

    ! local variables

    ! MPI: temp arrays for marshalling all types into a struct
    integer, allocatable, dimension(:) :: blocks
    integer(kind=MPI_ADDRESS_KIND), allocatable, dimension(:) :: displs
    integer, allocatable, dimension(:) :: types
    integer :: ntyp ! number of worker's types

    ! MPI: block lengths and strides for hvector representing matrices
    integer :: r1len, r2len, i1len, llen

    integer :: off, cnt
    integer :: bidx, ierr

    integer :: tsize
    integer(kind=MPI_ADDRESS_KIND) :: text, tmplb

    ! MPI: allocate temp vectors used for marshalling
    ntyp = nc13o2_flux
    allocate(blocks(ntyp))
    allocate(displs(ntyp))
    allocate(types(ntyp))

    !off = wpatch%patch0
    !cnt = wpatch%npatch
    off = 1
    cnt = mp

    r1len = cnt * extr1
    r2len = cnt * extr2
    i1len = cnt * extid
    llen  = cnt * extl

    bidx = 0

    ! ------------- 2D arrays -------------

    bidx = bidx + 1
    call MPI_Get_address(c13o2flux%An(off,1), displs(bidx), ierr)
    blocks(bidx) = mf * r2len

    bidx = bidx + 1
    call MPI_Get_address(c13o2flux%Disc(off,1), displs(bidx), ierr)
    blocks(bidx) = mf * r2len

    bidx = bidx + 1
    call MPI_Get_address(c13o2flux%Rsucrose(off,1), displs(bidx), ierr)
    blocks(bidx) = mf * r2len

    bidx = bidx + 1
    call MPI_Get_address(c13o2flux%Rphoto(off,1), displs(bidx), ierr)
    blocks(bidx) = mf * r2len

    ! ------------- 1D vectors -------------

    bidx = bidx + 1
    call MPI_Get_address(c13o2flux%ca(off), displs(bidx), ierr)
    blocks(bidx) = r2len

    bidx = bidx + 1
    call MPI_Get_address(c13o2flux%cAn12(off), displs(bidx), ierr)
    blocks(bidx) = r2len

    bidx = bidx + 1
    call MPI_Get_address(c13o2flux%cAn(off), displs(bidx), ierr)
    blocks(bidx) = r2len

    bidx = bidx + 1
    call MPI_Get_address(c13o2flux%RAn(off), displs(bidx), ierr)
    blocks(bidx) = r2len

    bidx = bidx + 1
    call MPI_Get_address(c13o2flux%Vstarch(off), displs(bidx), ierr)
    blocks(bidx) = r2len

    bidx = bidx + 1
    call MPI_Get_address(c13o2flux%Rstarch(off), displs(bidx), ierr)
    blocks(bidx) = r2len

    ! MPI: sanity check
    if (bidx /= ntyp) then
       write(*,*) 'worker: invalid number of c13o2_flux fields, fix it (32)!'
       write(*,*) 'ntyp: ', ntyp, 'bidx: ', bidx
       call MPI_Abort(comm, 66, ierr)
    end if

    types = MPI_BYTE

    call MPI_Type_create_struct(bidx, blocks, displs, types, c13o2_flux_t, ierr)
    call MPI_Type_commit(c13o2_flux_t, ierr)

    call MPI_Type_size(c13o2_flux_t, tsize, ierr)
    call MPI_Type_get_extent(c13o2_flux_t, tmplb, text, ierr)

    write(*,*) 'c13o2_flux type struct blocks, size, extent and lb: ', bidx, tsize, text, tmplb

    ! MPI: check whether total size of received data equals total
    ! data sent by all the workers
    call MPI_Reduce(tsize, MPI_DATATYPE_NULL, 1, MPI_INTEGER, MPI_Sum, 0, comm, ierr)

    deallocate(types)
    deallocate(displs)
    deallocate(blocks)

    return

  end subroutine worker_c13o2_flux_type


  subroutine worker_c13o2_pool_type(comm, c13o2pools)

    use mpi
    ! use mpi,                 only: &
    !      MPI_BYTE, MPI_ADDRESS_KIND, MPI_Get_address, MPI_Abort, MPI_Type_create_struct, &
    !      MPI_Type_commit, MPI_Type_size, MPI_Type_get_extent, MPI_Reduce, &
    !      MPI_DATATYPE_NULL, MPI_INTEGER, MPI_Sum
    use cable_def_types_mod, only: mp
    use casadimension,       only: mplant, mlitter, msoil
    use cable_c13o2_def,     only: c13o2_pool

    implicit none

    ! subroutine arguments
    integer,          intent(in)    :: comm ! MPI communicator to talk to the workers
    type(c13o2_pool), intent(inout) :: c13o2pools

    ! local variables

    ! MPI: temp arrays for marshalling all types into a struct
    integer, allocatable, dimension(:) :: blocks
    integer(kind=MPI_ADDRESS_KIND), allocatable, dimension(:) :: displs
    integer, allocatable, dimension(:) :: types
    integer :: ntyp ! number of worker's types

    ! MPI: block lengths and strides for hvector representing matrices
    integer :: r1len, r2len, i1len, llen

    integer :: off, cnt
    integer :: bidx, ierr

    integer :: tsize
    integer(kind=MPI_ADDRESS_KIND) :: text, tmplb

    ! MPI: allocate temp vectors used for marshalling
    ntyp = nc13o2_pool
    allocate(blocks(ntyp))
    allocate(displs(ntyp))
    allocate(types(ntyp))

    !off = wpatch%patch0
    !cnt = wpatch%npatch
    off = 1
    cnt = mp

    r1len = cnt * extr1
    r2len = cnt * extr2
    i1len = cnt * extid
    llen  = cnt * extl

    bidx = 0

    ! ------------- 2D arrays -------------

    bidx = bidx + 1
    call MPI_Get_address(c13o2pools%cplant(off,1), displs(bidx), ierr)
    blocks(bidx) = mplant * r2len

    bidx = bidx + 1
    call MPI_Get_address(c13o2pools%clitter(off,1), displs(bidx), ierr)
    blocks(bidx) = mlitter * r2len

    bidx = bidx + 1
    call MPI_Get_address(c13o2pools%csoil(off,1), displs(bidx), ierr)
    blocks(bidx) = msoil * r2len

    ! ------------- 1D vectors -------------

    bidx = bidx + 1
    call MPI_Get_address(c13o2pools%clabile(off), displs(bidx), ierr)
    blocks(bidx) = r2len

    bidx = bidx + 1
    call MPI_Get_address(c13o2pools%charvest(off), displs(bidx), ierr)
    blocks(bidx) = r2len

    ! MPI: sanity check
    if (bidx /= ntyp) then
       write(*,*) 'worker: invalid number of c13o2_pool fields, fix it (33)!'
       write(*,*) 'ntyp: ', ntyp, 'bidx: ', bidx
       call MPI_Abort(comm, 67, ierr)
    end if

    types = MPI_BYTE

    call MPI_Type_create_struct(bidx, blocks, displs, types, c13o2_pool_t, ierr)
    call MPI_Type_commit(c13o2_pool_t, ierr)

    call MPI_Type_size(c13o2_pool_t, tsize, ierr)
    call MPI_Type_get_extent(c13o2_pool_t, tmplb, text, ierr)

    write(*,*) 'c13o2_pool type struct blocks, size, extent and lb: ', bidx, tsize, text, tmplb

    ! MPI: check whether total size of received data equals total
    ! data sent by all the workers
    call MPI_Reduce(tsize, MPI_DATATYPE_NULL, 1, MPI_INTEGER, MPI_Sum, 0, comm, ierr)

    deallocate(types)
    deallocate(displs)
    deallocate(blocks)

    return

  end subroutine worker_c13o2_pool_type


  subroutine worker_c13o2_luc_type(comm, c13o2luc)

    use mpi
    ! use mpi,                 only: &
    !      MPI_BYTE, MPI_ADDRESS_KIND, MPI_Get_address, MPI_Abort, MPI_Type_create_struct, &
    !      MPI_Type_commit, MPI_Type_size, MPI_Type_get_extent, MPI_Reduce, &
    !      MPI_DATATYPE_NULL, MPI_INTEGER, MPI_Sum
    use cable_def_types_mod, only: mland
    ! use cable_def_types_mod, only: mp
    use cable_c13o2_def,     only: c13o2_luc
    use cable_mpicommon,     only: nc13o2_luc

    implicit none

    ! subroutine arguments
    integer,          intent(in)   :: comm ! MPI communicator to talk to the workers
    type(c13o2_luc), intent(inout) :: c13o2luc

    ! local variables

    ! MPI: temp arrays for marshalling all types into a struct
    integer, allocatable, dimension(:) :: blocks
    integer(kind=MPI_ADDRESS_KIND), allocatable, dimension(:) :: displs
    integer, allocatable, dimension(:) :: types
    integer :: ntyp ! number of worker's types

    ! MPI: block lengths and strides for hvector representing matrices
    integer :: r1len, r2len, i1len, llen

    integer :: off, cnt
    integer :: bidx, ierr

    integer :: tsize
    integer(kind=MPI_ADDRESS_KIND) :: text, tmplb

    ! MPI: allocate temp vectors used for marshalling
    ntyp = nc13o2_luc
    allocate(blocks(ntyp))
    allocate(displs(ntyp))
    allocate(types(ntyp))

    !off = wpatch%patch0
    !cnt = wpatch%npatch
    off = 1
    cnt = mland
    ! cnt = mp

    r1len = cnt * extr1
    r2len = cnt * extr2
    i1len = cnt * extid
    llen  = cnt * extl

    bidx = 0

    ! ------------- 2D arrays -------------

    bidx = bidx + 1
    call MPI_Get_address(c13o2luc%charvest(off,1), displs(bidx), ierr)
    blocks(bidx) = c13o2luc%nharvest * r2len

    bidx = bidx + 1
    call MPI_Get_address(c13o2luc%cclearance(off,1), displs(bidx), ierr)
    blocks(bidx) = c13o2luc%nclearance * r2len

    ! ------------- 1D vectors -------------

    bidx = bidx + 1
    call MPI_Get_address(c13o2luc%cagric(off), displs(bidx), ierr)
    blocks(bidx) = r2len

    ! MPI: sanity check
    if (bidx /= ntyp) then
       write(*,*) 'worker: invalid number of c13o2_luc fields, fix it (34)!'
       write(*,*) 'ntyp: ', ntyp, 'bidx: ', bidx
       call MPI_Abort(comm, 68, ierr)
    end if

    types = MPI_BYTE

    call MPI_Type_create_struct(bidx, blocks, displs, types, c13o2_luc_t, ierr)
    call MPI_Type_commit(c13o2_luc_t, ierr)

    call MPI_Type_size(c13o2_luc_t, tsize, ierr)
    call MPI_Type_get_extent(c13o2_luc_t, tmplb, text, ierr)

    write(*,*) 'c13o2_luc type struct blocks, size, extent and lb: ', bidx, tsize, text, tmplb

    ! MPI: check whether total size of received data equals total
    ! data sent by all the workers
    call MPI_Reduce(tsize, MPI_DATATYPE_NULL, 1, MPI_INTEGER, MPI_Sum, 0, comm, ierr)

    deallocate(types)
    deallocate(displs)
    deallocate(blocks)

    return

  end subroutine worker_c13o2_luc_type



SUBROUTINE worker_send_pop(POP, comm)

    use mpi
    use POP_mpi
    use POP_Types, only: POP_type

    implicit none

    integer,        intent(in) :: comm
    type(POP_type), intent(in) :: POP

    integer :: ierr

    if (POP%np .eq. 0) return

    call MPI_Send(POP%pop_grid(1), POP%np, pop_t, 0, 0, comm, ierr)
    call MPI_Send(POP%it_pop(1), POP%np, MPI_INTEGER, 0, 0, comm, ierr)

END SUBROUTINE worker_send_pop


! frees memory used for worker's data structures
SUBROUTINE worker_end(icycle, restart)

    use mpi
    use cable_common_module, only: cable_user

    implicit none

    integer, intent(in) :: icycle ! casa flag
    logical, intent(in) :: restart

    integer :: ierr

    call MPI_Type_free(inp_t, ierr)

    call MPI_Type_free(send_t, ierr)

    if (icycle>0) then
       call MPI_Type_free(casa_t, ierr)
       ! 13C
       if (cable_user%c13o2) then
          call MPI_Type_free(c13o2_flux_t, ierr)
          call MPI_Type_free(c13o2_pool_t, ierr)
       endif
    end if
    !MC - LUC is not freed. Does freeing objects close files as well?

    if (restart) then
       call MPI_Type_free(restart_t, ierr)
    end if

    return

END SUBROUTINE worker_end


!*********************************************************************************************

SUBROUTINE worker_spincasacnp(dels, kstart, kend, mloop, &
     veg, soil, casabiome, casapool, casaflux, casamet, casabal, phen, POP, climate, &
     LALLOC, c13o2flux, c13o2pools, icomm, ocomm)

  use cable_def_types_mod
  use cable_carbon_module
  use cable_common_module, only: cable_user
  use casadimension
  use casaparm
  use casavariable
  use phenvariable
  use POP_types,            only: POP_type
  use casa_cable,           only: POPdriver, analyticpool
  use casa_inout,           only: biogeochem
  use TypeDef,              only: dp
  ! 13C
  use cable_c13o2_def,      only: c13o2_pool, c13o2_flux
  use cable_c13o2,          only: c13o2_save_casapool, c13o2_update_pools, c13o2_sanity_pools
  use mo_isotope,           only: isoratio

  use mpi
  ! modules related to fire
  use blaze_mod,            only: type_blaze
  use simfire_mod,          only: type_simfire
  use POP_constants,        only: rshootfrac

  implicit none

  !!CLN  character(len=99), intent(in)  :: fcnpspin
  real,                      intent(in)    :: dels
  integer,                   intent(in)    :: kstart
  integer,                   intent(in)    :: kend
  integer,                   intent(in)    :: mloop
  integer,                   intent(in)    :: lalloc
  type(veg_parameter_type),  intent(inout) :: veg  ! vegetation parameters
  type(soil_parameter_type), intent(inout) :: soil ! soil parameters
  type(casa_biome),          intent(inout) :: casabiome
  type(casa_pool),           intent(inout) :: casapool
  type(casa_flux),           intent(inout) :: casaflux
  type(casa_met),            intent(inout) :: casamet
  type(casa_balance),        intent(inout) :: casabal
  type(phen_variable),       intent(inout) :: phen
  type(POP_type),            intent(inout) :: POP
  type(climate_type),        intent(inout) :: climate
  ! 13c
  type(c13o2_flux),          intent(inout) :: c13o2flux
  type(c13o2_pool),          intent(inout) :: c13o2pools
  ! communicator for error-messages
  integer,                   intent(in)  :: icomm, ocomm

  ! local variables
  real(r_2), dimension(:), allocatable, save  :: avg_cleaf2met, avg_cleaf2str, avg_croot2met, avg_croot2str, avg_cwood2cwd
  real(r_2), dimension(:), allocatable, save  :: avg_nleaf2met, avg_nleaf2str, avg_nroot2met, avg_nroot2str, avg_nwood2cwd
  real(r_2), dimension(:), allocatable, save  :: avg_pleaf2met, avg_pleaf2str, avg_proot2met, avg_proot2str, avg_pwood2cwd
  real(r_2), dimension(:), allocatable, save  :: avg_cgpp,      avg_cnpp,      avg_nuptake,   avg_puptake
  real(r_2), dimension(:), allocatable, save  :: avg_nsoilmin,  avg_psoillab,  avg_psoilsorb, avg_psoilocc
  ! chris 12/oct/2012 for spin up casa
  real(r_2), dimension(:), allocatable, save  :: avg_ratioNCsoilmic,  avg_ratioNCsoilslow,  avg_ratioNCsoilpass
  real(r_2), dimension(:), allocatable, save  :: avg_xnplimit,  avg_xkNlimiting,avg_xklitter, avg_xksoil

  ! local variables
  integer                  :: myearspin, nyear, nloop1, loy
  integer                  :: ktau,ktauday,nday,idoy,ktauy,nloop
  real(r_2), dimension(mp) :: cleaf2met, cleaf2str, croot2met, croot2str, cwood2cwd
  real(r_2), dimension(mp) :: nleaf2met, nleaf2str, nroot2met, nroot2str, nwood2cwd
  real(r_2), dimension(mp) :: pleaf2met, pleaf2str, proot2met, proot2str, pwood2cwd
  real(r_2), dimension(mp) :: xnplimit,  xknlimiting, xklitter, xksoil,xkleaf, xkleafcold, xkleafdry

  ! more variables to store the spinup pool size over the last 10 loops. Added by Yp Wang 30 Nov 2012
  integer, allocatable :: iw(:) ! array of indices corresponding to woody (shrub or forest) tiles
  real(dp) :: rday

  integer :: stat(MPI_STATUS_SIZE)
  integer :: ierr

  ! blaze variables
  type(type_blaze)   :: blaze
  type(type_simfire) :: simfire

  ! 13C
  real(r_2), dimension(:,:), allocatable :: casasave
  real(r_2), dimension(:), allocatable :: avg_c13leaf2met, avg_c13leaf2str, avg_c13root2met, &
       avg_c13root2str, avg_c13wood2cwd

  if (.not.allocated(iw)) allocate(iw(POP%np))

  !! vh_js !!
  if (cable_user%call_POP) Iw = POP%iwood

  ktauday = int(24.0*3600.0/dels)
  nday    = (kend-kstart+1)/ktauday
  loy     = 365

  ! 13C
  if (cable_user%c13o2) allocate(casasave(c13o2pools%ntile,c13o2pools%npools))

  ! chris 12/oct/2012 for spin up casa
  if (.not.(allocated(avg_cleaf2met)))  allocate(avg_cleaf2met(mp), avg_cleaf2str(mp), avg_croot2met(mp), avg_croot2str(mp), &
       avg_cwood2cwd(mp), &
       avg_nleaf2met(mp), avg_nleaf2str(mp), avg_nroot2met(mp), avg_nroot2str(mp), avg_nwood2cwd(mp), &
       avg_pleaf2met(mp), avg_pleaf2str(mp), avg_proot2met(mp), avg_proot2str(mp), avg_pwood2cwd(mp), &
       avg_cgpp(mp),      avg_cnpp(mp),      avg_nuptake(mp),   avg_puptake(mp),                     &
       avg_xnplimit(mp),  avg_xkNlimiting(mp), avg_xklitter(mp), avg_xksoil(mp),                      &
       avg_rationcsoilmic(mp),avg_rationcsoilslow(mp),avg_rationcsoilpass(mp),                        &
       avg_nsoilmin(mp),  avg_psoillab(mp),    avg_psoilsorb(mp), avg_psoilocc(mp))
  ! 13C - allocate in any case even if cable_user%c13o2==.false. to pass to analytic soil and litter pools
  allocate(avg_c13leaf2met(mp))
  allocate(avg_c13leaf2str(mp))
  allocate(avg_c13root2met(mp))
  allocate(avg_c13root2str(mp))
  allocate(avg_c13wood2cwd(mp))

  myearspin = cable_user%casa_spin_endyear - cable_user%casa_spin_startyear + 1

  ! compute the mean fluxes and residence time of each carbon pool
  avg_cleaf2met       = 0.0_dp
  avg_cleaf2str       = 0.0_dp
  avg_croot2met       = 0.0_dp
  avg_croot2str       = 0.0_dp
  avg_cwood2cwd       = 0.0_dp
  avg_nleaf2met       = 0.0_dp
  avg_nleaf2str       = 0.0_dp
  avg_nroot2met       = 0.0_dp
  avg_nroot2str       = 0.0_dp
  avg_nwood2cwd       = 0.0_dp
  avg_pleaf2met       = 0.0_dp
  avg_pleaf2str       = 0.0_dp
  avg_proot2met       = 0.0_dp
  avg_proot2str       = 0.0_dp
  avg_pwood2cwd       = 0.0_dp
  avg_cgpp            = 0.0_dp
  avg_cnpp            = 0.0_dp
  avg_nuptake         = 0.0_dp
  avg_puptake         = 0.0_dp
  avg_xnplimit        = 0.0_dp
  avg_xkNlimiting     = 0.0_dp
  avg_xklitter        = 0.0_dp
  avg_xksoil          = 0.0_dp
  avg_nsoilmin        = 0.0_dp
  avg_psoillab        = 0.0_dp
  avg_psoilsorb       = 0.0_dp
  avg_psoilocc        = 0.0_dp
  avg_rationcsoilmic  = 0.0_dp
  avg_rationcsoilslow = 0.0_dp
  avg_rationcsoilpass = 0.0_dp
  ! 13C
  if (cable_user%c13o2) then
      avg_c13leaf2met = 0.0_dp
      avg_c13leaf2str = 0.0_dp
      avg_c13root2met = 0.0_dp
      avg_c13root2str = 0.0_dp
      avg_c13wood2cwd = 0.0_dp
   endif

  do nyear=1, myearspin
     ! WRITE(CYEAR,FMT="(I4)") CABLE_USER%CASA_SPIN_STARTYEAR + nyear - 1
     ! ncfile = TRIM(casafile%c2cdumppath)//'c2c_'//CYEAR//'_dump.nc'
     ! call read_casa_dump( ncfile,casamet, casaflux, phen,climate, ktau ,kend,.TRUE. )
     do idoy=1, mdyear
        ktau = (idoy-1)*ktauday + 1
        call MPI_Recv(MPI_BOTTOM, 1, casa_dump_t, 0, idoy, icomm, stat, ierr)
        ! 13C
        if (cable_user%c13o2) call c13o2_save_casapool(casapool, casasave)
        call biogeochem(idoy,LALLOC,veg,soil,casabiome,casapool,casaflux, &
             casamet,casabal,phen,POP,climate,xnplimit,xkNlimiting,xklitter, &
             xksoil,xkleaf,xkleafcold,xkleafdry,&
             cleaf2met,cleaf2str,croot2met,croot2str,cwood2cwd,         &
             nleaf2met,nleaf2str,nroot2met,nroot2str,nwood2cwd,         &
             pleaf2met,pleaf2str,proot2met,proot2str,pwood2cwd)
        ! 13C
        if (cable_user%c13o2) then
           avg_c13leaf2met(:) = avg_c13leaf2met(:) + &
                cleaf2met(:) * isoratio(c13o2pools%cplant(:,leaf), casasave(:,leaf), 0.0_dp, tiny(1.0_dp)) ! 1.0_dp
           avg_c13leaf2str(:) = avg_c13leaf2str(:) + &
                cleaf2str(:) * isoratio(c13o2pools%cplant(:,leaf), casasave(:,leaf), 0.0_dp, tiny(1.0_dp))
           avg_c13root2met(:) = avg_c13root2met(:) + &
                croot2met(:) * isoratio(c13o2pools%cplant(:,froot), casasave(:,froot), 0.0_dp, tiny(1.0_dp))
           avg_c13root2str(:) = avg_c13root2str(:) + &
                croot2str(:) * isoratio(c13o2pools%cplant(:,froot), casasave(:,froot), 0.0_dp, tiny(1.0_dp))
           avg_c13wood2cwd(:) = avg_c13wood2cwd(:) + &
                cwood2cwd(:) * isoratio(c13o2pools%cplant(:,wood), casasave(:,wood), 0.0_dp, tiny(1.0_dp))
           call c13o2_update_pools(casasave, casaflux, c13o2flux, c13o2pools)
           if (cable_user%c13o2) call c13o2_sanity_pools(casapool, casaflux, c13o2pools)
        endif

        if (cable_user%call_POP .and. POP%np.gt.0) then ! CALL_POP
           ! accumulate annual variables for use in POP
           if (mod(ktau/ktauday,loy)==1) then
              ! (assumes 70% of wood NPP is allocated above ground)
              casaflux%stemnpp  = casaflux%cnpp * casaflux%fracCalloc(:,2) * 0.7_dp
              !#9238
              casaflux%potstemnpp = casaflux%stemnpp + (casaflux%fracClabile * casaflux%cgpp)
              casabal%LAImax    = casamet%glai
              casabal%Cleafmean = casapool%cplant(:,1) / real(LOY,dp) / 1000.0_dp
              casabal%Crootmean = casapool%cplant(:,3) / real(LOY,dp) / 1000.0_dp
           else
              casaflux%stemnpp  = casaflux%stemnpp + casaflux%cnpp * casaflux%fracCalloc(:,2) * 0.7_dp
              !#9238
              casaflux%potstemnpp = casaflux%potstemnpp + (casaflux%cnpp * casaflux%fracCalloc(:,2) * 0.7_dp + &
                                                           casaflux%fracClabile * casaflux%cgpp)
              casabal%LAImax    = max(casamet%glai, casabal%LAImax)
              casabal%Cleafmean = casabal%Cleafmean + casapool%cplant(:,1) / real(LOY,dp) / 1000.0_dp
              casabal%Crootmean = casabal%Crootmean + casapool%cplant(:,3) / real(LOY,dp) / 1000.0_dp
           endif

           if (idoy==mdyear) then ! end of year
              call POPdriver(casaflux, casabal, veg, POP)
           endif  ! end of year
        else
           casaflux%stemnpp = 0.0_dp
           casaflux%potstemnpp = 0.0_dp  !#9238
        endif ! CALL_POP

        !CLN CALL BLAZE_DRIVER(...)

        ! WHERE(xkNlimiting .eq. 0)  !Chris Lu 4/June/2012
        !    xkNlimiting = 0.001
        ! END WHERE

        avg_cleaf2met       = avg_cleaf2met       + cleaf2met
        avg_cleaf2str       = avg_cleaf2str       + cleaf2str
        avg_croot2met       = avg_croot2met       + croot2met
        avg_croot2str       = avg_croot2str       + croot2str
        avg_cwood2cwd       = avg_cwood2cwd       + cwood2cwd

        avg_nleaf2met       = avg_nleaf2met       + nleaf2met
        avg_nleaf2str       = avg_nleaf2str       + nleaf2str
        avg_nroot2met       = avg_nroot2met       + nroot2met
        avg_nroot2str       = avg_nroot2str       + nroot2str
        avg_nwood2cwd       = avg_nwood2cwd       + nwood2cwd

        avg_pleaf2met       = avg_pleaf2met       + pleaf2met
        avg_pleaf2str       = avg_pleaf2str       + pleaf2str
        avg_proot2met       = avg_proot2met       + proot2met
        avg_proot2str       = avg_proot2str       + proot2str
        avg_pwood2cwd       = avg_pwood2cwd       + pwood2cwd

        avg_cgpp            = avg_cgpp            + casaflux%cgpp
        avg_cnpp            = avg_cnpp            + casaflux%cnpp
        avg_nuptake         = avg_nuptake         + casaflux%Nminuptake
        avg_puptake         = avg_puptake         + casaflux%Plabuptake

        avg_xnplimit        = avg_xnplimit        + xnplimit
        avg_xkNlimiting     = avg_xkNlimiting     + xkNlimiting
        avg_xklitter        = avg_xklitter        + xklitter
        avg_xksoil          = avg_xksoil          + xksoil

        avg_nsoilmin        = avg_nsoilmin        + casapool%nsoilmin
        avg_psoillab        = avg_psoillab        + casapool%psoillab
        avg_psoilsorb       = avg_psoilsorb       + casapool%psoilsorb
        avg_psoilocc        = avg_psoilocc        + casapool%psoilocc

        avg_rationcsoilmic  = avg_rationcsoilmic  + casapool%ratioNCsoilnew(:,mic)
        avg_rationcsoilslow = avg_rationcsoilslow + casapool%ratioNCsoilnew(:,slow)
        avg_rationcsoilpass = avg_rationcsoilpass + casapool%ratioNCsoilnew(:,pass)
     enddo ! idoy=1, mdyear
  enddo ! nyear=1, myearspin

  !!CLN    CLOSE(91)
  ! average
  rday = 1.0_dp / real(nday*myearspin, dp)

  avg_cleaf2met       = avg_cleaf2met       * rday
  avg_cleaf2str       = avg_cleaf2str       * rday
  avg_croot2met       = avg_croot2met       * rday
  avg_croot2str       = avg_croot2str       * rday
  avg_cwood2cwd       = avg_cwood2cwd       * rday

  avg_nleaf2met       = avg_nleaf2met       * rday
  avg_nleaf2str       = avg_nleaf2str       * rday
  avg_nroot2met       = avg_nroot2met       * rday
  avg_nroot2str       = avg_nroot2str       * rday
  avg_nwood2cwd       = avg_nwood2cwd       * rday

  avg_pleaf2met       = avg_pleaf2met       * rday
  avg_pleaf2str       = avg_pleaf2str       * rday
  avg_proot2met       = avg_proot2met       * rday
  avg_proot2str       = avg_proot2str       * rday
  avg_pwood2cwd       = avg_pwood2cwd       * rday

  avg_cgpp            = avg_cgpp            * rday
  avg_cnpp            = avg_cnpp            * rday
  avg_nuptake         = avg_nuptake         * rday
  avg_puptake         = avg_puptake         * rday

  avg_xnplimit        = avg_xnplimit        * rday
  avg_xkNlimiting     = avg_xkNlimiting     * rday
  avg_xklitter        = avg_xklitter        * rday
  avg_xksoil          = avg_xksoil          * rday

  avg_nsoilmin        = avg_nsoilmin        * rday
  avg_psoillab        = avg_psoillab        * rday
  avg_psoilsorb       = avg_psoilsorb       * rday
  avg_psoilocc        = avg_psoilocc        * rday

  avg_rationcsoilmic  = avg_rationcsoilmic  * rday
  avg_rationcsoilslow = avg_rationcsoilslow * rday
  avg_rationcsoilpass = avg_rationcsoilpass * rday

  ! 13C
  if (cable_user%c13o2) then
     avg_c13leaf2met = avg_c13leaf2met * rday
     avg_c13leaf2str = avg_c13leaf2str * rday
     avg_c13root2met = avg_c13root2met * rday
     avg_c13root2str = avg_c13root2str * rday
     avg_c13wood2cwd = avg_c13wood2cwd * rday
  endif

  call analyticpool(veg,soil,casabiome,casapool, &
       casaflux,casamet,casabal,                                         &
       avg_cleaf2met,avg_cleaf2str,avg_croot2met,avg_croot2str,avg_cwood2cwd, &
       avg_nleaf2met,avg_nleaf2str,avg_nroot2met,avg_nroot2str,avg_nwood2cwd, &
       avg_pleaf2met,avg_pleaf2str,avg_proot2met,avg_proot2str,avg_pwood2cwd, &
       avg_cnpp,                          &
       avg_xkNlimiting,avg_xklitter,avg_xksoil,                  &
       avg_ratioNCsoilmic,avg_ratioNCsoilslow,avg_ratioNCsoilpass,            &
       avg_nsoilmin,avg_psoillab,avg_psoilsorb,avg_psoilocc, &
       avg_c13leaf2met, avg_c13leaf2str, avg_c13root2met, &
       avg_c13root2str, avg_c13wood2cwd, c13o2pools)
  if (cable_user%c13o2) call c13o2_sanity_pools(casapool, casaflux, c13o2pools)

  nloop1= max(1,mloop-3)

  do nloop=1, mloop
     !!CLN  OPEN(91,file=fcnpspin)
     !!CLN  read(91,*)
     do nyear=1, myearspin
       ! WRITE(CYEAR,FMT="(I4)") CABLE_USER%CASA_SPIN_STARTYEAR + nyear - 1
       ! ncfile = TRIM(casafile%c2cdumppath)//'c2c_'//CYEAR//'_dump.nc'
       ! call read_casa_dump( ncfile, casamet, casaflux, phen,climate, ktau, kend, .TRUE. )
        do idoy=1, mdyear
           ktauy = idoy*ktauday
           ktau  = (idoy-1)*ktauday + 1
           call MPI_Recv(MPI_BOTTOM, 1, casa_dump_t, 0, idoy, icomm, stat, ierr)

           ! 13C
           if (cable_user%c13o2) call c13o2_save_casapool(casapool, casasave)
           call biogeochem(idoy,LALLOC,veg,soil,casabiome,casapool,casaflux, &
                casamet,casabal,phen,POP,climate,xnplimit,xkNlimiting,xklitter,xksoil,xkleaf,&
                xkleafcold,xkleafdry,&
                cleaf2met,cleaf2str,croot2met,croot2str,cwood2cwd,         &
                nleaf2met,nleaf2str,nroot2met,nroot2str,nwood2cwd,         &
                pleaf2met,pleaf2str,proot2met,proot2str,pwood2cwd)
           ! 13C
           if (cable_user%c13o2) then
              call c13o2_update_pools(casasave, casaflux, c13o2flux, c13o2pools)
              call c13o2_sanity_pools(casapool, casaflux, c13o2pools)
           endif

           !! CLN BLAZE here
           !!CLNblaze_spin_year = 1900 - myearspin + nyear
           !CLN CALL BLAZE_DRIVER(casapool,casaflux,shootfrac, met..., idoy,blaze_spin_year, DO_BLAZE_TO

           if (cable_user%call_POP .and. POP%np.gt.0) then ! CALL_POP
              ! accumulate annual variables for use in POP
              if (mod(ktau/ktauday,LOY) == 1) then
                 casaflux%stemnpp  =  casaflux%cnpp * casaflux%fracCalloc(:,2) * 0.7_dp
                 ! (assumes 70% of wood NPP is allocated above ground)
                 !#9238
                 casaflux%potstemnpp = casaflux%stemnpp + (casaflux%fracClabile * casaflux%cgpp)
                 casabal%LAImax    = casamet%glai
                 casabal%Cleafmean = casapool%cplant(:,1) / real(LOY,dp) / 1000.0_dp
                 casabal%Crootmean = casapool%cplant(:,3) / real(LOY,dp) / 1000.0_dp
              else
                 casaflux%stemnpp  = casaflux%stemnpp + casaflux%cnpp * casaflux%fracCalloc(:,2) * 0.7_dp
                 casaflux%potstemnpp = casaflux%potstemnpp + (casaflux%cnpp * casaflux%fracCalloc(:,2) * 0.7_dp + &
                                                              casaflux%fracClabile * casaflux%cgpp)
                 casabal%LAImax    = max(casamet%glai, casabal%LAImax)
                 casabal%Cleafmean = casabal%Cleafmean + casapool%cplant(:,1) / real(LOY,dp) / 1000.0_dp
                 casabal%Crootmean = casabal%Crootmean + casapool%cplant(:,3) / real(LOY,dp) / 1000.0_dp
              endif

              if (idoy==mdyear) then ! end of year
                 call POPdriver(casaflux, casabal, veg, POP)
                 ! CLN Check here accounting missing
                 ! call_blaze=0 if blaze off, >0 if blaze on to some extent
                 if (cable_user%call_blaze) then
                    call blaze_driver(blaze%ncells, blaze, simfire, casapool, casaflux, &
                         casamet, climate, rshootfrac, idoy, 1900, 1, POP, veg)
                 endif
                 !! CLN BLAZE TURNOVER
              endif  ! end of year
           else
              casaflux%stemnpp = 0.0_dp
              casaflux%potstemnpp = 0.0_dp
           endif ! CALL_POP
           write(wlogn,*) 'idoy ', idoy

        enddo   ! end of idoy

     enddo   ! end of nyear

  enddo     ! end of nloop
  write(wlogn,*) 'b4 MPI_SEND'
  CALL MPI_Send(MPI_BOTTOM, 1, casa_t, 0, 0, ocomm, ierr)
  ! 13C
  if (cable_user%c13o2) then
     call MPI_Send(MPI_BOTTOM, 1, c13o2_pool_t, 0, 0, ocomm, ierr)
  endif
  write(wlogn,*) 'after MPI_SEND'
  IF(CABLE_USER%CALL_POP) CALL worker_send_pop(POP, ocomm)
  write(wlogn,*) 'cplant ', casapool%cplant(1,:)

END SUBROUTINE worker_spincasacnp

!*********************************************************************************************

SUBROUTINE worker_CASAONLY_LUC(dels, kstart, kend, veg, soil, casabiome, casapool, &
     casaflux, casamet, casabal, phen, POP, climate, LALLOC, &
     c13o2flux, c13o2pools, icomm, ocomm)

  USE cable_def_types_mod
  USE cable_carbon_module
  USE cable_common_module, ONLY: cable_user
  USE casadimension
  USE casaparm
  USE casavariable
  USE phenvariable
  USE POP_Types,           only: POP_TYPE
  use casa_cable,          only: POPdriver
  use casa_inout,          only: biogeochem
  USE TypeDef,             ONLY: dp
  use mpi
  ! 13C
  use cable_c13o2_def,     only: c13o2_flux, c13o2_pool
  use cable_c13o2,         only: c13o2_save_casapool, c13o2_update_pools, &
       c13o2_sanity_pools

  IMPLICIT NONE

  !!CLN  CHARACTER(LEN=99), INTENT(IN)  :: fcnpspin
  real,                      intent(in)    :: dels
  integer,                   intent(in)    :: kstart
  integer,                   intent(in)    :: kend
  integer,                   intent(in)    :: lalloc
  type(veg_parameter_type),  intent(inout) :: veg  ! vegetation parameters
  type(soil_parameter_type), intent(inout) :: soil ! soil parameters
  type(casa_biome),          intent(inout) :: casabiome
  type(casa_pool),           intent(inout) :: casapool
  type(casa_flux),           intent(inout) :: casaflux
  type(casa_met),            intent(inout) :: casamet
  type(casa_balance),        intent(inout) :: casabal
  type(phen_variable),       intent(inout) :: phen
  type(pop_type),            intent(inout) :: pop
  type(climate_type),        intent(inout) :: climate
  ! 13c
  type(c13o2_flux),          intent(inout) :: c13o2flux
  type(c13o2_pool),          intent(inout) :: c13o2pools
  ! communicator for error-messages
  integer,                   intent(in)    :: icomm, ocomm

  ! local variables
  integer           :: myearspin, nyear
  integer           :: ktau, ktauday, nday, idoy
  real(r_2), dimension(mp) :: cleaf2met, cleaf2str, croot2met, croot2str, cwood2cwd
  real(r_2), dimension(mp) :: nleaf2met, nleaf2str, nroot2met, nroot2str, nwood2cwd
  real(r_2), dimension(mp) :: pleaf2met, pleaf2str, proot2met, proot2str, pwood2cwd
  real(r_2), dimension(mp) :: xnplimit,  xkNlimiting, xklitter, xksoil,xkleaf, xkleafcold, xkleafdry

  ! more variables to store the spinup pool size over the last 10 loops. Added by Yp Wang 30 Nov 2012
  real(dp) :: StemNPP(mp,2)
  real(dp) :: PotStemNPP(mp)    !#9238

  integer :: stat(MPI_STATUS_SIZE)
  integer :: ierr, rank
  integer :: yyyy
  ! 13C
  real(dp), dimension(:,:), allocatable :: casasave

  ! 13C
  if (cable_user%c13o2) allocate(casasave(c13o2pools%ntile,c13o2pools%npools))

  ktauday = int(24.0*3600.0/dels)
  nday    = (kend-kstart+1)/ktauday

  myearspin = CABLE_USER%YEAREND - CABLE_USER%YEARSTART + 1
  yyyy      = CABLE_USER%YEARSTART - 1

  do nyear=1, myearspin
     do idoy=1, mdyear
        !CASAONLY_LUC ktau = (idoy-1)*ktauday + ktauday
        ktau = (idoy-1)*ktauday + 1
        CALL MPI_Recv(MPI_BOTTOM, 1, casa_dump_t, 0, idoy, icomm, stat, ierr)

        ! 13C
        if (cable_user%c13o2) call c13o2_save_casapool(casapool, casasave)
        CALL biogeochem(idoy,LALLOC,veg,soil,casabiome,casapool,casaflux, &
             casamet,casabal,phen,POP,climate,xnplimit,xkNlimiting,xklitter, &
             xksoil,xkleaf,xkleafcold,xkleafdry,&
             cleaf2met,cleaf2str,croot2met,croot2str,cwood2cwd,         &
             nleaf2met,nleaf2str,nroot2met,nroot2str,nwood2cwd,         &
             pleaf2met,pleaf2str,proot2met,proot2str,pwood2cwd)
        ! 13C
        if (cable_user%c13o2) then
           call c13o2_update_pools(casasave, casaflux, c13o2flux, c13o2pools)
           call c13o2_sanity_pools(casapool, casaflux, c13o2pools)
        endif

        !! CLN BLAZE here

        ! accumulate annual variables for use in POP
        IF (idoy==1) THEN
           ! (assumes 70% of wood NPP is allocated above ground)
           casaflux%stemnpp  =  casaflux%cnpp * casaflux%fracCalloc(:,2) * 0.7_dp
           !#9238
           casaflux%potstemnpp = casaflux%stemnpp + (casaflux%fracClabile * casaflux%cgpp)
           casabal%LAImax    = casamet%glai
           casabal%Cleafmean = casapool%cplant(:,1) / real(mdyear,dp) / 1000.0_dp
           casabal%Crootmean = casapool%cplant(:,3) / real(mdyear,dp) / 1000.0_dp
        ELSE
           casaflux%stemnpp = casaflux%stemnpp + casaflux%cnpp * casaflux%fracCalloc(:,2) * 0.7_dp
           casaflux%potstemnpp = casaflux%potstemnpp + (casaflux%cnpp * casaflux%fracCalloc(:,2) * 0.7_dp + &
                                                        casaflux%fracClabile * casaflux%cgpp)
           casabal%LAImax    = max(casamet%glai, casabal%LAImax)
           casabal%Cleafmean = casabal%Cleafmean + casapool%cplant(:,1) / real(mdyear,dp) / 1000.0_dp
           casabal%Crootmean = casabal%Crootmean +casapool%cplant(:,3) / real(mdyear,dp) / 1000.0_dp
        ENDIF

        IF (idoy==mdyear) THEN ! end of year
           write(wlogn,*) 'b4 MPI_SEND, casa_LUC_t ', casapool%cplant(:,2)
           flush(wlogn)
           CALL MPI_Send(MPI_BOTTOM, 1, casa_t, 0, 0, ocomm, ierr)
           CALL MPI_Send(MPI_BOTTOM, 1, casa_LUC_t, 0, 0, ocomm, ierr)
           ! 13C
           if (cable_user%c13o2) then
              call MPI_Send(MPI_BOTTOM, 1, c13o2_flux_t, 0, 0, ocomm, ierr)
              call MPI_Send(MPI_BOTTOM, 1, c13o2_pool_t, 0, 0, ocomm, ierr)
              call MPI_Send(MPI_BOTTOM, 1, c13o2_luc_t, 0, 0, ocomm, ierr)
           endif
           write(wlogn,*) 'after MPI_SEND, casa_LUC_t ', casapool%cplant(:,2)
           flush(wlogn)
           StemNPP(:,1) = casaflux%stemnpp
           StemNPP(:,2) = 0.0_dp
           PotStemNPP(:) = casaflux%potstemnpp  !#9238

           CALL MPI_Comm_rank(icomm, rank, ierr)
           write(wlogn,*)
           write(wlogn,*) 'rank receiving pop_grid from master ', rank
           ! write(wlogn,*) 'b4 MPI_Recv, pop_t cmass: ', POP%pop_grid%cmass_sum
           ! write(wlogn,*) 'b4 MPI_Recv, pop_t LU: ', POP%pop_grid%LU
           CALL MPI_Recv(POP%pop_grid(1), POP%np, pop_t, 0, 0, icomm, stat, ierr )
           ! write(wlogn,*)
           ! write(wlogn,*) 'after MPI_Recv, pop_t cmass: ', POP%pop_grid%cmass_sum
           write(wlogn,*) 'after MPI_Recv, pop_t'
           flush(wlogn)
           IF (cable_user%CALL_POP .and. POP%np.gt.0) THEN ! CALL_POP
              write(wlogn,*) 'b4  POPdriver ', POP%pop_grid%cmass_sum
              CALL POPdriver(casaflux, casabal, veg, POP)
              !! CLN BLAZE TURNOVER
           ENDIF
           ! write(wlogn,*)
           ! write(wlogn,*) 'after POPstep cmass: ', POP%pop_grid%cmass_sum
           write(wlogn,*) 'after POPstep ',  POP%pop_grid%cmass_sum
           flush(wlogn)
           CALL worker_send_pop(POP, ocomm)
           write(wlogn,*) 'after worker_send_pop'
           flush(wlogn)
        ENDIF

     enddo ! idoy=1,mdyear

     ! receive updates to CASA pools resulting from LUC
     write(wlogn,*)
     write(wlogn,*) 'b4 mpi_recv casa_LUC_t'
     !NEW CALL MPI_Recv(MPI_BOTTOM, 1, casa_t, 0, nyear, icomm, stat, ierr)
     CALL MPI_Recv(MPI_BOTTOM, 1, casa_LUC_t, 0, nyear, icomm, stat, ierr)
     ! 13C
     if (cable_user%c13o2) then
        call MPI_Recv(MPI_BOTTOM, 1, c13o2_flux_t, 0, nyear, icomm, stat, ierr)
        call MPI_Recv(MPI_BOTTOM, 1, c13o2_pool_t, 0, nyear, icomm, stat, ierr)
        call MPI_Recv(MPI_BOTTOM, 1, c13o2_luc_t, 0, nyear, icomm, stat, ierr)
     endif
     write(wlogn,*) 'after mpi_recv casa_LUC_t'
  enddo

END SUBROUTINE WORKER_CASAONLY_LUC


END MODULE cable_mpiworker
